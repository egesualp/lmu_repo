{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- NumPy: Ideal for general-purpose numerical computations and works well on CPUs. It lacks built-in support for GPU computation and automatic differentiation.\n",
    "- PyTorch: Tailored for deep learning with built-in support for GPU acceleration and automatic differentiation. It is highly suitable for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Football Pitch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_x: \n",
      " tensor([[1, 1, 1, 1, 1],\n",
      "        [2, 2, 2, 2, 2],\n",
      "        [3, 3, 3, 3, 3],\n",
      "        [4, 4, 4, 4, 4],\n",
      "        [5, 5, 5, 5, 5],\n",
      "        [6, 6, 6, 6, 6],\n",
      "        [7, 7, 7, 7, 7],\n",
      "        [8, 8, 8, 8, 8],\n",
      "        [9, 9, 9, 9, 9]])\n",
      "grid_y: \n",
      " tensor([[1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Define the coordinate vectors for the length and width of the pitch\n",
    "length = torch.arange(1, 10)\n",
    "width = torch.arange(1, 6)\n",
    "\n",
    "# Generate coordinate grids\n",
    "grid_x, grid_y = torch.meshgrid(length, width)\n",
    "\n",
    "print(f\"grid_x: \\n {grid_x}\")\n",
    "\n",
    "print(f\"grid_y: \\n {grid_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- grid_x contains the x-coordinates (length) repeated along the columns.\n",
    "- grid_y contains the y-coordinates (width) repeated along the rows.\n",
    "\n",
    "Each element (grid_x[i, j], grid_y[i, j]) corresponds to a specific coordinate on the pitch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: Locating a Football Player*\n",
    "\n",
    "Let's say we want to place a football player at the coordinate (3, 2). This means the player is standing at the 3rd position in length and the 2nd position in width.\n",
    "\n",
    "Using the grid indices:\n",
    "\n",
    "i corresponds to the length index.\n",
    "j corresponds to the width index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The football player stands at coordinate: (tensor(3), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "j = 1\n",
    "\n",
    "player_position = (grid_x[i, j], grid_y[i, j])\n",
    "print(f\"The football player stands at coordinate: {player_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_x[i, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar tensors have no dimensions \n",
    "# and can be converted to standard Python numbers using .item().\n",
    "grid_x[i, j].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_y[i, j].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing player position\n",
    "\n",
    "def visualize_player_pos(grid_x, grid_y, player_position):\n",
    "    for i in range(grid_x.shape[0]):\n",
    "        row = []\n",
    "        for j in range(grid_x.shape[1]):\n",
    "            if (grid_x[i, j], grid_y[i,j]) == player_position:\n",
    "                row.append(f\"({grid_x[i, j].item()},{grid_y[i, j].item()})*\")  # Mark player position\n",
    "            else:\n",
    "                row.append(f\"({grid_x[i, j].item()},{grid_y[i, j].item()})\")\n",
    "        print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,1) (1,2) (1,3) (1,4) (1,5)\n",
      "(2,1) (2,2) (2,3) (2,4) (2,5)\n",
      "(3,1) (3,2)* (3,3) (3,4) (3,5)\n",
      "(4,1) (4,2) (4,3) (4,4) (4,5)\n",
      "(5,1) (5,2) (5,3) (5,4) (5,5)\n",
      "(6,1) (6,2) (6,3) (6,4) (6,5)\n",
      "(7,1) (7,2) (7,3) (7,4) (7,5)\n",
      "(8,1) (8,2) (8,3) (8,4) (8,5)\n",
      "(9,1) (9,2) (9,3) (9,4) (9,5)\n"
     ]
    }
   ],
   "source": [
    "visualize_player_pos(grid_x, grid_y, player_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    torch_tensor = torch_tensor.to('cuda')\n",
    "    print(torch_tensor)  # Tensor is now on the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.8\n",
      "ID of current CUDA device:0\n",
      "Name of current CUDA device:NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    " \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "       \n",
    "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example for gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x: 4.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Create a tensor with requires_grad = True to track computations\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a simple function\n",
    "y = x ** 2\n",
    "\n",
    "# Calculate the gradient of y with respect to x\n",
    "y.backward() # This computes the gradient dy/dx\n",
    "print(f\"Gradient of y with respect to x: {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of z (x + 1) without tracking gradients: 3.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Now let's use no_grad to perform operations without tracking gradients\n",
    "with torch.no_grad():\n",
    "    z = x + 1\n",
    "    print(f'Value of z (x + 1) without tracking gradients: {z}')\n",
    "    print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset gradient to zero\n",
    "x.grad.zero_()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w wrt x: 24.0\n"
     ]
    }
   ],
   "source": [
    "# Perform operations with gradients again\n",
    "w = x ** 3 + 3 * x ** 2\n",
    "w.backward() # This computes the gradient dw/dx\n",
    "print(f'Gradient of w wrt x: {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      5\u001b[0m     q \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\esual\\Locals\\local_repos\\venvLmu\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\esual\\Locals\\local_repos\\venvLmu\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\esual\\Locals\\local_repos\\venvLmu\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Now demonstrating the effect of torch.no_grad()\n",
    "x.grad.zero_()\n",
    "\n",
    "with torch.no_grad():\n",
    "    q = x ** 3 + 3 * x ** 2\n",
    "    q.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x after first backward: 4.0\n",
      "Value of z (x^3 + 3x^2) without tracking gradients: 20.0\n",
      "Stored gradient of y with respect to x: 4.0\n",
      "Gradient of w with respect to x after third backward (without zeroing): 36.0\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with requires_grad=True to track computations\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First function: y = x^2\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f'Gradient of y with respect to x after first backward: {x.grad}')  # Prints: 4.0\n",
    "\n",
    "# Store the gradient from the first function\n",
    "grad_y = x.grad.clone()\n",
    "\n",
    "# Perform operations without tracking gradients using torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    z = x ** 3 + 3 * x ** 2\n",
    "    print(f'Value of z (x^3 + 3x^2) without tracking gradients: {z}')  # This will not affect x.grad\n",
    "\n",
    "# Verify that the gradient from the first function is preserved\n",
    "print(f'Stored gradient of y with respect to x: {grad_y}')  # Should still be 4.0\n",
    "\n",
    "# Perform another operation with gradient tracking enabled\n",
    "w = x ** 4\n",
    "w.backward()\n",
    "print(f'Gradient of w with respect to x after third backward (without zeroing): {x.grad}')  # Prints: 36.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, w is defined as x^4. The gradient dw/dx = 4x^3, so dw/dx is 32.0 when x = 2.0. Since we did not reset the gradient, x.grad now accumulates the gradient from the previous backward pass (4.0 for y = x^2) and the new backward pass (32.0 for w = x^4). Therefore, x.grad is 4.0 + 32.0 = 36.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer vision and deep learning, N, C, H, and W are commonly used abbreviations that represent dimensions in multi-dimensional arrays or tensors. Here's what they typically stand for:\n",
    "\n",
    "N: Stands for the batch size or the number of samples in a batch. In deep learning, it is common to process data in batches rather than individual samples. The batch size determines how many samples are processed simultaneously during training or inference.\n",
    "\n",
    "C: Represents the number of channels. In computer vision, an image can have multiple channels, such as Red, Green, and Blue (RGB) channels. The number of channels can also represent different features or filters in a convolutional neural network (CNN).\n",
    "\n",
    "H: Denotes the height of an image or the number of rows in a tensor. In the context of computer vision, it refers to the vertical dimension of an image or the height of a feature map in a CNN.\n",
    "\n",
    "W: Represents the width of an image or the number of columns in a tensor. In computer vision, it refers to the horizontal dimension of an image or the width of a feature map in a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W] torch.Size([64, 1, 28, 28])\n",
      "Shape of Y torch.Size([64])\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Passing the 'Dataset' as an argument to 'DataLoader' \n",
    "# This wraps an iterable over our dataset, and support automatic batcvhing, sampling etc.\n",
    "# Here we define batch size of 64 -> a botch of 64 features and labels\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W] {X.shape}\") \n",
    "    print(f\"Shape of Y {y.shape}\")\n",
    "    print(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Defining NN model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3) # stochastic GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a single training loop, the model makes predictions on the training dataset\n",
    "# fed to it in batches\n",
    "# and backpropagates the prediciton error to adjust model's parameters\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediciton error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also check the model's performance against the test dataset to ensure its learning\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning:\n",
    "\n",
    "- Epoch: One complete pass through the entire training dataset.\n",
    "- Batch: A subset of the training data used to update model weights in one iteration.\n",
    "\n",
    "*TL;DR: Epoch = full dataset pass, Batch = data subset for one update.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.182179 [   64/60000]\n",
      "loss: 2.160292 [ 6464/60000]\n",
      "loss: 2.114250 [12864/60000]\n",
      "loss: 2.138345 [19264/60000]\n",
      "loss: 2.080538 [25664/60000]\n",
      "loss: 2.031904 [32064/60000]\n",
      "loss: 2.064902 [38464/60000]\n",
      "loss: 1.978102 [44864/60000]\n",
      "loss: 1.993188 [51264/60000]\n",
      "loss: 1.938186 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.919589 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.949327 [   64/60000]\n",
      "loss: 1.903763 [ 6464/60000]\n",
      "loss: 1.800116 [12864/60000]\n",
      "loss: 1.856154 [19264/60000]\n",
      "loss: 1.737408 [25664/60000]\n",
      "loss: 1.687487 [32064/60000]\n",
      "loss: 1.729351 [38464/60000]\n",
      "loss: 1.612513 [44864/60000]\n",
      "loss: 1.645349 [51264/60000]\n",
      "loss: 1.556500 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.556003 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.619138 [   64/60000]\n",
      "loss: 1.570495 [ 6464/60000]\n",
      "loss: 1.429782 [12864/60000]\n",
      "loss: 1.516708 [19264/60000]\n",
      "loss: 1.392405 [25664/60000]\n",
      "loss: 1.378889 [32064/60000]\n",
      "loss: 1.412593 [38464/60000]\n",
      "loss: 1.319684 [44864/60000]\n",
      "loss: 1.357994 [51264/60000]\n",
      "loss: 1.270517 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.284629 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.361275 [   64/60000]\n",
      "loss: 1.330132 [ 6464/60000]\n",
      "loss: 1.172454 [12864/60000]\n",
      "loss: 1.289941 [19264/60000]\n",
      "loss: 1.162359 [25664/60000]\n",
      "loss: 1.177834 [32064/60000]\n",
      "loss: 1.214034 [38464/60000]\n",
      "loss: 1.136982 [44864/60000]\n",
      "loss: 1.177064 [51264/60000]\n",
      "loss: 1.104202 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.115198 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.188268 [   64/60000]\n",
      "loss: 1.176177 [ 6464/60000]\n",
      "loss: 1.001644 [12864/60000]\n",
      "loss: 1.147092 [19264/60000]\n",
      "loss: 1.017973 [25664/60000]\n",
      "loss: 1.040710 [32064/60000]\n",
      "loss: 1.089897 [38464/60000]\n",
      "loss: 1.019291 [44864/60000]\n",
      "loss: 1.059090 [51264/60000]\n",
      "loss: 0.999647 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.004709 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved as model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
