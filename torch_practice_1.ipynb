{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- NumPy: Ideal for general-purpose numerical computations and works well on CPUs. It lacks built-in support for GPU computation and automatic differentiation.\n",
    "- PyTorch: Tailored for deep learning with built-in support for GPU acceleration and automatic differentiation. It is highly suitable for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Football Pitch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_x: \n",
      " tensor([[1, 1, 1, 1, 1],\n",
      "        [2, 2, 2, 2, 2],\n",
      "        [3, 3, 3, 3, 3],\n",
      "        [4, 4, 4, 4, 4],\n",
      "        [5, 5, 5, 5, 5],\n",
      "        [6, 6, 6, 6, 6],\n",
      "        [7, 7, 7, 7, 7],\n",
      "        [8, 8, 8, 8, 8],\n",
      "        [9, 9, 9, 9, 9]])\n",
      "grid_y: \n",
      " tensor([[1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Define the coordinate vectors for the length and width of the pitch\n",
    "length = torch.arange(1, 10)\n",
    "width = torch.arange(1, 6)\n",
    "\n",
    "# Generate coordinate grids\n",
    "grid_x, grid_y = torch.meshgrid(length, width)\n",
    "\n",
    "print(f\"grid_x: \\n {grid_x}\")\n",
    "\n",
    "print(f\"grid_y: \\n {grid_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- grid_x contains the x-coordinates (length) repeated along the columns.\n",
    "- grid_y contains the y-coordinates (width) repeated along the rows.\n",
    "\n",
    "Each element (grid_x[i, j], grid_y[i, j]) corresponds to a specific coordinate on the pitch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: Locating a Football Player*\n",
    "\n",
    "Let's say we want to place a football player at the coordinate (3, 2). This means the player is standing at the 3rd position in length and the 2nd position in width.\n",
    "\n",
    "Using the grid indices:\n",
    "\n",
    "i corresponds to the length index.\n",
    "j corresponds to the width index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The football player stands at coordinate: (tensor(3), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "j = 1\n",
    "\n",
    "player_position = (grid_x[i, j], grid_y[i, j])\n",
    "print(f\"The football player stands at coordinate: {player_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_x[i, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar tensors have no dimensions \n",
    "# and can be converted to standard Python numbers using .item().\n",
    "grid_x[i, j].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_y[i, j].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing player position\n",
    "\n",
    "def visualize_player_pos(grid_x, grid_y, player_position):\n",
    "    for i in range(grid_x.shape[0]):\n",
    "        row = []\n",
    "        for j in range(grid_x.shape[1]):\n",
    "            if (grid_x[i, j], grid_y[i,j]) == player_position:\n",
    "                row.append(f\"({grid_x[i, j].item()},{grid_y[i, j].item()})*\")  # Mark player position\n",
    "            else:\n",
    "                row.append(f\"({grid_x[i, j].item()},{grid_y[i, j].item()})\")\n",
    "        print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,1) (1,2) (1,3) (1,4) (1,5)\n",
      "(2,1) (2,2) (2,3) (2,4) (2,5)\n",
      "(3,1) (3,2)* (3,3) (3,4) (3,5)\n",
      "(4,1) (4,2) (4,3) (4,4) (4,5)\n",
      "(5,1) (5,2) (5,3) (5,4) (5,5)\n",
      "(6,1) (6,2) (6,3) (6,4) (6,5)\n",
      "(7,1) (7,2) (7,3) (7,4) (7,5)\n",
      "(8,1) (8,2) (8,3) (8,4) (8,5)\n",
      "(9,1) (9,2) (9,3) (9,4) (9,5)\n"
     ]
    }
   ],
   "source": [
    "visualize_player_pos(grid_x, grid_y, player_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    torch_tensor = torch_tensor.to('cuda')\n",
    "    print(torch_tensor)  # Tensor is now on the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.8\n",
      "ID of current CUDA device:0\n",
      "Name of current CUDA device:NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    " \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "       \n",
    "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example for gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x: 4.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Create a tensor with requires_grad = True to track computations\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a simple function\n",
    "y = x ** 2\n",
    "\n",
    "# Calculate the gradient of y with respect to x\n",
    "y.backward() # This computes the gradient dy/dx\n",
    "print(f\"Gradient of y with respect to x: {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of z (x + 1) without tracking gradients: 3.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Now let's use no_grad to perform operations without tracking gradients\n",
    "with torch.no_grad():\n",
    "    z = x + 1\n",
    "    print(f'Value of z (x + 1) without tracking gradients: {z}')\n",
    "    print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset gradient to zero\n",
    "x.grad.zero_()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w wrt x: 24.0\n"
     ]
    }
   ],
   "source": [
    "# Perform operations with gradients again\n",
    "w = x ** 3 + 3 * x ** 2\n",
    "w.backward() # This computes the gradient dw/dx\n",
    "print(f'Gradient of w wrt x: {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      5\u001b[0m     q \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\esual\\Locals\\local_repos\\venvLmu\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\esual\\Locals\\local_repos\\venvLmu\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\esual\\Locals\\local_repos\\venvLmu\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Now demonstrating the effect of torch.no_grad()\n",
    "x.grad.zero_()\n",
    "\n",
    "with torch.no_grad():\n",
    "    q = x ** 3 + 3 * x ** 2\n",
    "    q.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x after first backward: 4.0\n",
      "Value of z (x^3 + 3x^2) without tracking gradients: 20.0\n",
      "Stored gradient of y with respect to x: 4.0\n",
      "Gradient of w with respect to x after third backward (without zeroing): 36.0\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with requires_grad=True to track computations\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First function: y = x^2\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f'Gradient of y with respect to x after first backward: {x.grad}')  # Prints: 4.0\n",
    "\n",
    "# Store the gradient from the first function\n",
    "grad_y = x.grad.clone()\n",
    "\n",
    "# Perform operations without tracking gradients using torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    z = x ** 3 + 3 * x ** 2\n",
    "    print(f'Value of z (x^3 + 3x^2) without tracking gradients: {z}')  # This will not affect x.grad\n",
    "\n",
    "# Verify that the gradient from the first function is preserved\n",
    "print(f'Stored gradient of y with respect to x: {grad_y}')  # Should still be 4.0\n",
    "\n",
    "# Perform another operation with gradient tracking enabled\n",
    "w = x ** 4\n",
    "w.backward()\n",
    "print(f'Gradient of w with respect to x after third backward (without zeroing): {x.grad}')  # Prints: 36.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, w is defined as x^4. The gradient dw/dx = 4x^3, so dw/dx is 32.0 when x = 2.0. Since we did not reset the gradient, x.grad now accumulates the gradient from the previous backward pass (4.0 for y = x^2) and the new backward pass (32.0 for w = x^4). Therefore, x.grad is 4.0 + 32.0 = 36.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer vision and deep learning, N, C, H, and W are commonly used abbreviations that represent dimensions in multi-dimensional arrays or tensors. Here's what they typically stand for:\n",
    "\n",
    "N: Stands for the batch size or the number of samples in a batch. In deep learning, it is common to process data in batches rather than individual samples. The batch size determines how many samples are processed simultaneously during training or inference.\n",
    "\n",
    "C: Represents the number of channels. In computer vision, an image can have multiple channels, such as Red, Green, and Blue (RGB) channels. The number of channels can also represent different features or filters in a convolutional neural network (CNN).\n",
    "\n",
    "H: Denotes the height of an image or the number of rows in a tensor. In the context of computer vision, it refers to the vertical dimension of an image or the height of a feature map in a CNN.\n",
    "\n",
    "W: Represents the width of an image or the number of columns in a tensor. In computer vision, it refers to the horizontal dimension of an image or the width of a feature map in a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W] torch.Size([64, 1, 28, 28])\n",
      "Shape of Y torch.Size([64])\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Passing the 'Dataset' as an argument to 'DataLoader' \n",
    "# This wraps an iterable over our dataset, and support automatic batcvhing, sampling etc.\n",
    "# Here we define batch size of 64 -> a botch of 64 features and labels\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W] {X.shape}\") \n",
    "    print(f\"Shape of Y {y.shape}\")\n",
    "    print(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Defining NN model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3) # stochastic GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a single training loop, the model makes predictions on the training dataset\n",
    "# fed to it in batches\n",
    "# and backpropagates the prediciton error to adjust model's parameters\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediciton error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also check the model's performance against the test dataset to ensure its learning\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning:\n",
    "\n",
    "- Epoch: One complete pass through the entire training dataset.\n",
    "- Batch: A subset of the training data used to update model weights in one iteration.\n",
    "\n",
    "*TL;DR: Epoch = full dataset pass, Batch = data subset for one update.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.182179 [   64/60000]\n",
      "loss: 2.160292 [ 6464/60000]\n",
      "loss: 2.114250 [12864/60000]\n",
      "loss: 2.138345 [19264/60000]\n",
      "loss: 2.080538 [25664/60000]\n",
      "loss: 2.031904 [32064/60000]\n",
      "loss: 2.064902 [38464/60000]\n",
      "loss: 1.978102 [44864/60000]\n",
      "loss: 1.993188 [51264/60000]\n",
      "loss: 1.938186 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.919589 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.949327 [   64/60000]\n",
      "loss: 1.903763 [ 6464/60000]\n",
      "loss: 1.800116 [12864/60000]\n",
      "loss: 1.856154 [19264/60000]\n",
      "loss: 1.737408 [25664/60000]\n",
      "loss: 1.687487 [32064/60000]\n",
      "loss: 1.729351 [38464/60000]\n",
      "loss: 1.612513 [44864/60000]\n",
      "loss: 1.645349 [51264/60000]\n",
      "loss: 1.556500 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.556003 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.619138 [   64/60000]\n",
      "loss: 1.570495 [ 6464/60000]\n",
      "loss: 1.429782 [12864/60000]\n",
      "loss: 1.516708 [19264/60000]\n",
      "loss: 1.392405 [25664/60000]\n",
      "loss: 1.378889 [32064/60000]\n",
      "loss: 1.412593 [38464/60000]\n",
      "loss: 1.319684 [44864/60000]\n",
      "loss: 1.357994 [51264/60000]\n",
      "loss: 1.270517 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.284629 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.361275 [   64/60000]\n",
      "loss: 1.330132 [ 6464/60000]\n",
      "loss: 1.172454 [12864/60000]\n",
      "loss: 1.289941 [19264/60000]\n",
      "loss: 1.162359 [25664/60000]\n",
      "loss: 1.177834 [32064/60000]\n",
      "loss: 1.214034 [38464/60000]\n",
      "loss: 1.136982 [44864/60000]\n",
      "loss: 1.177064 [51264/60000]\n",
      "loss: 1.104202 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.115198 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.188268 [   64/60000]\n",
      "loss: 1.176177 [ 6464/60000]\n",
      "loss: 1.001644 [12864/60000]\n",
      "loss: 1.147092 [19264/60000]\n",
      "loss: 1.017973 [25664/60000]\n",
      "loss: 1.040710 [32064/60000]\n",
      "loss: 1.089897 [38464/60000]\n",
      "loss: 1.019291 [44864/60000]\n",
      "loss: 1.059090 [51264/60000]\n",
      "loss: 0.999647 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.004709 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved as model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does .squeeze()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example output tensor with an extra dimension\n",
    "outputs = torch.tensor([[0.8], [0.3], [0.9], [0.1]])\n",
    "print(outputs.shape)  # Output: torch.Size([4, 1])\n",
    "\n",
    "# Using squeeze() to remove the extra dimension\n",
    "outputs_squeezed = outputs.squeeze()\n",
    "print(outputs_squeezed.shape)  # Output: torch.Size([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: tensor([[0.8000],\n",
      "        [0.3000],\n",
      "        [0.9000],\n",
      "        [0.1000]])\n",
      "Outputs Squeezed: tensor([0.8000, 0.3000, 0.9000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Outputs: {outputs}\")\n",
    "print(f\"Outputs Squeezed: {outputs_squeezed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outputs_squeezed > 0.2).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does .item()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss as tensor: tensor(0.5222)\n",
      "Loss as scalar (Python float): 0.522203266620636\n",
      "Correct predictions as tensor: tensor(3)\n",
      "Correct predictions as scalar: 3\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example: Binary classification problem\n",
    "# Suppose we have a batch of 3 predictions and corresponding labels\n",
    "predictions = torch.tensor([[0.8], [0.3], [0.9]])  # logits\n",
    "labels = torch.tensor([1, 0, 1])  # true labels\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(predictions.squeeze(), labels.float())  # Compute loss\n",
    "\n",
    "# Print the tensor returned by the loss function\n",
    "print(\"Loss as tensor:\", loss)  # This is a tensor with a single scalar value\n",
    "\n",
    "# Convert the tensor to a Python float using .item()\n",
    "print(\"Loss as scalar (Python float):\", loss.item())\n",
    "\n",
    "# Accuracy calculation\n",
    "preds = (predictions.squeeze() > 0.5).int()  # Convert logits to binary predictions\n",
    "correct_predictions = (preds == labels).sum()  # Count correct predictions\n",
    "\n",
    "# Print the tensor result of the sum\n",
    "print(\"Correct predictions as tensor:\", correct_predictions)\n",
    "\n",
    "# Convert to a Python scalar\n",
    "print(\"Correct predictions as scalar:\", correct_predictions.item())\n",
    "\n",
    "# Use for metrics calculation\n",
    "total_samples = labels.size(0)\n",
    "accuracy = correct_predictions.item() / total_samples\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 3 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a Tensor with 3 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "predictions.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need squeze and float?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error without squeeze and float: Target size (torch.Size([4])) must be the same as input size (torch.Size([4, 1]))\n",
      "Error with squeeze but no float: result type Float can't be cast to the desired output type Long\n",
      "Correct loss (squeeze + float): 0.577751636505127\n",
      "Correct loss (labels already float): 0.577751636505127\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple batch of model outputs (logits) and labels\n",
    "logits = torch.tensor([[0.8], [0.3], [0.9], [0.1]])  # Shape: (4, 1)\n",
    "labels_int = torch.tensor([1, 0, 1, 0])  # Integer labels, shape: (4,)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Case 1: Incorrect usage (no squeeze, no float conversion)\n",
    "try:\n",
    "    batch_loss_incorrect = criterion(logits, labels_int)\n",
    "    print(\"Loss without squeeze and float:\", batch_loss_incorrect.item())\n",
    "except Exception as e:\n",
    "    print(\"Error without squeeze and float:\", e)\n",
    "\n",
    "# Case 2: Incorrect usage (squeeze but no float conversion)\n",
    "try:\n",
    "    batch_loss_incorrect_float = criterion(logits.squeeze(), labels_int)\n",
    "    print(\"Loss with squeeze but no float:\", batch_loss_incorrect_float.item())\n",
    "except Exception as e:\n",
    "    print(\"Error with squeeze but no float:\", e)\n",
    "\n",
    "# Case 3: Correct usage (squeeze logits and convert labels to float)\n",
    "try:\n",
    "    batch_loss_correct = criterion(logits.squeeze(), labels_int.float())\n",
    "    print(\"Correct loss (squeeze + float):\", batch_loss_correct.item())\n",
    "except Exception as e:\n",
    "    print(\"Error in correct case:\", e)\n",
    "\n",
    "# Case 4: Correct usage (labels already as floats, just squeeze logits)\n",
    "labels_float = labels_int.float()\n",
    "try:\n",
    "    batch_loss_correct_float = criterion(logits.squeeze(), labels_float)\n",
    "    print(\"Correct loss (labels already float):\", batch_loss_correct_float.item())\n",
    "except Exception as e:\n",
    "    print(\"Error with float labels:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using view\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 28\n",
    "emb_dim = 128\n",
    "num_heads = 4\n",
    "head_dim = emb_dim // num_heads\n",
    "\n",
    "Q = torch.rand(batch_size, seq_len, num_heads * head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9.4619e-02, 9.0232e-01, 6.6131e-01,  ..., 5.8428e-02,\n",
       "           6.6578e-01, 4.8033e-01],\n",
       "          [6.2897e-01, 3.9584e-01, 7.3918e-01,  ..., 4.4627e-01,\n",
       "           5.3433e-01, 2.7637e-01],\n",
       "          [1.1914e-01, 5.3952e-01, 1.2978e-01,  ..., 7.8426e-01,\n",
       "           3.1282e-01, 8.7217e-01],\n",
       "          [2.5570e-01, 6.3406e-01, 4.3206e-01,  ..., 1.5554e-01,\n",
       "           5.1569e-01, 6.8033e-01]],\n",
       "\n",
       "         [[1.6979e-01, 2.3182e-02, 9.8731e-01,  ..., 3.7517e-01,\n",
       "           5.8407e-01, 8.7816e-01],\n",
       "          [7.4487e-01, 5.6965e-01, 6.2576e-01,  ..., 5.4415e-01,\n",
       "           6.9752e-01, 4.9220e-01],\n",
       "          [9.6668e-01, 9.9252e-01, 5.9463e-01,  ..., 5.8988e-01,\n",
       "           9.5338e-01, 5.5533e-01],\n",
       "          [2.6364e-01, 7.8745e-01, 5.5403e-01,  ..., 8.8815e-01,\n",
       "           4.4679e-01, 1.3823e-01]],\n",
       "\n",
       "         [[1.9543e-01, 1.8294e-01, 1.1664e-01,  ..., 4.0223e-01,\n",
       "           7.7993e-01, 8.8495e-01],\n",
       "          [1.3703e-01, 6.1146e-01, 4.5294e-01,  ..., 4.4185e-01,\n",
       "           1.7802e-01, 1.4189e-01],\n",
       "          [9.2291e-01, 5.7076e-01, 3.6859e-01,  ..., 7.0829e-01,\n",
       "           1.7843e-01, 1.5774e-01],\n",
       "          [1.2690e-01, 7.1965e-01, 7.6547e-01,  ..., 8.5843e-01,\n",
       "           8.2685e-01, 5.6382e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.7670e-01, 6.4571e-01, 8.1315e-01,  ..., 1.2870e-01,\n",
       "           6.8922e-01, 1.9845e-01],\n",
       "          [3.4363e-02, 7.0212e-01, 3.2690e-01,  ..., 3.6072e-01,\n",
       "           2.3011e-03, 5.9781e-01],\n",
       "          [8.6027e-01, 4.5750e-01, 5.0193e-01,  ..., 2.0940e-01,\n",
       "           1.9528e-01, 2.8988e-01],\n",
       "          [9.0335e-01, 2.1654e-01, 6.1498e-01,  ..., 6.8620e-01,\n",
       "           4.4761e-01, 5.1926e-02]],\n",
       "\n",
       "         [[1.0469e-01, 2.4170e-01, 9.0209e-01,  ..., 9.5676e-01,\n",
       "           9.8379e-01, 3.3487e-01],\n",
       "          [3.7028e-01, 5.6794e-01, 1.2479e-01,  ..., 9.4264e-01,\n",
       "           6.3740e-01, 6.9038e-01],\n",
       "          [2.5172e-01, 4.6625e-01, 3.1526e-01,  ..., 6.3426e-01,\n",
       "           8.8763e-01, 9.3333e-01],\n",
       "          [5.2280e-01, 1.4479e-01, 8.3706e-01,  ..., 2.0924e-01,\n",
       "           9.4485e-01, 7.6203e-01]],\n",
       "\n",
       "         [[9.7900e-02, 2.7314e-01, 3.1512e-01,  ..., 2.6354e-01,\n",
       "           8.9125e-01, 1.1312e-01],\n",
       "          [9.8216e-02, 4.5386e-02, 7.9197e-01,  ..., 9.8736e-01,\n",
       "           7.7301e-01, 1.4922e-01],\n",
       "          [1.0623e-01, 2.4062e-01, 2.5251e-01,  ..., 2.6437e-01,\n",
       "           6.3085e-01, 6.1396e-02],\n",
       "          [8.8654e-01, 4.2379e-01, 6.7523e-01,  ..., 3.7162e-01,\n",
       "           9.9528e-01, 2.9098e-01]]],\n",
       "\n",
       "\n",
       "        [[[4.3625e-01, 8.9979e-01, 5.5839e-01,  ..., 4.7627e-01,\n",
       "           1.5463e-01, 8.5144e-01],\n",
       "          [5.7576e-01, 4.2912e-01, 8.1926e-01,  ..., 2.1677e-01,\n",
       "           7.7128e-01, 6.8469e-01],\n",
       "          [4.0923e-01, 2.0173e-01, 8.5175e-01,  ..., 2.3359e-01,\n",
       "           1.3050e-01, 4.0158e-01],\n",
       "          [1.9839e-01, 6.0448e-01, 6.2207e-01,  ..., 2.0198e-01,\n",
       "           7.7335e-01, 4.5541e-02]],\n",
       "\n",
       "         [[4.1021e-01, 4.3594e-01, 3.9073e-01,  ..., 7.1405e-01,\n",
       "           6.0857e-02, 1.4940e-01],\n",
       "          [7.1044e-01, 6.8658e-01, 2.9686e-01,  ..., 5.8795e-02,\n",
       "           7.9422e-01, 6.4659e-01],\n",
       "          [4.0209e-01, 9.3930e-01, 8.5624e-01,  ..., 5.1917e-01,\n",
       "           8.2661e-01, 3.1187e-01],\n",
       "          [8.6402e-01, 3.2288e-01, 4.4450e-01,  ..., 4.0455e-01,\n",
       "           2.1587e-01, 2.8078e-01]],\n",
       "\n",
       "         [[7.9427e-01, 6.5898e-01, 5.8368e-01,  ..., 3.0419e-01,\n",
       "           6.6454e-01, 4.1423e-01],\n",
       "          [4.4647e-01, 7.1066e-01, 9.0102e-01,  ..., 3.0240e-01,\n",
       "           2.0726e-01, 8.4160e-01],\n",
       "          [2.8335e-01, 7.1064e-01, 5.6907e-01,  ..., 6.9628e-01,\n",
       "           1.2083e-02, 8.6014e-01],\n",
       "          [8.1247e-01, 1.9099e-02, 3.5671e-01,  ..., 4.2929e-01,\n",
       "           7.7407e-01, 1.2008e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.6833e-01, 3.2975e-01, 6.3318e-01,  ..., 7.2855e-02,\n",
       "           6.9224e-01, 6.0475e-01],\n",
       "          [3.6958e-01, 4.5903e-01, 7.5293e-01,  ..., 5.1923e-01,\n",
       "           7.4354e-01, 1.3278e-01],\n",
       "          [8.1146e-03, 4.6128e-01, 2.8167e-01,  ..., 7.9679e-01,\n",
       "           1.2809e-01, 2.3377e-01],\n",
       "          [3.2430e-01, 6.7324e-01, 2.0792e-01,  ..., 8.3150e-01,\n",
       "           6.4293e-01, 8.6106e-01]],\n",
       "\n",
       "         [[7.1141e-01, 1.5781e-01, 6.6197e-01,  ..., 8.6185e-01,\n",
       "           2.5478e-01, 1.6944e-01],\n",
       "          [5.8679e-01, 4.1085e-01, 5.0415e-01,  ..., 4.9863e-01,\n",
       "           2.6460e-02, 7.8216e-01],\n",
       "          [2.4422e-01, 6.5874e-01, 8.1495e-01,  ..., 9.8708e-02,\n",
       "           2.0159e-02, 6.6017e-01],\n",
       "          [1.8206e-01, 9.7616e-01, 3.7679e-01,  ..., 7.2156e-01,\n",
       "           8.2113e-01, 5.4549e-01]],\n",
       "\n",
       "         [[9.1376e-01, 6.9763e-01, 6.5704e-02,  ..., 2.9751e-01,\n",
       "           1.4177e-01, 5.1184e-01],\n",
       "          [7.6015e-01, 5.5071e-01, 6.9137e-01,  ..., 3.1201e-01,\n",
       "           2.8088e-01, 3.5235e-01],\n",
       "          [7.3877e-01, 6.0289e-01, 2.5175e-01,  ..., 6.8704e-01,\n",
       "           3.3481e-01, 2.6214e-01],\n",
       "          [9.9631e-01, 4.1403e-01, 3.5936e-01,  ..., 4.2307e-01,\n",
       "           9.5672e-01, 9.3384e-01]]],\n",
       "\n",
       "\n",
       "        [[[2.1407e-01, 6.6622e-01, 2.2073e-01,  ..., 8.5554e-01,\n",
       "           8.2551e-01, 5.7160e-01],\n",
       "          [7.9641e-01, 4.0732e-03, 4.6745e-01,  ..., 1.5904e-02,\n",
       "           1.1400e-01, 1.0874e-01],\n",
       "          [1.3210e-01, 1.0530e-01, 5.5608e-01,  ..., 8.8027e-01,\n",
       "           9.8815e-01, 9.2990e-01],\n",
       "          [4.8444e-01, 8.0570e-02, 3.6480e-01,  ..., 6.5684e-01,\n",
       "           4.5497e-01, 1.5256e-01]],\n",
       "\n",
       "         [[4.1207e-01, 5.5474e-01, 7.7431e-01,  ..., 1.2072e-01,\n",
       "           3.2011e-01, 8.4843e-01],\n",
       "          [7.3304e-01, 1.0380e-01, 6.3190e-01,  ..., 2.4557e-01,\n",
       "           5.6810e-01, 7.2999e-02],\n",
       "          [1.0512e-01, 5.3485e-01, 1.3501e-01,  ..., 3.5441e-01,\n",
       "           5.2946e-01, 2.4564e-01],\n",
       "          [1.5089e-01, 1.4322e-01, 8.2014e-01,  ..., 3.5650e-01,\n",
       "           8.7498e-01, 2.5844e-01]],\n",
       "\n",
       "         [[4.3306e-01, 9.9269e-01, 7.4842e-01,  ..., 6.6378e-03,\n",
       "           4.3664e-01, 3.5923e-01],\n",
       "          [4.4070e-01, 5.9534e-01, 5.0957e-01,  ..., 9.4318e-01,\n",
       "           8.6013e-01, 7.0774e-01],\n",
       "          [1.1825e-01, 2.8285e-01, 2.0581e-01,  ..., 1.8616e-02,\n",
       "           1.5715e-01, 7.2556e-01],\n",
       "          [9.2535e-01, 2.2185e-01, 5.7832e-01,  ..., 9.6729e-02,\n",
       "           2.2706e-01, 6.1799e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[6.6699e-01, 1.1248e-01, 2.7720e-01,  ..., 8.7794e-01,\n",
       "           1.9177e-01, 2.3804e-01],\n",
       "          [8.4826e-01, 7.4317e-01, 6.5067e-01,  ..., 5.3232e-01,\n",
       "           5.2470e-01, 3.8465e-01],\n",
       "          [2.5388e-01, 1.9660e-01, 1.3021e-01,  ..., 9.6450e-01,\n",
       "           9.0344e-01, 4.8415e-01],\n",
       "          [1.2662e-01, 8.7067e-01, 2.7300e-01,  ..., 5.0129e-01,\n",
       "           9.4094e-01, 1.3085e-01]],\n",
       "\n",
       "         [[7.2133e-01, 3.5898e-01, 8.1555e-01,  ..., 9.6054e-01,\n",
       "           3.1090e-01, 2.1382e-01],\n",
       "          [8.8668e-01, 6.3698e-01, 3.1000e-02,  ..., 6.6724e-01,\n",
       "           7.6594e-01, 7.7764e-01],\n",
       "          [2.7898e-01, 3.2016e-01, 3.7537e-01,  ..., 1.9791e-01,\n",
       "           2.3911e-01, 3.7106e-02],\n",
       "          [9.4156e-01, 3.6151e-01, 1.6236e-01,  ..., 8.5030e-01,\n",
       "           2.3374e-02, 2.3258e-01]],\n",
       "\n",
       "         [[1.7517e-01, 5.0606e-01, 4.9796e-01,  ..., 5.6913e-02,\n",
       "           4.5341e-01, 9.7966e-01],\n",
       "          [3.5035e-01, 4.3277e-01, 4.4243e-01,  ..., 3.6462e-01,\n",
       "           1.4302e-01, 2.7021e-01],\n",
       "          [7.2502e-01, 9.3978e-01, 6.0146e-01,  ..., 3.4271e-01,\n",
       "           4.0606e-01, 5.0833e-01],\n",
       "          [2.3694e-01, 2.0085e-01, 7.0838e-01,  ..., 6.8203e-01,\n",
       "           1.0371e-02, 2.3818e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[7.8571e-01, 3.6918e-01, 7.6053e-01,  ..., 3.7139e-02,\n",
       "           5.8591e-01, 2.2089e-01],\n",
       "          [7.4813e-01, 4.6997e-02, 4.0007e-01,  ..., 2.6963e-01,\n",
       "           3.7597e-01, 5.3566e-01],\n",
       "          [4.4007e-01, 9.8994e-01, 3.4425e-01,  ..., 5.9653e-01,\n",
       "           7.6506e-01, 5.5754e-01],\n",
       "          [3.1188e-01, 8.3338e-01, 5.3242e-01,  ..., 9.3172e-01,\n",
       "           8.9405e-02, 6.8517e-01]],\n",
       "\n",
       "         [[8.5708e-01, 2.7052e-01, 3.4007e-01,  ..., 6.7920e-01,\n",
       "           1.0232e-01, 3.7891e-01],\n",
       "          [5.3610e-01, 3.5821e-01, 3.0933e-02,  ..., 7.1445e-01,\n",
       "           3.1529e-01, 7.0954e-02],\n",
       "          [8.0063e-01, 1.6188e-01, 2.0169e-02,  ..., 7.2768e-01,\n",
       "           3.8986e-01, 6.7902e-01],\n",
       "          [2.3644e-01, 6.2383e-01, 8.5760e-01,  ..., 4.5782e-01,\n",
       "           5.2194e-01, 3.3367e-02]],\n",
       "\n",
       "         [[7.1315e-01, 7.5948e-01, 7.4919e-01,  ..., 9.0769e-01,\n",
       "           4.6724e-01, 9.6490e-01],\n",
       "          [6.2065e-01, 7.6614e-02, 4.7293e-01,  ..., 9.8876e-02,\n",
       "           4.0422e-01, 1.9955e-01],\n",
       "          [8.7517e-01, 9.0254e-01, 4.2553e-01,  ..., 1.2478e-01,\n",
       "           3.4443e-01, 5.0715e-01],\n",
       "          [7.4380e-02, 1.1758e-01, 2.0503e-01,  ..., 4.6631e-02,\n",
       "           4.8024e-01, 2.5517e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[4.8002e-01, 3.6509e-01, 6.5172e-01,  ..., 7.3152e-01,\n",
       "           7.5910e-01, 5.8697e-01],\n",
       "          [4.7340e-01, 7.2466e-01, 6.6305e-01,  ..., 4.6501e-01,\n",
       "           3.5361e-01, 5.7036e-04],\n",
       "          [3.0084e-01, 1.1266e-01, 9.4507e-01,  ..., 3.7446e-01,\n",
       "           3.9084e-01, 2.8191e-01],\n",
       "          [1.2363e-01, 2.2842e-02, 2.4149e-01,  ..., 1.4548e-01,\n",
       "           6.4006e-01, 9.1778e-01]],\n",
       "\n",
       "         [[5.7506e-01, 3.7441e-01, 9.6422e-01,  ..., 1.5415e-01,\n",
       "           9.1769e-01, 2.0701e-01],\n",
       "          [8.9785e-01, 5.7831e-01, 8.2305e-01,  ..., 3.0070e-01,\n",
       "           3.3981e-01, 8.8117e-01],\n",
       "          [9.5117e-01, 1.6950e-01, 2.4365e-01,  ..., 7.8339e-01,\n",
       "           4.9362e-01, 1.8770e-01],\n",
       "          [4.2391e-01, 2.1381e-01, 5.5311e-01,  ..., 1.7699e-01,\n",
       "           4.1321e-01, 4.8143e-02]],\n",
       "\n",
       "         [[8.8831e-01, 6.4070e-01, 5.9875e-01,  ..., 7.1618e-01,\n",
       "           9.1993e-02, 1.9644e-01],\n",
       "          [9.5068e-01, 3.0561e-01, 2.4299e-01,  ..., 4.0650e-01,\n",
       "           2.2226e-01, 7.8509e-01],\n",
       "          [2.9233e-01, 8.2055e-01, 7.6059e-01,  ..., 7.9204e-01,\n",
       "           8.1962e-01, 1.3057e-02],\n",
       "          [4.0356e-01, 7.0273e-01, 6.8008e-01,  ..., 5.2173e-01,\n",
       "           4.0094e-01, 2.3919e-01]]],\n",
       "\n",
       "\n",
       "        [[[7.2103e-01, 2.6858e-01, 3.6815e-01,  ..., 7.5097e-01,\n",
       "           6.3486e-01, 7.4355e-01],\n",
       "          [8.9202e-01, 1.6973e-01, 1.2703e-01,  ..., 8.8166e-01,\n",
       "           7.5963e-01, 3.3524e-01],\n",
       "          [8.0341e-01, 4.7895e-01, 7.0971e-02,  ..., 2.1517e-02,\n",
       "           3.0657e-03, 6.5414e-01],\n",
       "          [1.3412e-01, 6.2983e-01, 2.9776e-01,  ..., 8.6631e-01,\n",
       "           2.2512e-01, 5.8056e-01]],\n",
       "\n",
       "         [[8.2219e-02, 5.8588e-01, 1.7479e-01,  ..., 8.4860e-01,\n",
       "           4.3882e-01, 3.4598e-01],\n",
       "          [6.7420e-01, 7.8120e-01, 6.5145e-01,  ..., 8.3092e-01,\n",
       "           3.3519e-01, 8.4780e-01],\n",
       "          [9.6590e-01, 7.5578e-01, 4.5916e-01,  ..., 5.5081e-01,\n",
       "           1.8436e-01, 8.9867e-01],\n",
       "          [3.5404e-01, 4.0499e-01, 2.8823e-01,  ..., 9.7694e-02,\n",
       "           6.2107e-01, 1.5186e-01]],\n",
       "\n",
       "         [[6.8831e-01, 6.8520e-01, 8.5112e-01,  ..., 6.6789e-01,\n",
       "           2.5533e-01, 1.5402e-03],\n",
       "          [9.6635e-01, 6.1856e-01, 8.6951e-01,  ..., 2.6447e-02,\n",
       "           1.4033e-01, 1.1826e-01],\n",
       "          [4.9731e-01, 7.0265e-01, 3.0137e-01,  ..., 4.0622e-01,\n",
       "           6.2885e-02, 4.9796e-01],\n",
       "          [2.4112e-01, 1.4264e-01, 3.1277e-01,  ..., 3.9962e-01,\n",
       "           2.5378e-01, 2.4981e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[3.7516e-02, 5.2606e-02, 6.7705e-01,  ..., 9.9845e-01,\n",
       "           9.3017e-01, 2.8764e-01],\n",
       "          [5.7235e-01, 4.9557e-01, 3.2893e-01,  ..., 3.9945e-01,\n",
       "           1.3587e-01, 5.9314e-01],\n",
       "          [4.9079e-01, 9.1040e-01, 5.9735e-01,  ..., 9.3935e-01,\n",
       "           4.0236e-01, 4.5233e-01],\n",
       "          [2.0270e-01, 9.6022e-01, 9.2800e-01,  ..., 3.7325e-02,\n",
       "           5.9096e-02, 8.9018e-01]],\n",
       "\n",
       "         [[4.8781e-01, 1.3847e-01, 7.7540e-02,  ..., 7.0155e-01,\n",
       "           5.1147e-01, 4.1834e-01],\n",
       "          [6.7289e-01, 3.4296e-01, 1.6057e-01,  ..., 1.6746e-01,\n",
       "           5.0621e-02, 3.5642e-01],\n",
       "          [5.2199e-01, 2.8768e-01, 5.8077e-01,  ..., 3.3587e-01,\n",
       "           4.8817e-01, 2.9796e-01],\n",
       "          [5.3658e-01, 7.8131e-01, 1.4626e-01,  ..., 9.0540e-01,\n",
       "           2.1507e-01, 9.4337e-01]],\n",
       "\n",
       "         [[6.1091e-01, 4.8865e-01, 9.8282e-01,  ..., 8.8251e-01,\n",
       "           6.0987e-01, 4.5542e-01],\n",
       "          [4.0998e-01, 5.9416e-01, 5.7109e-01,  ..., 9.1162e-01,\n",
       "           3.4775e-01, 1.5563e-01],\n",
       "          [1.2928e-01, 7.3027e-01, 4.1194e-01,  ..., 2.6616e-01,\n",
       "           2.9634e-01, 8.7486e-02],\n",
       "          [3.2289e-01, 7.2821e-01, 7.8058e-01,  ..., 6.0426e-01,\n",
       "           6.7286e-01, 7.6601e-01]]],\n",
       "\n",
       "\n",
       "        [[[2.6338e-01, 3.0596e-01, 4.2954e-01,  ..., 4.2134e-01,\n",
       "           3.5634e-01, 4.0825e-01],\n",
       "          [8.0316e-02, 6.2472e-01, 1.6564e-01,  ..., 3.0610e-01,\n",
       "           8.2456e-01, 2.8248e-01],\n",
       "          [7.8983e-01, 7.0034e-01, 3.3217e-01,  ..., 2.5622e-01,\n",
       "           6.9386e-01, 4.3846e-01],\n",
       "          [2.4528e-01, 3.7641e-01, 6.6454e-02,  ..., 2.2716e-02,\n",
       "           3.5245e-01, 7.6190e-01]],\n",
       "\n",
       "         [[4.3062e-01, 6.9282e-01, 4.2314e-01,  ..., 8.7704e-01,\n",
       "           1.9732e-01, 6.9908e-01],\n",
       "          [8.5922e-01, 3.7306e-01, 3.3027e-01,  ..., 2.3489e-01,\n",
       "           4.7274e-01, 4.5686e-02],\n",
       "          [2.3115e-01, 9.9013e-01, 7.3400e-02,  ..., 3.2281e-01,\n",
       "           8.3519e-01, 8.7764e-01],\n",
       "          [5.6888e-01, 5.9392e-01, 6.6993e-01,  ..., 8.3389e-01,\n",
       "           1.6878e-01, 7.4963e-01]],\n",
       "\n",
       "         [[9.7800e-02, 9.0549e-01, 8.1382e-01,  ..., 2.7409e-01,\n",
       "           6.1146e-01, 2.4399e-01],\n",
       "          [5.6212e-01, 4.3883e-01, 8.3539e-01,  ..., 5.2330e-01,\n",
       "           7.8437e-01, 4.3735e-01],\n",
       "          [5.2449e-01, 8.3927e-01, 1.7910e-01,  ..., 1.8086e-01,\n",
       "           2.0622e-02, 9.8627e-01],\n",
       "          [2.6454e-01, 7.9051e-01, 5.0862e-01,  ..., 6.0708e-02,\n",
       "           2.5546e-01, 6.9997e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[2.9735e-01, 8.0451e-01, 9.8436e-01,  ..., 7.9022e-01,\n",
       "           6.5505e-01, 6.1582e-01],\n",
       "          [5.3085e-01, 3.5686e-01, 7.6285e-01,  ..., 2.8216e-01,\n",
       "           8.1693e-02, 5.0582e-03],\n",
       "          [1.1521e-01, 6.5700e-01, 3.9259e-01,  ..., 6.5393e-01,\n",
       "           1.2219e-01, 3.5144e-02],\n",
       "          [1.4082e-02, 6.6868e-01, 5.2106e-01,  ..., 5.4776e-01,\n",
       "           1.4289e-01, 7.8343e-01]],\n",
       "\n",
       "         [[8.6125e-01, 5.8985e-01, 6.0114e-01,  ..., 9.8883e-01,\n",
       "           3.3441e-01, 9.0560e-02],\n",
       "          [4.4762e-01, 2.2890e-01, 7.8180e-01,  ..., 4.7868e-01,\n",
       "           1.8242e-01, 4.4054e-02],\n",
       "          [4.4176e-01, 1.2743e-01, 2.9034e-01,  ..., 7.8453e-01,\n",
       "           8.8603e-01, 1.8779e-01],\n",
       "          [6.8445e-01, 6.2473e-02, 9.9332e-01,  ..., 3.9729e-01,\n",
       "           1.7063e-01, 5.3126e-01]],\n",
       "\n",
       "         [[7.1083e-01, 5.1877e-01, 4.7568e-01,  ..., 8.3491e-01,\n",
       "           1.6062e-01, 7.5649e-01],\n",
       "          [9.1964e-01, 5.8404e-01, 1.5585e-01,  ..., 3.1679e-01,\n",
       "           7.3117e-01, 9.7448e-01],\n",
       "          [2.6060e-01, 8.2009e-01, 1.1575e-01,  ..., 4.1799e-01,\n",
       "           7.1490e-01, 9.6195e-01],\n",
       "          [8.3549e-01, 3.1495e-01, 3.9119e-02,  ..., 7.4286e-01,\n",
       "           6.9884e-01, 1.7791e-02]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.view(batch_size, seq_len, num_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 4, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.view(batch_size, seq_len, num_heads, head_dim).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_linear = nn.Linear(emb_dim, num_heads * head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_hat = Q_linear(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads * head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 4, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_hat.view(batch_size, seq_len, num_heads, head_dim).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.rand(batch_size, seq_len, num_heads * head_dim)\n",
    "Q_linear = nn.Linear(emb_dim, num_heads * head_dim)\n",
    "Q_hat = Q_linear(Q)\n",
    "\n",
    "Q_hat = Q_hat.view(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "\n",
    "K = torch.rand(batch_size, seq_len, num_heads * head_dim)\n",
    "K_linear = nn.Linear(emb_dim, num_heads * head_dim)\n",
    "K_hat = K_linear(K)\n",
    "K_hat = K.view(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "\n",
    "key_out = torch.matmul(Q_hat, K_hat.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 32, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_hat.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 4, 32])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0909, 0.6132, 0.8577,  ..., 0.8918, 0.1224, 0.3543],\n",
       "         [0.7814, 0.1341, 0.1922,  ..., 0.5481, 0.2141, 0.4154],\n",
       "         [0.8447, 0.7150, 0.4266,  ..., 0.3919, 0.6560, 0.6337],\n",
       "         ...,\n",
       "         [0.2432, 0.4523, 0.3986,  ..., 0.9314, 0.6383, 0.5309],\n",
       "         [0.4871, 0.3860, 0.2546,  ..., 0.0959, 0.9028, 0.9500],\n",
       "         [0.4697, 0.7216, 0.0304,  ..., 0.7844, 0.2969, 0.1696]],\n",
       "\n",
       "        [[0.9752, 0.4729, 0.5563,  ..., 0.9844, 0.1385, 0.8637],\n",
       "         [0.8090, 0.1349, 0.0985,  ..., 0.7581, 0.6185, 0.0483],\n",
       "         [0.0466, 0.5107, 0.6249,  ..., 0.0528, 0.6865, 0.6051],\n",
       "         ...,\n",
       "         [0.2516, 0.3157, 0.0164,  ..., 0.7983, 0.2466, 0.9173],\n",
       "         [0.6927, 0.2813, 0.5569,  ..., 0.6450, 0.2963, 0.7299],\n",
       "         [0.6675, 0.6297, 0.7989,  ..., 0.9333, 0.2770, 0.3121]],\n",
       "\n",
       "        [[0.8325, 0.5633, 0.5766,  ..., 0.1106, 0.3868, 0.6065],\n",
       "         [0.3926, 0.4704, 0.4585,  ..., 0.8406, 0.2138, 0.1335],\n",
       "         [0.1487, 0.6410, 0.3817,  ..., 0.7092, 0.0149, 0.6156],\n",
       "         ...,\n",
       "         [0.6742, 0.8740, 0.5132,  ..., 0.0331, 0.6500, 0.0041],\n",
       "         [0.2100, 0.6430, 0.4730,  ..., 0.5539, 0.6983, 0.8826],\n",
       "         [0.0202, 0.8264, 0.1440,  ..., 0.4009, 0.3089, 0.4573]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0922, 0.3189, 0.9513,  ..., 0.4450, 0.8759, 0.7034],\n",
       "         [0.5178, 0.3842, 0.1555,  ..., 0.3402, 0.1769, 0.7243],\n",
       "         [0.5523, 0.3283, 0.6321,  ..., 0.6147, 0.9304, 0.0634],\n",
       "         ...,\n",
       "         [0.0302, 0.9796, 0.8236,  ..., 0.3941, 0.5435, 0.2760],\n",
       "         [0.3188, 0.2829, 0.1616,  ..., 0.7316, 0.9499, 0.7733],\n",
       "         [0.7400, 0.2483, 0.8837,  ..., 0.7233, 0.5974, 0.9475]],\n",
       "\n",
       "        [[0.8215, 0.8367, 0.3471,  ..., 0.7927, 0.9720, 0.6911],\n",
       "         [0.4068, 0.4554, 0.0338,  ..., 0.9935, 0.6009, 0.8946],\n",
       "         [0.2293, 0.6871, 0.3156,  ..., 0.4151, 0.4312, 0.4273],\n",
       "         ...,\n",
       "         [0.4838, 0.3924, 0.3467,  ..., 0.6151, 0.9744, 0.5227],\n",
       "         [0.2529, 0.9020, 0.1678,  ..., 0.5789, 0.8439, 0.1959],\n",
       "         [0.9985, 0.6725, 0.1695,  ..., 0.5732, 0.3164, 0.3745]],\n",
       "\n",
       "        [[0.9269, 0.1440, 0.4351,  ..., 0.5325, 0.0213, 0.9433],\n",
       "         [0.8430, 0.2851, 0.8580,  ..., 0.0641, 0.3904, 0.5671],\n",
       "         [0.1265, 0.0501, 0.6457,  ..., 0.3226, 0.4006, 0.0758],\n",
       "         ...,\n",
       "         [0.5077, 0.0787, 0.7995,  ..., 0.1617, 0.7315, 0.8725],\n",
       "         [0.0523, 0.3853, 0.5684,  ..., 0.6388, 0.1866, 0.8443],\n",
       "         [0.0751, 0.7977, 0.1686,  ..., 0.7785, 0.5428, 0.2955]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.transpose(-2,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\n",
    "    [\"I\", \"love\", \"deep\", \"learning\", \"<PAD>\"],  # Sentence 1 (padded to length 5)\n",
    "    [\"Transformers\", \"are\", \"powerful\", \"<PAD>\", \"<PAD>\"],  # Sentence 2\n",
    "    [\"PyTorch\", \"is\", \"amazing\", \"<PAD>\", \"<PAD>\"]  # Sentence 3\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "torch.tensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.tensor([\n",
    "    [1, 2, 3, 4, 0],  # Sentence 1\n",
    "    [5, 6, 7, 0, 0],  # Sentence 2\n",
    "    [8, 9, 10, 0, 0]  # Sentence 3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len = batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(seq_len) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2, 3, 4]), tensor([0, 1, 2, 3, 4]), tensor([0, 1, 2, 3, 4])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.arange(seq_len).squeeze() for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(seq_len).unsqueeze(0).expand(batch_size, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(seq_len).expand(batch_size, seq_len) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(seq_len).expand(batch_size, seq_len) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(seq_len).expand(3, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between permute and view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "Shape: torch.Size([2, 3, 4])\n",
      "\n",
      "Viewed Tensor (reshaped using .view):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19],\n",
      "        [20, 21, 22, 23]])\n",
      "Shape: torch.Size([6, 4])\n",
      "\n",
      "Permuted Tensor (dimensions reordered):\n",
      "tensor([[[ 0, 12],\n",
      "         [ 1, 13],\n",
      "         [ 2, 14],\n",
      "         [ 3, 15]],\n",
      "\n",
      "        [[ 4, 16],\n",
      "         [ 5, 17],\n",
      "         [ 6, 18],\n",
      "         [ 7, 19]],\n",
      "\n",
      "        [[ 8, 20],\n",
      "         [ 9, 21],\n",
      "         [10, 22],\n",
      "         [11, 23]]])\n",
      "Shape: torch.Size([3, 4, 2])\n",
      "\n",
      "Tensor Elements in Memory:\n",
      "Original Tensor Flattened: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "Viewed Tensor Flattened: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "Permuted Tensor Flattened: tensor([ 0, 12,  1, 13,  2, 14,  3, 15,  4, 16,  5, 17,  6, 18,  7, 19,  8, 20,\n",
      "         9, 21, 10, 22, 11, 23])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 3D tensor\n",
    "tensor = torch.arange(24).reshape(2, 3, 4)  # Shape: (2, 3, 4)\n",
    "print(\"Original Tensor:\")\n",
    "print(tensor)\n",
    "print(\"Shape:\", tensor.shape)\n",
    "\n",
    "# Using .view to reshape (keeps original dimension order)\n",
    "viewed_tensor = tensor.view(6, 4)  # Shape: (6, 4)\n",
    "print(\"\\nViewed Tensor (reshaped using .view):\")\n",
    "print(viewed_tensor)\n",
    "print(\"Shape:\", viewed_tensor.shape)\n",
    "# NOTE: The dimension order (2, 3, 4) remains intact, just reshaped to (6, 4).\n",
    "\n",
    "# Using .permute to change dimension order\n",
    "permuted_tensor = tensor.permute(1, 2, 0)  # Shape: (3, 4, 2)\n",
    "print(\"\\nPermuted Tensor (dimensions reordered):\")\n",
    "print(permuted_tensor)\n",
    "print(\"Shape:\", permuted_tensor.shape)\n",
    "# NOTE: The order of dimensions has been rearranged: (2, 3, 4) -> (3, 4, 2)\n",
    "\n",
    "# Comparing the data layout\n",
    "print(\"\\nTensor Elements in Memory:\")\n",
    "print(\"Original Tensor Flattened:\", tensor.flatten())\n",
    "print(\"Viewed Tensor Flattened:\", viewed_tensor.flatten())\n",
    "print(\"Permuted Tensor Flattened:\", permuted_tensor.flatten())\n",
    "# NOTE: The .view tensor retains the same data order in memory,\n",
    "# while .permute alters how the data is accessed (new dimension order).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0],\n",
       "         [ 1]],\n",
       "\n",
       "        [[ 2],\n",
       "         [ 3]],\n",
       "\n",
       "        [[ 4],\n",
       "         [ 5]],\n",
       "\n",
       "        [[ 6],\n",
       "         [ 7]],\n",
       "\n",
       "        [[ 8],\n",
       "         [ 9]],\n",
       "\n",
       "        [[10],\n",
       "         [11]],\n",
       "\n",
       "        [[12],\n",
       "         [13]],\n",
       "\n",
       "        [[14],\n",
       "         [15]],\n",
       "\n",
       "        [[16],\n",
       "         [17]],\n",
       "\n",
       "        [[18],\n",
       "         [19]],\n",
       "\n",
       "        [[20],\n",
       "         [21]],\n",
       "\n",
       "        [[22],\n",
       "         [23]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.view(12, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvBjk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
