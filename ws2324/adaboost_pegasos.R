pegasos_linear <- function(
    y,
    X,
    nr_iter = 50000,
    theta = rnorm(ncol(X)),
    lambda = 1,
    alpha = 0.01)
{
  t <- 1
  n <- NROW(y)
  while(t <= nr_iter){
    f_current = X%*%theta
    i <- sample(1:n, 1)
    # update
    theta <- (1 - lambda * alpha) * theta
    # add second term if within margin
    if(y[i]*f_current[i] < 1) theta <- theta + alpha * y[i]*X[i,]
    t <- t + 1
  }
  return(theta)
}

## Check on a simple example
## --------------------------------------------
set.seed(2L)
C = 1
library(mlbench)
library(kernlab)
data = mlbench.twonorm(n = 100, d = 2)
data = as.data.frame(data)
X = as.matrix(data[, 1:2])
y = data$classes
par(mar = c(5,4,4,6))
plot(x = data$x.1, y = data$x.2, pch = ifelse(data$classes == 1, "-", "+"), col = "black",
     xlab = "x1", ylab = "x2")
# recode y
y = ifelse(y == "2", 1, -1)
mod_pegasos = pegasos_linear(y, cbind(1,X), lambda = 1/(C*NROW(y)))
# Add estimated decision boundary:
abline(a = - mod_pegasos[1] / mod_pegasos[3],
       b = - mod_pegasos[2] / mod_pegasos[3], col = "#D55E00")
# Compare to logistic regression:
mod_logreg = glm(classes ~ ., data = data, family = binomial())
abline(a = - coef(mod_logreg)[1] / coef(mod_logreg)[3],
       b = - coef(mod_logreg)[2] / coef(mod_logreg)[3], col = "#56B4E9",
       lty = 3, lwd = 2)
# decision values
f_pegasos = cbind(1,X) %*% mod_pegasos
# How many wrong classified examples?
table(sign(f_pegasos * y))

## compare to kernlab. we CANNOT expect a PERFECT match
## -------------------------------------------------------------
mod_kernlab = ksvm(classes~.,
                   data = data,
                   kernel = "vanilladot",
                   C = C,
                   kpar = list(),
                   scaled = FALSE)
f_kernlab = predict(mod_kernlab, newdata = data, type = "decision")
# How many wrong classified examples?
table(sign(f_kernlab * y))

# compare outputs
print(range(abs(f_kernlab - f_pegasos)))
## [1] 0.00014996 0.38049736

# compare coeffs
# (b is negative offset)
all_params = rbind(
  mod_pegasos,
  mod_kernlab = c(-mod_kernlab@b,
                  (params <- colSums(X[mod_kernlab@SVindex, ] *
                                       mod_kernlab@alpha[[1]] *
                                       y[mod_kernlab@SVindex])))
)
all_params

# compare empirical risks
emp_risk <- function(theta){
  f = cbind(1, X) %*% theta
  return(0.5 * sum(theta[2:3]^2) + C*sum(ifelse(f > 0, f, 0)))
}
apply(all_params, 1, emp_risk)

params = all_params[2,]
# recompute margin
margin = 1 / sqrt(sum(params[2:3]^2))

# compute value of intercept shift (the margin shift is in orthogonal direction
# to the decision boundary, so this has to be transformed first)
m = - params[2] / params[3]
t_0 = margin / (cos(atan(m)))

# add margins to visualization:
#
abline(a = -params[1] / params[3],
       b = m, col = "#0072B2")
abline(a = -params[1] / params[3] + t_0,
       b = m, col = "#0072B2", lty = 2)
abline(a = -params[1] / params[3] - t_0,
       b = m, col = "#0072B2", lty = 2)

# find support vectors on decision border
nv_sv_idx = y[mod_kernlab@SVindex] * f_kernlab[mod_kernlab@SVindex] > 0.98
points(X[mod_kernlab@SVindex[nv_sv_idx],])

# add legends
legend(par("usr")[2], par("usr")[4], bty="n", xpd=NA, legend=c("1","2"),
       pch=c("-","+"), title="Classes", cex = 0.8)
legend(par("usr")[2], 1.8, bty="n", xpd=NA,legend=c("Pegasos","Logistic","Kernlab","Margin"),
       lty=c(1,3,1,2),
       col = c("#D55E00","#56B4E9","#0072B2","#0072B2"),
       title="", cex = 0.8, lwd = c(1,2,1,1))
