{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial pi = [0.33 0.33 0.34]\n",
      "Initial means = [1.76405235 0.40015721 0.97873798]\n",
      "Initial variances = [2.2408932  1.86755799 0.97727788]\n",
      "Iteration 0: Log-Likelihood = -3266.579099343927\n",
      "Iteration 10: Log-Likelihood = -2409.2214826353297\n",
      "Iteration 20: Log-Likelihood = -2377.4324818959367\n",
      "Iteration 30: Log-Likelihood = -2376.8694994274974\n",
      "Iteration 40: Log-Likelihood = -2380.1648494097135\n",
      "Iteration 50: Log-Likelihood = -2384.5879230728483\n",
      "Iteration 60: Log-Likelihood = -2388.076182417474\n",
      "Iteration 70: Log-Likelihood = -2388.495702547733\n",
      "Iteration 80: Log-Likelihood = -2382.121412942661\n",
      "Iteration 90: Log-Likelihood = -2359.2596615120397\n",
      "Iteration 100: Log-Likelihood = -2293.5990713997885\n",
      "Iteration 110: Log-Likelihood = -2184.8598251432795\n",
      "Iteration 120: Log-Likelihood = -2122.211973038834\n",
      "Iteration 130: Log-Likelihood = -2088.7053539622357\n",
      "Iteration 140: Log-Likelihood = -2070.210675112268\n",
      "Iteration 150: Log-Likelihood = -2064.6127001476625\n",
      "Iteration 160: Log-Likelihood = -2065.605841736371\n",
      "Iteration 170: Log-Likelihood = -2068.115981660718\n",
      "Iteration 180: Log-Likelihood = -2070.241561673262\n",
      "Iteration 190: Log-Likelihood = -2071.687351744817\n",
      "Iteration 200: Log-Likelihood = -2072.600679657195\n",
      "Iteration 210: Log-Likelihood = -2073.167692892217\n",
      "Iteration 220: Log-Likelihood = -2073.525020412386\n",
      "Iteration 230: Log-Likelihood = -2073.7591916237225\n",
      "Iteration 240: Log-Likelihood = -2073.921782357305\n",
      "Iteration 250: Log-Likelihood = -2074.0426661625834\n",
      "Iteration 260: Log-Likelihood = -2074.138904013956\n",
      "Iteration 270: Log-Likelihood = -2074.2201768609048\n",
      "Iteration 280: Log-Likelihood = -2074.2919688509114\n",
      "Iteration 290: Log-Likelihood = -2074.357393771797\n",
      "Iteration 300: Log-Likelihood = -2074.418231493931\n",
      "Iteration 310: Log-Likelihood = -2074.475512610577\n",
      "Iteration 320: Log-Likelihood = -2074.5298471365622\n",
      "Iteration 330: Log-Likelihood = -2074.581608960602\n",
      "Iteration 340: Log-Likelihood = -2074.631039229034\n",
      "Iteration 350: Log-Likelihood = -2074.678304238378\n",
      "Iteration 360: Log-Likelihood = -2074.723527823793\n",
      "Iteration 370: Log-Likelihood = -2074.7668094579717\n",
      "Iteration 380: Log-Likelihood = -2074.808234348263\n",
      "Iteration 390: Log-Likelihood = -2074.8478790563922\n",
      "Iteration 400: Log-Likelihood = -2074.8858146156745\n",
      "Iteration 410: Log-Likelihood = -2074.9221082519484\n",
      "Iteration 420: Log-Likelihood = -2074.956824327792\n",
      "Iteration 430: Log-Likelihood = -2074.990024856674\n",
      "Iteration 440: Log-Likelihood = -2075.021769780848\n",
      "Iteration 450: Log-Likelihood = -2075.0521171210653\n",
      "Iteration 460: Log-Likelihood = -2075.0811230582676\n",
      "Iteration 470: Log-Likelihood = -2075.1088419804832\n",
      "Iteration 480: Log-Likelihood = -2075.1353265131743\n",
      "Iteration 490: Log-Likelihood = -2075.1606275428285\n",
      "Iteration 500: Log-Likelihood = -2075.1847942390505\n",
      "Iteration 510: Log-Likelihood = -2075.2078740776515\n",
      "Iteration 520: Log-Likelihood = -2075.22991286596\n",
      "Iteration 530: Log-Likelihood = -2075.250954770729\n",
      "Iteration 540: Log-Likelihood = -2075.2710423486506\n",
      "Iteration 550: Log-Likelihood = -2075.290216579217\n",
      "Iteration 560: Log-Likelihood = -2075.3085168997623\n",
      "Iteration 570: Log-Likelihood = -2075.3259812421566\n",
      "Iteration 580: Log-Likelihood = -2075.342646070969\n",
      "Iteration 590: Log-Likelihood = -2075.358546422775\n",
      "Iteration 600: Log-Likelihood = -2075.3737159462207\n",
      "Iteration 610: Log-Likelihood = -2075.3881869426937\n",
      "Iteration 620: Log-Likelihood = -2075.401990407286\n",
      "Iteration 630: Log-Likelihood = -2075.4151560699042\n",
      "Iteration 640: Log-Likelihood = -2075.4277124362493\n",
      "Iteration 650: Log-Likelihood = -2075.439686828607\n",
      "Iteration 660: Log-Likelihood = -2075.4511054263426\n",
      "Iteration 670: Log-Likelihood = -2075.461993305689\n",
      "Iteration 680: Log-Likelihood = -2075.472374479194\n",
      "Iteration 690: Log-Likelihood = -2075.4822719343083\n",
      "Iteration 700: Log-Likelihood = -2075.49170767132\n",
      "Iteration 710: Log-Likelihood = -2075.500702740403\n",
      "Iteration 720: Log-Likelihood = -2075.509277277872\n",
      "Iteration 730: Log-Likelihood = -2075.517450541461\n",
      "Iteration 740: Log-Likelihood = -2075.5252409447417\n",
      "Iteration 750: Log-Likelihood = -2075.532666090514\n",
      "Iteration 760: Log-Likelihood = -2075.539742803294\n",
      "Iteration 770: Log-Likelihood = -2075.5464871607564\n",
      "Iteration 780: Log-Likelihood = -2075.5529145242594\n",
      "Iteration 790: Log-Likelihood = -2075.5590395682852\n",
      "Iteration 800: Log-Likelihood = -2075.5648763090453\n",
      "Iteration 810: Log-Likelihood = -2075.570438131942\n",
      "Iteration 820: Log-Likelihood = -2075.5757378182197\n",
      "Iteration 830: Log-Likelihood = -2075.580787570589\n",
      "Iteration 840: Log-Likelihood = -2075.5855990379246\n",
      "Iteration 850: Log-Likelihood = -2075.590183339056\n",
      "Iteration 860: Log-Likelihood = -2075.594551085708\n",
      "Iteration 870: Log-Likelihood = -2075.598712404506\n",
      "Iteration 880: Log-Likelihood = -2075.602676958117\n",
      "Iteration 890: Log-Likelihood = -2075.606453965684\n",
      "Iteration 900: Log-Likelihood = -2075.610052222295\n",
      "Converged after 904 iterations\n",
      "pi = [0.25791763 0.3464511  0.39563127]\n",
      "means = [4.57873067 2.97930372 1.38670006]\n",
      "variances = [0.27068157 0.32158965 0.7313457 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "\n",
    "# Define the normal distribution PDF\n",
    "def normal_pdf(x, mean, var):\n",
    "    return (1 / np.sqrt(2 * np.pi * var)) * np.exp(- (x - mean) ** 2 / (2 * var))\n",
    "\n",
    "# Initialize parameters\n",
    "np.random.seed(0)  # For reproducibility\n",
    "n = 1000  # Number of data points\n",
    "G = 3  # Number of components\n",
    "\n",
    "# Mixture weights (initial guess)\n",
    "pi = np.array([0.33, 0.33, 0.34])\n",
    "\n",
    "# Means and variances for each component (initial guess) drawn from a positive normal distribution\n",
    "means = np.random.normal(0, 1, G)\n",
    "variances = np.abs(np.random.normal(0, 1, G))\n",
    "\n",
    "# Generate synthetic data\n",
    "true_means = [1.5, 3, 4.5]\n",
    "true_variances = [1, 0.25, 0.36]\n",
    "true_pi = [0.45, 0.26, 0.29]\n",
    "\n",
    "# Assign data points to components\n",
    "z = np.random.choice(G, size=n, p=true_pi)\n",
    "y = np.array([np.random.normal(true_means[zi], np.sqrt(true_variances[zi])) for zi in z])\n",
    "\n",
    "def e_step(y, pi, means, variances):\n",
    "    n = len(y)\n",
    "    G = len(pi)\n",
    "    responsibilities = np.zeros((n, G))\n",
    "\n",
    "    for i in range(n):\n",
    "        for g in range(G):\n",
    "            responsibilities[i, g] = pi[g] * normal_pdf(y[i], means[g], variances[g])\n",
    "        responsibilities[i, :] /= np.sum(responsibilities[i, :])\n",
    "    \n",
    "    return responsibilities\n",
    "\n",
    "def m_step(y, responsibilities):\n",
    "    n, G = responsibilities.shape\n",
    "    N_k = np.sum(responsibilities, axis=0)\n",
    "    \n",
    "    pi_new = N_k / n\n",
    "    means_new = np.sum(responsibilities * y[:, np.newaxis], axis=0) / N_k\n",
    "    variances_new = np.zeros(G)\n",
    "    \n",
    "    for g in range(G):\n",
    "        variances_new[g] = np.sum(responsibilities[:, g] * (y - means_new[g])**2) / N_k[g]\n",
    "    \n",
    "    return pi_new, means_new, variances_new\n",
    "\n",
    "def calculate_expected_complete_log_likelihood(y, responsibilities, means, variances, pi):\n",
    "    n, G = responsibilities.shape\n",
    "    log_likelihood = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        for g in range(G):\n",
    "            log_likelihood += responsibilities[i, g] * (np.log(pi[g]) + np.log(normal_pdf(y[i], means[g], variances[g])))\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "def em_algorithm(y, pi, means, variances, max_iter=100, tol=1e-6):\n",
    "    # Print the initial parameters\n",
    "    print(f\"Initial pi = {pi}\")\n",
    "    print(f\"Initial means = {means}\")\n",
    "    print(f\"Initial variances = {variances}\")\n",
    "    for iteration in range(max_iter):\n",
    "        # E-Step\n",
    "        responsibilities = e_step(y, pi, means, variances)\n",
    "        \n",
    "        # Calculate the expected complete log-likelihood\n",
    "        log_likelihood = calculate_expected_complete_log_likelihood(y, responsibilities, means, variances, pi)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}: Log-Likelihood = {log_likelihood}\")\n",
    "        \n",
    "            # check if log-likelihood is improved at least by 5 percent\n",
    "            if iteration > 0 and (log_likelihood - log_likelihood_prev) / np.abs(log_likelihood_prev) < -0.05:\n",
    "                # print the improvment rate\n",
    "                print(f\"Improvement rate = {(log_likelihood - log_likelihood_prev) / np.abs(log_likelihood_prev)}\")\n",
    "                \n",
    "        \n",
    "        log_likelihood_prev = log_likelihood\n",
    "        # M-Step\n",
    "        pi_new, means_new, variances_new = m_step(y, responsibilities)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.allclose(pi, pi_new, atol=tol) and np.allclose(means, means_new, atol=tol) and np.allclose(variances, variances_new, atol=tol):\n",
    "            print(f\"Converged after {iteration} iterations\")\n",
    "            # print the converged parameters\n",
    "            print(f\"pi = {pi_new}\")\n",
    "            print(f\"means = {means_new}\")\n",
    "            print(f\"variances = {variances_new}\")\n",
    "            break\n",
    "        \n",
    "        pi, means, variances = pi_new, means_new, variances_new\n",
    "    \n",
    "    return pi, means, variances, responsibilities\n",
    "\n",
    "# Run the EM algorithm\n",
    "pi_est, means_est, variances_est, responsibilities_est = em_algorithm(y, pi, means, variances, max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Initial pi = [0.33 0.33 0.34]\n",
      "Initial means = [0.40015721 0.97873798 1.76405235]\n",
      "Initial variances = [0.97727788 1.86755799 2.2408932 ]\n",
      "-----------------------------------\n",
      "Estimated mixture weights: [0.257917   0.34645448 0.39562852]\n",
      "Estimated means: [1.38669275 2.9793012  4.57873216]\n",
      "Estimated variances: [0.2706809  0.32159385 0.73134055]\n",
      "-----------------------------------\n",
      "True mixture weights: [0.26 0.29 0.45]\n",
      "True means: [1.5 3.  4.5]\n",
      "True variances: [0.25 0.36 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Print sort initial and estimated parameters\n",
    "print('-----------------------------------')\n",
    "print(f\"Initial pi = {np.sort(pi)}\")\n",
    "print(f\"Initial means = {np.sort(means)}\")\n",
    "print(f\"Initial variances = {np.sort(variances)}\")\n",
    "# Print estimated parameters\n",
    "print('-----------------------------------')\n",
    "print(f\"Estimated mixture weights: {np.sort(pi_est)}\")\n",
    "print(f\"Estimated means: {np.sort(means_est)}\")\n",
    "print(f\"Estimated variances: {np.sort(variances_est)}\")\n",
    "print('-----------------------------------')\n",
    "print(f\"True mixture weights: {np.sort(true_pi)}\")\n",
    "print(f\"True means: {np.sort(true_means)}\")\n",
    "print(f\"True variances: {np.sort(true_variances)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Log-Likelihood = -5953.014927521345\n",
      "Iteration 10: Log-Likelihood = -2129.5893905976045\n",
      "Iteration 20: Log-Likelihood = -2153.922766183279\n",
      "Iteration 30: Log-Likelihood = -2174.8955020871113\n",
      "Iteration 40: Log-Likelihood = -2194.3266114534085\n",
      "Iteration 50: Log-Likelihood = -2211.945342159297\n",
      "Iteration 60: Log-Likelihood = -2226.0333200816285\n",
      "Iteration 70: Log-Likelihood = -2233.6621150031588\n",
      "Iteration 80: Log-Likelihood = -2234.726655462529\n",
      "Iteration 90: Log-Likelihood = -2231.433391791867\n"
     ]
    }
   ],
   "source": [
    "means = np.random.normal(0, 1, G)\n",
    "variances = np.abs(np.random.normal(0, 1, G))\n",
    "tol = 1e-6\n",
    "# Manually calculate the responsibilities\n",
    "# logging\n",
    "resp_dict = {}\n",
    "log_likelihood_dict = {}\n",
    "\n",
    "for iteration in range(100):\n",
    "        # E-Step\n",
    "        responsibilities = e_step(y, pi, means, variances)\n",
    "        \n",
    "        # Calculate the expected complete log-likelihood\n",
    "        log_likelihood = calculate_expected_complete_log_likelihood(y, responsibilities, means, variances, pi)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}: Log-Likelihood = {log_likelihood}\")\n",
    "        \n",
    "            # check if log-likelihood is improved at least by 5 percent\n",
    "            if iteration > 0 and (log_likelihood - log_likelihood_prev) / np.abs(log_likelihood_prev) < -0.05:\n",
    "                # print the improvment rate\n",
    "                print(f\"Improvement rate = {(log_likelihood - log_likelihood_prev) / np.abs(log_likelihood_prev)}\")\n",
    "                \n",
    "        \n",
    "        log_likelihood_prev = log_likelihood\n",
    "        # M-Step\n",
    "        pi_new, means_new, variances_new = m_step(y, responsibilities)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.allclose(pi, pi_new, atol=tol) and np.allclose(means, means_new, atol=tol) and np.allclose(variances, variances_new, atol=tol):\n",
    "            print(f\"Converged after {iteration} iterations\")\n",
    "            # print the converged parameters\n",
    "            print(f\"pi = {pi_new}\")\n",
    "            print(f\"means = {means_new}\")\n",
    "            print(f\"variances = {variances_new}\")\n",
    "            break\n",
    "        \n",
    "        pi, means, variances = pi_new, means_new, variances_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.00984197e-01, 2.37579552e-01, 6.14362519e-02],\n",
       "       [8.63562193e-03, 4.24857572e-15, 9.91364378e-01],\n",
       "       [3.35172113e-02, 1.57830004e-11, 9.66482789e-01],\n",
       "       ...,\n",
       "       [1.32545968e-01, 5.21021378e-08, 8.67453979e-01],\n",
       "       [5.87806498e-01, 3.80322817e-01, 3.18706849e-02],\n",
       "       [5.01647779e-04, 4.17024595e-23, 9.99498352e-01]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responsibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responsibilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Log-Likelihood = -245.28506402383815\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "\n",
    "# Define the normal distribution PDF using sympy\n",
    "def normal_pdf_sympy(x, mean, var):\n",
    "    return (1 / sp.sqrt(2 * sp.pi * var)) * sp.exp(- (x - mean)**2 / (2 * var))\n",
    "\n",
    "# Define the normal distribution PDF using numpy\n",
    "def normal_pdf_numpy(x, mean, var):\n",
    "    return (1 / np.sqrt(2 * np.pi * var)) * np.exp(- (x - mean) ** 2 / (2 * var))\n",
    "\n",
    "# Initialize parameters\n",
    "np.random.seed(0)  # For reproducibility\n",
    "n = 100  # Number of data points\n",
    "G = 3  # Number of components\n",
    "\n",
    "# Mixture weights (initial guess)\n",
    "pi = np.array([0.33, 0.33, 0.34])\n",
    "\n",
    "# Means and variances for each component (initial guess)\n",
    "means = np.array([1.0, 3.0, 5.0])\n",
    "variances = np.array([1.0, 1.0, 1.0])\n",
    "\n",
    "# Generate synthetic data\n",
    "true_means = [1.5, 3, 4.5]\n",
    "true_variances = [1, 0.25, 0.36]\n",
    "true_pi = [0.33, 0.33, 0.34]\n",
    "\n",
    "# Assign data points to components\n",
    "z = np.random.choice(G, size=n, p=true_pi)\n",
    "y = np.array([np.random.normal(true_means[zi], np.sqrt(true_variances[zi])) for zi in z])\n",
    "\n",
    "# E-Step: Calculate responsibilities\n",
    "def e_step(y, pi, means, variances):\n",
    "    n = len(y)\n",
    "    G = len(pi)\n",
    "    responsibilities = np.zeros((n, G))\n",
    "\n",
    "    for i in range(n):\n",
    "        for g in range(G):\n",
    "            responsibilities[i, g] = pi[g] * normal_pdf_numpy(y[i], means[g], variances[g])\n",
    "        responsibilities[i, :] /= np.sum(responsibilities[i, :])  # Normalize responsibilities\n",
    "    \n",
    "    return responsibilities\n",
    "\n",
    "# Calculate expected complete log-likelihood\n",
    "def calculate_expected_complete_log_likelihood(y, responsibilities, means, variances, pi):\n",
    "    n, G = responsibilities.shape\n",
    "    log_likelihood = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        for g in range(G):\n",
    "            log_likelihood += responsibilities[i, g] * (np.log(pi[g]) + np.log(normal_pdf_numpy(y[i], means[g], variances[g])))\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "# Symbolic differentiation using sympy\n",
    "def symbolic_m_step(y, responsibilities):\n",
    "    n, G = responsibilities.shape\n",
    "    N_k = np.sum(responsibilities, axis=0)  # Effective number of points assigned to each component\n",
    "    \n",
    "    # Initialize new parameters\n",
    "    pi_new = N_k / n\n",
    "    \n",
    "    # Define symbols for mean and variance\n",
    "    mu, sigma2, x = sp.symbols('mu sigma2 x')\n",
    "    \n",
    "    # Log of the normal PDF using sympy\n",
    "    log_normal_pdf = sp.log((1 / sp.sqrt(2 * sp.pi * sigma2)) * sp.exp(- (x - mu)**2 / (2 * sigma2)))\n",
    "    \n",
    "    means_new = np.zeros(G)\n",
    "    variances_new = np.zeros(G)\n",
    "    \n",
    "    for g in range(G):\n",
    "        # Calculate means\n",
    "        Q_mu = sum(responsibilities[i, g] * log_normal_pdf.subs({x: y[i], sigma2: variances[g]}) for i in range(n))\n",
    "        dQ_mu = sp.diff(Q_mu, mu)\n",
    "        means_new[g] = float(sp.solve(dQ_mu, mu)[0])\n",
    "        \n",
    "        # Calculate variances\n",
    "        Q_sigma2 = sum(responsibilities[i, g] * log_normal_pdf.subs({x: y[i], mu: means_new[g]}) for i in range(n))\n",
    "        dQ_sigma2 = sp.diff(Q_sigma2, sigma2)\n",
    "        variances_new[g] = float(sp.solve(dQ_sigma2, sigma2)[0])\n",
    "    \n",
    "    return pi_new, means_new, variances_new\n",
    "\n",
    "# EM Algorithm\n",
    "def em_algorithm(y, pi, means, variances, max_iter=100, tol=1e-6):\n",
    "    for iteration in range(max_iter):\n",
    "        # E-Step\n",
    "        responsibilities = e_step(y, pi, means, variances)\n",
    "        \n",
    "        # Calculate the expected complete log-likelihood\n",
    "        log_likelihood = calculate_expected_complete_log_likelihood(y, responsibilities, means, variances, pi)\n",
    "        print(f\"Iteration {iteration}: Log-Likelihood = {log_likelihood}\")\n",
    "        \n",
    "        # M-Step using symbolic differentiation\n",
    "        pi_new, means_new, variances_new = symbolic_m_step(y, responsibilities)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.allclose(pi, pi_new, atol=tol) and np.allclose(means, means_new, atol=tol) and np.allclose(variances, variances_new, atol=tol):\n",
    "            break\n",
    "        \n",
    "        # Update parameters for the next iteration\n",
    "        pi, means, variances = pi_new, means_new, variances_new\n",
    "    \n",
    "    return pi, means, variances, responsibilities\n",
    "\n",
    "# Run the EM algorithm\n",
    "pi_est, means_est, variances_est, responsibilities_est = em_algorithm(y, pi, means, variances)\n",
    "\n",
    "# Print estimated parameters\n",
    "print(f\"Estimated mixture weights: {pi_est}\")\n",
    "print(f\"Estimated means: {means_est}\")\n",
    "print(f\"Estimated variances: {variances_est}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvLmu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
