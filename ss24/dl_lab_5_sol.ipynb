{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "aac398b0",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "# Lab 5\n",
                "\n",
                "**Lecture**: Deep Learning (Prof. Dr. David RÃ¼gamer, Emanuel Sommer)\n",
                "\n",
                "Welcome to the fifth lab. We will first implement a simple scalar automatic\n",
                "differentiation engine to compute partial derivatives for us,\n",
                "then do a theoretical exercise about L2 regularization.\n",
                "\n",
                "## Imports\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "08c3e93d",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.557626Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.557338Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.824517Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.823724Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "import math\n",
                "from abc import ABC, abstractmethod\n",
                "from random import uniform\n",
                "from typing import Optional, List, Union, Dict\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
                "\n",
                "set_matplotlib_formats('png', 'pdf')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bc881939",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "## Exercise 1\n",
                "\n",
                "Modern deep learning frameworks compute gradients automatically,\n",
                "so that you only need to define how to perform the forward pass in your code.\n",
                "Under the hood, the framework constructs a computational graph based on the operations\n",
                "you used. For example, consider the node:\n",
                "\n",
                "$$\n",
                "4xy+e^{-y}\n",
                "$$\n",
                "\n",
                "It can be translated into a graph that looks like this:\n",
                "\n",
                "![](../utils/05-lab-graph1.png)\n",
                "\n",
                "Where we have 'leaf' nodes at the top for variables and constants, and 'internal' nodes\n",
                "for operations. To make things simpler, in this exercise we will only work with\n",
                "scalar operations and scalar variables, but what we are going to create could, in\n",
                "principle, be extended to work with vectors and matrices. Section 6 of chapter 5 of the\n",
                "_Mathematics for Machine Learning_ book (https://mml-book.github.io/) is a good\n",
                "supplementary read.\n",
                "\n",
                "The naming of the classes responds to:\n",
                "\n",
                " - `Sum` for Addition $x+y$\n",
                " - `Sub` for Subtraction $x-y$\n",
                " - `Mul` for Product $x\\cdot y$\n",
                " - `Div` for Division $x / y$\n",
                " - `Exp` for Exponentiation $e^x$\n",
                " - `TanH` for Hyperbolic tangent $\\tanh(x)$\n",
                " - `Log` for Logarithm $\\log(x)$\n",
                "\n",
                "\n",
                "We first define some utilities to easily create nodes.\n",
                "An abstract class gives us a common interface across all the respective nodes that\n",
                "we will derive from it. This doesn't make sense now, as the class is basically empty,\n",
                "but we will extend the definition over the course of the exercise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "e543d71a",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.827228Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.826938Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.832708Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.832104Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "class BaseNode(ABC):\n",
                "    pass\n",
                "\n",
                "\n",
                "class Const(BaseNode):\n",
                "    def __init__(self, value: Union[float, int]):\n",
                "        self.value = value\n",
                "\n",
                "\n",
                "class Var(BaseNode):\n",
                "    def __init__(self, name: str):\n",
                "        self.name = name\n",
                "\n",
                "\n",
                "class BinaryOperation(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode, y: BaseNode):\n",
                "        self.x = x\n",
                "        self.y = y\n",
                "\n",
                "\n",
                "class Sum(BinaryOperation):\n",
                "    pass\n",
                "\n",
                "\n",
                "class Sub(BinaryOperation):\n",
                "    pass\n",
                "\n",
                "\n",
                "class Mul(BinaryOperation):\n",
                "    pass\n",
                "\n",
                "\n",
                "class Div(BinaryOperation):\n",
                "    pass\n",
                "\n",
                "\n",
                "class Function(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode):\n",
                "        self.x = x\n",
                "\n",
                "\n",
                "class Exp(Function):\n",
                "    pass\n",
                "\n",
                "\n",
                "class Log(Function):\n",
                "    pass\n",
                "\n",
                "\n",
                "class TanH(Function):\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "d6e6289f",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.834903Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.834773Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.838124Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.837614Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<__main__.Sum object at 0x7f629640ad40>\n",
                        "<__main__.Mul object at 0x7f629640ae30>\n",
                        "<__main__.Exp object at 0x7f629640ac50>\n"
                    ]
                }
            ],
            "source": [
                "# We then define the graph from the equation before.\n",
                "x = Var('x')\n",
                "y = Var('y')\n",
                "\n",
                "z = Sum(\n",
                "    x=Mul(\n",
                "        x=Mul(\n",
                "            x=Const(4),\n",
                "            y=x\n",
                "        ),\n",
                "        y=y,\n",
                "    ),\n",
                "    y=Exp(\n",
                "        x=Mul(\n",
                "            x=Const(-1),\n",
                "            y=y\n",
                "    ))\n",
                ")\n",
                "\n",
                "print(z)\n",
                "print(z.x.x)\n",
                "print(z.y)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2f34ad38",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "This structure of nested objects contains the computational graph for the node above.\n",
                "Now, we can write code to manipulate this expression as we please.\n",
                "In the course of this exercise, we will see how:\n",
                "\n",
                " 1. Print an expression,\n",
                " 2. Compute its value, given the values of the variables involved,\n",
                " 3. Differentiate it to automatically find partial derivatives with respect to any given variable,\n",
                " 4. Transform it into simpler expressions that are cheaper to handle, and\n",
                " 5. Write code to train a neural network without getting our hands dirty with derivatives ever again.\n",
                "\n",
                "\n",
                "### Printing an expression\n",
                "First, since it is quite hard to understand the node from the representation above,\n",
                "let us extend the classes to convert a computational graph into a string representation\n",
                "that is easier to understand. For example, the expression $x+2y$ should be converted to"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "75823a7b",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.840958Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.840635Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.845648Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.845254Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['(', 'x', '+', '(', '2', '*', 'y', ')', ')']"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "['(', 'x', '+', '(', '2', '*', 'y', ')', ')']"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73e1f0aa",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "Which can be printed easily using `\" \".join(<list>)`, resulting in `( x + ( 2 * y ) )`.\n",
                "\n",
                "Such a function should be _recursive_. This means that when simplifying a complicated\n",
                "expression it will call itself on each constituting piece of that expression, and\n",
                "\"assemble\" the results together.\n",
                "Conceptually, the procedure is similar to the factorial operation,\n",
                "which is recursively defined in terms of the factorial of a smaller number:\n",
                "\n",
                "\\begin{equation}\n",
                "n!=\\begin{cases}\n",
                "1 & \\text{if }n < 1 \\\\\n",
                "n\\cdot(n-1)! & \\text{otherwise}\n",
                "\\end{cases}\n",
                "\\end{equation}\n",
                "\n",
                "This definition can be converted into Python as:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "902011eb",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.847411Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.847299Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.849793Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.849415Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "24\n"
                    ]
                }
            ],
            "source": [
                "def factorial(n: int) -> int:\n",
                "    if n < 1:\n",
                "        return 1\n",
                "    else:\n",
                "        return n * factorial(n - 1)\n",
                "\n",
                "print(factorial(4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fef5abff",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "In a similar way, we extend the classes with `__str__`, which is a Python utility to\n",
                "obtain a custom string representation of an object. If we define it correctly, we\n",
                "are able to traverse through the tree and print the expression."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "f0118daf",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.851710Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.851416Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.858043Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.857401Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "class BaseNode(ABC):\n",
                "    @abstractmethod\n",
                "    def __str__(self) -> str:\n",
                "        pass\n",
                "\n",
                "\n",
                "class Const(BaseNode):\n",
                "    def __init__(self, value: Union[float, int]):\n",
                "        self.value = value\n",
                "\n",
                "    def __str__(self) -> str:\n",
                "        return str(self.value)\n",
                "\n",
                "\n",
                "class Var(BaseNode):\n",
                "    def __init__(self, name: str):\n",
                "        self.name = name\n",
                "\n",
                "    def __str__(self) -> str:\n",
                "        return self.name\n",
                "\n",
                "\n",
                "class BinaryOperation(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode, y: BaseNode):\n",
                "        self.x = x\n",
                "        self.y = y\n",
                "\n",
                "\n",
                "class Sum(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return f'({self.x} + {self.y})'\n",
                "\n",
                "\n",
                "class Sub(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return f'({self.x} - {self.y})'\n",
                "\n",
                "\n",
                "class Mul(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return f'{self.x} * {self.y}'\n",
                "\n",
                "\n",
                "class Div(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return f'{self.x} / {self.y}'\n",
                "\n",
                "\n",
                "class Function(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode):\n",
                "        self.x = x\n",
                "\n",
                "\n",
                "class Exp(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return f'exp({self.x})'\n",
                "\n",
                "\n",
                "class Log(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return f'log({self.x})'\n",
                "\n",
                "\n",
                "class TanH(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return f'tanh({self.x})'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "fddeafbe",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.860341Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.860131Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.863032Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.862692Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(4 * x * y + exp(-1 * y))\n"
                    ]
                }
            ],
            "source": [
                "x = Var('x')\n",
                "y = Var('y')\n",
                "\n",
                "z = Sum(\n",
                "    x=Mul(\n",
                "        x=Mul(\n",
                "            x=Const(4),\n",
                "            y=x\n",
                "        ),\n",
                "        y=y,\n",
                "    ),\n",
                "    y=Exp(\n",
                "        x=Mul(\n",
                "            x=Const(-1),\n",
                "            y=y\n",
                "    ))\n",
                ")\n",
                "\n",
                "print(z)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f26cc92d",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "This is much simpler to read!\n",
                "\n",
                "### Computing the value of an expression\n",
                "We can now extend the classes to compute the value of an expression given values for\n",
                "the variables. The methods should be recursive too."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "939bec32",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.864583Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.864439Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.871499Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.870970Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "class BaseNode(ABC):\n",
                "    @abstractmethod\n",
                "    def __str__(self) -> str:\n",
                "        pass\n",
                "\n",
                "    @abstractmethod\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        pass\n",
                "\n",
                "\n",
                "class Const(BaseNode):\n",
                "    def __init__(self, value: Union[float, int]):\n",
                "        self.value = value\n",
                "\n",
                "    def __str__(self) -> str:\n",
                "        return str(self.value)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.value\n",
                "\n",
                "\n",
                "class Var(BaseNode):\n",
                "    def __init__(self, name: str, value: Optional[Union[float, int]] = None):\n",
                "        self.name = name\n",
                "        self.value = value\n",
                "        \n",
                "    def set_value(self, value: Union[float, int]) -> None:\n",
                "        self.value = value\n",
                "\n",
                "    def __str__(self) -> str:\n",
                "        return self.name\n",
                "    \n",
                "    def eval(self) -> Union[float, int]:\n",
                "        if self.value is None:\n",
                "            raise ValueError('Value is not set. Evaluation is not possible.')\n",
                "        return self.value\n",
                "\n",
                "\n",
                "class BinaryOperation(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode, y: BaseNode):\n",
                "        self.x = x\n",
                "        self.y = y\n",
                "\n",
                "\n",
                "class Sum(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} + {})'.format(self.x, self.y)\n",
                "    \n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() + self.y.eval()\n",
                "\n",
                "\n",
                "class Sub(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} - {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() - self.y.eval()\n",
                "\n",
                "\n",
                "class Mul(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} * {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() * self.y.eval()\n",
                "\n",
                "\n",
                "class Div(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} / {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() / self.y.eval()\n",
                "\n",
                "\n",
                "class Function(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode):\n",
                "        self.x = x\n",
                "\n",
                "\n",
                "class Exp(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return 'exp({})'.format(self.x)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return math.exp(self.x.eval())\n",
                "\n",
                "class Log(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return 'log({})'.format(self.x)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return math.log(self.x.eval())\n",
                "\n",
                "class TanH(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return 'tanh({})'.format(self.x)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return math.tanh(self.x.eval())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "52978e0a",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.873693Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.873467Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.877037Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.876553Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "24.0498\n"
                    ]
                }
            ],
            "source": [
                "x = Var('x', value=2)\n",
                "y = Var('y', value=3)\n",
                "\n",
                "z = Sum(\n",
                "    x=Mul(\n",
                "        x=Mul(\n",
                "            x=Const(4),\n",
                "            y=x\n",
                "        ),\n",
                "        y=y,\n",
                "    ),\n",
                "    y=Exp(\n",
                "        x=Mul(\n",
                "            x=Const(-1),\n",
                "            y=y\n",
                "    ))\n",
                ")\n",
                "\n",
                "print(round(z.eval(), 4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5481ec2a",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "The result that we expect is, of course:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "dabab2c2",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.878808Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.878653Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.880952Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.880605Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "24.0498\n"
                    ]
                }
            ],
            "source": [
                "print(round(4 * 2 * 3 + math.exp(-3), 4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f9ff316d",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Differentiating an expression\n",
                "\n",
                "We can finally see how to differentiate an expression with respect to a variable.\n",
                "We do this again through extending the classes to differentiates each argument\n",
                "and merge the result. Note that this should return a new computational\n",
                "graph that contains the operations necessary to compute the partial derivative\n",
                "we are interested in.\n",
                "Each `differentiate` method gets the argument `var` that specifies the name of the\n",
                "variable of which we are computing the gradient.\n",
                "\n",
                "Remember to use the chain rule where appropriate!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "c0d22a48",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.882551Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.882422Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.892793Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.892384Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "class BaseNode(ABC):\n",
                "    @abstractmethod\n",
                "    def __str__(self) -> str:\n",
                "        pass\n",
                "\n",
                "    @abstractmethod\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        pass\n",
                "\n",
                "    @abstractmethod\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        pass\n",
                "\n",
                "\n",
                "class Const(BaseNode):\n",
                "    def __init__(self, value: Union[float, int]):\n",
                "        self.value = value\n",
                "\n",
                "    def __str__(self) -> str:\n",
                "        return str(self.value)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.value\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Const(0)\n",
                "\n",
                "\n",
                "class Var(BaseNode):\n",
                "    def __init__(self, name: str, value: Optional[Union[float, int]] = None):\n",
                "        self.name = name\n",
                "        self.value = value\n",
                "\n",
                "    def set_value(self, value: Union[float, int]) -> None:\n",
                "        self.value = value\n",
                "\n",
                "    def __str__(self) -> str:\n",
                "        return self.name\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        if self.value is None:\n",
                "            raise ValueError('Value is not set. Evaluation is not possible.')\n",
                "        return self.value\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Const(1) if self.name == var else Const(0)\n",
                "\n",
                "\n",
                "class BinaryOperation(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode, y: BaseNode):\n",
                "        self.x = x\n",
                "        self.y = y\n",
                "\n",
                "\n",
                "class Sum(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} + {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() + self.y.eval()\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Sum(self.x.differentiate(var), self.y.differentiate(var))\n",
                "\n",
                "\n",
                "class Sub(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} - {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() - self.y.eval()\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Sub(self.x.differentiate(var), self.y.differentiate(var))\n",
                "\n",
                "\n",
                "class Mul(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} * {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() * self.y.eval()\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Sum(\n",
                "            x=Mul(self.x.differentiate(var), self.y),\n",
                "            y=Mul(self.x, self.y.differentiate(var))\n",
                "        )\n",
                "\n",
                "\n",
                "class Div(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} / {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() / self.y.eval()\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Div(\n",
                "            x=Sub(\n",
                "                x=Mul(self.x.differentiate(var), self.y),\n",
                "                y=Mul(self.y.differentiate(var), self.x)\n",
                "            ),\n",
                "            y=Mul(self.y, self.y)\n",
                "        )\n",
                "\n",
                "\n",
                "class Function(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode):\n",
                "        self.x = x\n",
                "\n",
                "\n",
                "class Exp(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return 'exp({})'.format(self.x)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return math.exp(self.x.eval())\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Mul(\n",
                "            x=Exp(self.x),\n",
                "            y=self.x.differentiate(var)\n",
                "        )\n",
                "\n",
                "\n",
                "class Log(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return 'log({})'.format(self.x)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return math.log(self.x.eval())\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Div(\n",
                "            x=self.x.differentiate(var),\n",
                "            y=self.x\n",
                "        )\n",
                "\n",
                "\n",
                "class TanH(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return 'tanh({})'.format(self.x)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return math.tanh(self.x.eval())\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Mul(\n",
                "            x=Sub(\n",
                "                x=Const(1),\n",
                "                y=Mul(TanH(self.x), TanH(self.x))\n",
                "            ),\n",
                "            y=self.x.differentiate(var)\n",
                "        )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "572578ea",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.894850Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.894537Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.897693Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.897180Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(((((0 * x) + (4 * 1)) * y) + ((4 * x) * 0)) + (exp((-1 * y)) * ((0 * y) + (-1 * 0))))\n"
                    ]
                }
            ],
            "source": [
                "x = Var('x', value=2)\n",
                "y = Var('y', value=3)\n",
                "\n",
                "z = Sum(\n",
                "    x=Mul(\n",
                "        x=Mul(\n",
                "            x=Const(4),\n",
                "            y=x\n",
                "        ),\n",
                "        y=y,\n",
                "    ),\n",
                "    y=Exp(\n",
                "        x=Mul(\n",
                "            x=Const(-1),\n",
                "            y=y\n",
                "    ))\n",
                ")\n",
                "\n",
                "dz = z.differentiate('x')\n",
                "print(dz)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "88e091d4",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "This looks a bit complicated, but by applying some trivial simplifications we see it is correct:\n",
                "\n",
                "\\begin{align*}\n",
                "& ( ( ( ( ( 0 \\cdot x ) + ( 4 \\cdot 1 ) ) \\cdot y ) + ( ( 4 \\cdot x ) \\cdot 0 ) ) + ( exp ( ( -1 \\cdot y ) ) \\cdot ( ( 0 \\cdot y ) + ( -1 \\cdot 0 ) ) ) )  \\\\\n",
                "&\\qquad= ( ( ( 0 + 4 ) \\cdot y ) + 0 ) + ( exp ( ( -1 \\cdot y ) ) \\cdot ( 0 + 0 ) ) ) \\\\\n",
                "&\\qquad= ( 4 \\cdot y ) + ( exp ( ( -1 \\cdot y ) ) \\cdot 0 ) ) \\\\\n",
                "&\\qquad= ( 4 \\cdot y ) + 0 \\\\\n",
                "&\\qquad= 4 \\cdot y \\\\\n",
                "&\\qquad= \\frac{\\text{d}}{\\text{d}x} \\left(4xy+e^{-y}\\right)\n",
                "\\end{align*}\n",
                "\n",
                "These simplification rules are trivial arithmetic identities:\n",
                "\n",
                " - $0+x=x$\n",
                " - $0\\cdot x=0$\n",
                " - $1\\cdot x=x$\n",
                " - $0/x=0$\n",
                "\n",
                "Let us extend the classes that use these identities to automatically simplify `dz` in\n",
                "the same way we just did. As with differentiation, this should return a\n",
                "new computational graph."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "18ab92bb",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.899429Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.899312Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.918453Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.917959Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "class BaseNode(ABC):\n",
                "    @abstractmethod\n",
                "    def __str__(self) -> str:\n",
                "        pass\n",
                "\n",
                "    @abstractmethod\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        pass\n",
                "\n",
                "    @abstractmethod\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        pass\n",
                "\n",
                "    @abstractmethod\n",
                "    def simplify(self):\n",
                "        pass\n",
                "\n",
                "\n",
                "class Const(BaseNode):\n",
                "    def __init__(self, value: Union[float, int]):\n",
                "        self.value = value\n",
                "\n",
                "    def __str__(self) -> str:\n",
                "        return str(self.value)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.value\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Const(0)\n",
                "\n",
                "    def simplify(self):\n",
                "        return self\n",
                "\n",
                "\n",
                "class Var(BaseNode):\n",
                "    def __init__(self, name: str, value: Optional[Union[float, int]] = None):\n",
                "        self.name = name\n",
                "        self.value = value\n",
                "\n",
                "    def set_value(self, value: Union[float, int]) -> None:\n",
                "        self.value = value\n",
                "\n",
                "    def __str__(self) -> str:\n",
                "        return self.name\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        if self.value is None:\n",
                "            raise ValueError('Value is not set. Evaluation is not possible.')\n",
                "        return self.value\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Const(1) if self.name == var else Const(0)\n",
                "\n",
                "    def simplify(self):\n",
                "        return self\n",
                "\n",
                "\n",
                "class BinaryOperation(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode, y: BaseNode):\n",
                "        self.x = x\n",
                "        self.y = y\n",
                "\n",
                "\n",
                "class Sum(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} + {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() + self.y.eval()\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Sum(self.x.differentiate(var), self.y.differentiate(var))\n",
                "\n",
                "    def simplify(self):\n",
                "        simple_x = (\n",
                "            self.x.simplify()\n",
                "        )\n",
                "\n",
                "        simple_y = (\n",
                "            self.y.simplify()\n",
                "        )\n",
                "\n",
                "        if isinstance(simple_x, Const):\n",
                "            if simple_x.value == 0:\n",
                "                # Rule: 0 + y = y\n",
                "                return simple_y\n",
                "\n",
                "        if isinstance(simple_y, Const):\n",
                "            if simple_y.value == 0:\n",
                "                # Rule: x + 0 = x\n",
                "                return simple_x\n",
                "\n",
                "        if isinstance(simple_x, Const) and isinstance(simple_y, Const):\n",
                "            # If both arguments are constants we can perform the sum immediately\n",
                "            return Const(simple_x.value + simple_y.value)\n",
                "\n",
                "        # Cannot simplify further. Return a new sum node with the simplified operands.\n",
                "        return Sum(simple_x, simple_y)\n",
                "\n",
                "\n",
                "class Sub(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} - {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() - self.y.eval()\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Sub(self.x.differentiate(var), self.y.differentiate(var))\n",
                "\n",
                "    def simplify(self):\n",
                "        simple_x =  self.x.simplify()\n",
                "        simple_y =  self.y.simplify()\n",
                "\n",
                "        if isinstance(simple_x, Const):\n",
                "            if simple_x.value == 0:\n",
                "                # Rule: 0 - y = -1 * y\n",
                "                return Mul(Const(-1), simple_y)\n",
                "\n",
                "        if isinstance(simple_y, Const):\n",
                "            if simple_y.value == 0:\n",
                "                # Rule: x - 0 = x\n",
                "                return simple_y\n",
                "\n",
                "        if isinstance(simple_x, Const) and isinstance(simple_y, Const):\n",
                "            # If both arguments are constants we can perform the subtraction immediately\n",
                "            return Const(simple_x.value - simple_y.value)\n",
                "\n",
                "        # Cannot simplify further.\n",
                "        return Sub(simple_x, simple_y)\n",
                "\n",
                "\n",
                "class Mul(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} * {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() * self.y.eval()\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Sum(\n",
                "            x=Mul(self.x.differentiate(var), self.y),\n",
                "            y=Mul(self.x, self.y.differentiate(var))\n",
                "        )\n",
                "\n",
                "    def simplify(self):\n",
                "        simple_x =  self.x.simplify()\n",
                "        simple_y =  self.y.simplify()\n",
                "\n",
                "        if isinstance(simple_x, Const):\n",
                "            if simple_x.value == 0:\n",
                "                # Rule: 0 * y = 0\n",
                "                return Const(0)\n",
                "            elif simple_x.value == 1:\n",
                "                # Rule: 1 * y = y\n",
                "                return simple_y\n",
                "\n",
                "        if isinstance(simple_y, Const):\n",
                "            if simple_y.value == 0:\n",
                "                # Rule: x * 0 = 0\n",
                "                return Const(0)\n",
                "            elif simple_y.value == 1:\n",
                "                # Rule: x * 1 = x\n",
                "                return simple_x\n",
                "\n",
                "        if isinstance(simple_x, Const) and isinstance(simple_y, Const):\n",
                "            # Perform the operation if possible\n",
                "            return Const(simple_x.value * simple_y.value)\n",
                "\n",
                "        # Cannot simplify further.\n",
                "        return Mul(simple_x, simple_y)\n",
                "\n",
                "\n",
                "class Div(BinaryOperation):\n",
                "    def __str__(self) -> str:\n",
                "        return '({} / {})'.format(self.x, self.y)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return self.x.eval() / self.y.eval()\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Div(\n",
                "            x=Sub(\n",
                "                x=Mul(self.x.differentiate(var), self.y),\n",
                "                y=Mul(self.y.differentiate(var), self.x)\n",
                "            ),\n",
                "            y=Mul(self.y, self.y)\n",
                "        )\n",
                "\n",
                "    def simplify(self):\n",
                "        simple_x =  self.x.simplify()\n",
                "        simple_y =  self.y.simplify()\n",
                "\n",
                "        if isinstance(simple_x, Const):\n",
                "            if simple_x.value == 0:\n",
                "                # Rule: 0 / y = 0\n",
                "                return Const(0)\n",
                "\n",
                "        if isinstance(simple_y, Const):\n",
                "            if simple_y.value == 0:\n",
                "                # Rule: x / 0 = ERROR\n",
                "                raise ZeroDivisionError()\n",
                "            elif simple_y.value == 1:\n",
                "                # Rule: x / 1 = x\n",
                "                return simple_x\n",
                "\n",
                "        if isinstance(simple_x, Const) and isinstance(simple_y, Const):\n",
                "            # Perform the operation if possible\n",
                "            return Const(simple_x.value / simple_y.value)\n",
                "\n",
                "        # Cannot simplify further.\n",
                "        return Div(simple_x, simple_y)\n",
                "\n",
                "\n",
                "class Function(BaseNode, ABC):\n",
                "    def __init__(self, x: BaseNode):\n",
                "        self.x = x\n",
                "\n",
                "\n",
                "class Exp(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return 'exp({})'.format(self.x)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return math.exp(self.x.eval())\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Mul(\n",
                "            x=Exp(self.x),\n",
                "            y=self.x.differentiate(var)\n",
                "        )\n",
                "\n",
                "    def simplify(self):\n",
                "        simple_x =  self.x.simplify()\n",
                "\n",
                "        if isinstance(simple_x, Const):\n",
                "            if simple_x.value == 0:\n",
                "                # Rule: exp(0) = 1\n",
                "                return Const(1)\n",
                "            # Perform the operation if possible\n",
                "            return Const(math.exp(simple_x.value))\n",
                "\n",
                "        # Cannot simplify further.\n",
                "        return Exp(simple_x)\n",
                "\n",
                "\n",
                "class Log(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return 'log({})'.format(self.x)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return math.log(self.x.eval())\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Div(\n",
                "            x=self.x.differentiate(var),\n",
                "            y=self.x\n",
                "        )\n",
                "\n",
                "    def simplify(self):\n",
                "        simple_x =  self.x.simplify()\n",
                "\n",
                "        if isinstance(simple_x, Const):\n",
                "            if simple_x.value <= 0:\n",
                "                # Rule: No log of non-positiv number!\n",
                "                raise ValueError('Logarithm of non-positiv number.')\n",
                "            # Perform the operation if possible\n",
                "            return Const(math.log(simple_x.value))\n",
                "\n",
                "        # Cannot simplify further.\n",
                "        return Log(simple_x)\n",
                "\n",
                "\n",
                "class TanH(Function):\n",
                "    def __str__(self) -> str:\n",
                "        return 'tanh({})'.format(self.x)\n",
                "\n",
                "    def eval(self) -> Union[float, int]:\n",
                "        return math.tanh(self.x.eval())\n",
                "\n",
                "    def differentiate(self, var: str) -> BaseNode:\n",
                "        return Mul(\n",
                "            x=Sub(\n",
                "                x=Const(1),\n",
                "                y=Mul(TanH(self.x), TanH(self.x))\n",
                "            ),\n",
                "            y=self.x.differentiate(var)\n",
                "        )\n",
                "\n",
                "    def simplify(self):\n",
                "        simple_x =  self.x.simplify()\n",
                "\n",
                "        if isinstance(simple_x, Const):\n",
                "            if simple_x.value == 0:\n",
                "                # Rule: tanh(0) = 0\n",
                "                return Const(0)\n",
                "            # Perform the operation if possible\n",
                "            return Const(math.tanh(simple_x.value))\n",
                "\n",
                "        # Cannot simplify further.\n",
                "        return TanH(simple_x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "5c0dbe06",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.920627Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.920334Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.924627Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.924036Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(4 * y)\n"
                    ]
                }
            ],
            "source": [
                "x = Var('x', value=2)\n",
                "y = Var('y', value=3)\n",
                "\n",
                "z = Sum(\n",
                "    x=Mul(\n",
                "        x=Mul(\n",
                "            x=Const(4),\n",
                "            y=x\n",
                "        ),\n",
                "        y=y,\n",
                "    ),\n",
                "    y=Exp(\n",
                "        x=Mul(\n",
                "            x=Const(-1),\n",
                "            y=y\n",
                "    ))\n",
                ")\n",
                "\n",
                "dz = z.differentiate('x')\n",
                "print(dz.simplify())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e02eb8d8",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "The result matches what we showed above, $4y$. Simplifying the graph with these and\n",
                "other, more advanced tricks, can greatly speed up code.\n",
                "\n",
                "Now we are also equipped to perform differentiation of any order,\n",
                "for example $\\partial z / \\partial x\\partial y$ is simply:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "b11879b9",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.927001Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.926829Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.929580Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.929228Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(((((((0 * x) + (0 * 0)) + ((0 * 1) + (4 * 0))) * y) + (((0 * x) + (4 * 1)) * 1)) + ((((0 * x) + (4 * 0)) * 0) + ((4 * x) * 0))) + (((exp((-1 * y)) * ((0 * y) + (-1 * 1))) * ((0 * y) + (-1 * 0))) + (exp((-1 * y)) * (((0 * y) + (0 * 1)) + ((0 * 0) + (-1 * 0))))))\n",
                        "4\n",
                        "4.0\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "print(z.differentiate('x').differentiate('y'))\n",
                "print(z.differentiate('x').differentiate('y').simplify())\n",
                "print(z.differentiate('x').differentiate('y').eval())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "774aae67",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Training a network\n",
                "\n",
                "Let us now define a computational graph that performs the forward pass of a\n",
                "simple network, and use the functions above to compute the gradients of the parameters.\n",
                "We will use the same network we used in the third lab, reproduced below, and, as usual,\n",
                "we will test the code on the five points dataset. Since the functions we have written\n",
                "so far only work with scalar values, we will perform stochastic gradient descent\n",
                "using one sample at a time.\n",
                "\n",
                "![](../utils/03-lab-nn.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "deae960f",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.931259Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.931112Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.933931Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.933522Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tanh((b1 + ((x1 * w11) + (x2 * w21))))\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# The two input nodes\n",
                "x1 = Var('x1')\n",
                "x2 = Var('x2')\n",
                "\n",
                "# Parameters for the first hidden neuron\n",
                "b1 = Var('b1')\n",
                "w11 = Var('w11')\n",
                "w21 = Var('w21')\n",
                "\n",
                "# Compute the output of the first hidden neuron\n",
                "z1in = Sum(\n",
                "    x=b1,\n",
                "    y=Sum(\n",
                "       x=Mul(x1, w11),\n",
                "       y=Mul(x2, w21)\n",
                "    )\n",
                ")\n",
                "\n",
                "z1out = TanH(z1in)\n",
                "\n",
                "print(z1out)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b0decb7b",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "Now, complete the remaining part of the network:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "c2b38bbe",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.935643Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.935513Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.940010Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.939520Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(1 / (1 + exp((-1 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2)))))))\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "b2 = Var('b2')\n",
                "w12 = Var('w12')\n",
                "w22 = Var('w22')\n",
                "\n",
                "z2out = (\n",
                "    TanH(\n",
                "        Sum(\n",
                "            x=b2,\n",
                "            y=Sum(\n",
                "                x=Mul(x1, w12),\n",
                "                y=Mul(x2, w22)\n",
                "            )\n",
                "        )\n",
                "    )\n",
                ")\n",
                "\n",
                "c = Var('c')\n",
                "u1 = Var('u1')\n",
                "u2 = Var('u2')\n",
                "\n",
                "fin = (\n",
                "    Sum(\n",
                "        x=c,\n",
                "        y=Sum(\n",
                "            x=Mul(z1out, u1),\n",
                "            y=Mul(z2out, u2)\n",
                "        )\n",
                "    )\n",
                ")\n",
                "\n",
                "fout = (\n",
                "    Div(\n",
                "        x=Const(1),\n",
                "        y=Sum(\n",
                "            x=Const(1),\n",
                "            y=Exp(Mul(Const(-1), fin))\n",
                "        )\n",
                "    )\n",
                ")\n",
                "\n",
                "print(fout)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8883fa03",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "And this defines the forward pass.\n",
                "\n",
                "We can now compute the predictions of the network by evaluating `fout`,\n",
                "providing values for the inputs and weights. For example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "64c6523f",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.942153Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.941992Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.945725Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.945356Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.9533527373765618"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "\n",
                "# Values for weights and biases\n",
                "b1.set_value(1.543385)\n",
                "w11.set_value(3.111573)\n",
                "w12.set_value(-2.808800)\n",
                "\n",
                "b2.set_value(1.373085)\n",
                "w21.set_value(3.130452)\n",
                "w22.set_value(-2.813466)\n",
                "\n",
                "c.set_value(-4.241453)\n",
                "u1.set_value(4.036489)\n",
                "u2.set_value(4.074885)\n",
                "\n",
                "# Values for the input\n",
                "x1.set_value(1)\n",
                "x2.set_value(-1)\n",
                "\n",
                "fout.eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "77ba3f53",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "Which should be about 0.9.\n",
                "We now have to compute the cross-entropy loss.\n",
                "For numerical stability, we will compute the loss using $f_{in}$ instead of $f_{out}$.\n",
                "Therefore, first, show that:\n",
                "\n",
                "\\begin{equation}\n",
                "-y\\cdot\\log(f_{out})-(1-y)\\cdot\\log(1-f_{out})=\n",
                "f_{in}-f_{in}\\cdot y+\\log(1+e^{-f_{in}})\n",
                "\\end{equation}\n",
                "\n",
                "\n",
                "Solution:\n",
                "\n",
                "\\begin{align}\n",
                "&-y\\cdot\\log (f_{out})-(1-y)\\cdot\\log(1-f_{out}) \\\\\n",
                "&\\qquad=-y\\cdot\\log\\frac{1}{1+e^{-f_{in}}}-(1-y)\\cdot\\log\\left(1-\\frac{1}{1+e^{-f_{in}}}\\right) \\\\\n",
                "&\\qquad=\n",
                "-y\\cdot-\\log\\left(1+e^{-f_{in}}\\right)\n",
                "-(1-y)\\cdot\\left(-f_{in}-\\log\\left(1+e^{-f_{in}}\\right)\\right) \\\\\n",
                "&\\qquad=\n",
                "y\\cdot\\log\\left(1+e^{-f_{in}}\\right)\n",
                "+f_{in}+\\log\\left(1+e^{-f_{in}}\\right)\n",
                "-y\\cdot f_{in}-y\\cdot\\log\\left(1+e^{-f_{in}}\\right) \\\\\n",
                "&\\qquad=f_{in}-f_{in}\\cdot y+\\log(1+e^{-f_{in}})\n",
                "\\end{align}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "d4fb0171",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.947490Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.947333Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.950102Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.949739Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(((c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))) - ((c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))) * y)) + log((1 + exp((-1 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))))))))\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# This variable contains the label for the sample the network is predicting\n",
                "y = Var('y')\n",
                "\n",
                "loss = (\n",
                "    Sum(\n",
                "        x=Sub(\n",
                "            x=fin,\n",
                "            y=Mul(fin, y)\n",
                "        ),\n",
                "        y=Log(\n",
                "            Sum(\n",
                "                x=Const(1),\n",
                "                y=Exp(Mul(Const(-1), fin)))\n",
                "        )\n",
                "    )\n",
                ")\n",
                "\n",
                "print(loss)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ba0076fa",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "This is starting to look complicated!\n",
                "Luckily, this time, we do not have to get our hands dirty with derivatives.\n",
                "Let us find the graphs for the derivatives of each parameter of the network."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "a3710473",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.951845Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.951720Z",
                    "iopub.status.idle": "2024-04-23T15:24:01.956670Z",
                    "shell.execute_reply": "2024-04-23T15:24:01.956224Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(((0 + (((((1 - (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * tanh((b1 + ((x1 * w11) + (x2 * w21)))))) * (0 + (((0 * w11) + (x1 * 1)) + ((0 * w21) + (x2 * 0))))) * u1) + (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * 0)) + ((((1 - (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * tanh((b2 + ((x1 * w12) + (x2 * w22)))))) * (0 + (((0 * w12) + (x1 * 0)) + ((0 * w22) + (x2 * 0))))) * u2) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * 0)))) - (((0 + (((((1 - (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * tanh((b1 + ((x1 * w11) + (x2 * w21)))))) * (0 + (((0 * w11) + (x1 * 1)) + ((0 * w21) + (x2 * 0))))) * u1) + (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * 0)) + ((((1 - (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * tanh((b2 + ((x1 * w12) + (x2 * w22)))))) * (0 + (((0 * w12) + (x1 * 0)) + ((0 * w22) + (x2 * 0))))) * u2) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * 0)))) * y) + ((c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))) * 0))) + ((0 + (exp((-1 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))))) * ((0 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2)))) + (-1 * (0 + (((((1 - (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * tanh((b1 + ((x1 * w11) + (x2 * w21)))))) * (0 + (((0 * w11) + (x1 * 1)) + ((0 * w21) + (x2 * 0))))) * u1) + (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * 0)) + ((((1 - (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * tanh((b2 + ((x1 * w12) + (x2 * w22)))))) * (0 + (((0 * w12) + (x1 * 0)) + ((0 * w22) + (x2 * 0))))) * u2) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * 0)))))))) / (1 + exp((-1 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))))))))\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "def get_gradient_graphs(graph: BaseNode, param_nodes: List[Var]) -> Dict:\n",
                "    graph_dict: Dict = {}\n",
                "    for param_node in param_nodes:\n",
                "        graph_dict.update({\n",
                "            param_node.name : graph.differentiate(param_node.name)\n",
                "        })\n",
                "    return graph_dict\n",
                "\n",
                "parameter_nodes = [b1, w11, w12, b2, w21, w22, c, u1, u2]\n",
                "gradient_graphs = get_gradient_graphs(loss, parameter_nodes)\n",
                "print(gradient_graphs.get('w11'))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "80ae88ef",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "As you can see, there is a great deal of repetition in this expression.\n",
                "The repetitions could be removed by storing, in each node, its current value and\n",
                "gradient, so that we would not need to re-compute them every time. Modern deep\n",
                "learning frameworks indeed do this, and are able to compute the gradient of the\n",
                "loss with respect to all parameters in a single pass, but here we accept these\n",
                "inefficiencies for the sake of simplicity.\n",
                "\n",
                "We are now ready to train this network:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "522fb56f",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:01.959103Z",
                    "iopub.status.busy": "2024-04-23T15:24:01.958924Z",
                    "iopub.status.idle": "2024-04-23T15:24:04.708944Z",
                    "shell.execute_reply": "2024-04-23T15:24:04.708400Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t     1 \t LOSS: \t 3.48492\r",
                        "EPOCH: \t     2 \t LOSS: \t 5.01075\r",
                        "EPOCH: \t     3 \t LOSS: \t 5.54483\r",
                        "EPOCH: \t     4 \t LOSS: \t 7.12922\r",
                        "EPOCH: \t     5 \t LOSS: \t 6.42948\r",
                        "EPOCH: \t     6 \t LOSS: \t 5.12526\r",
                        "EPOCH: \t     7 \t LOSS: \t 7.26116\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t     8 \t LOSS: \t 6.85913\r",
                        "EPOCH: \t     9 \t LOSS: \t 5.55125\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    10 \t LOSS: \t 5.82260\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    11 \t LOSS: \t 7.02269\r",
                        "EPOCH: \t    12 \t LOSS: \t 5.44379\r",
                        "EPOCH: \t    13 \t LOSS: \t 7.20913\r",
                        "EPOCH: \t    14 \t LOSS: \t 6.67301\r",
                        "EPOCH: \t    15 \t LOSS: \t 5.28024\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    16 \t LOSS: \t 6.38105\r",
                        "EPOCH: \t    17 \t LOSS: \t 7.32556\r",
                        "EPOCH: \t    18 \t LOSS: \t 7.25564\r",
                        "EPOCH: \t    19 \t LOSS: \t 7.11014\r",
                        "EPOCH: \t    20 \t LOSS: \t 6.74638\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    21 \t LOSS: \t 5.76206\r",
                        "EPOCH: \t    22 \t LOSS: \t 7.15044\r",
                        "EPOCH: \t    23 \t LOSS: \t 7.18060\r",
                        "EPOCH: \t    24 \t LOSS: \t 7.07405\r",
                        "EPOCH: \t    25 \t LOSS: \t 6.47557\r",
                        "EPOCH: \t    26 \t LOSS: \t 6.65996\r",
                        "EPOCH: \t    27 \t LOSS: \t 6.45169\r",
                        "EPOCH: \t    28 \t LOSS: \t 5.93486\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    29 \t LOSS: \t 6.43673\r",
                        "EPOCH: \t    30 \t LOSS: \t 4.57812\r",
                        "EPOCH: \t    31 \t LOSS: \t 7.42168\r",
                        "EPOCH: \t    32 \t LOSS: \t 6.58457\r",
                        "EPOCH: \t    33 \t LOSS: \t 5.31060\r",
                        "EPOCH: \t    34 \t LOSS: \t 5.63915\r",
                        "EPOCH: \t    35 \t LOSS: \t 1.36297\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    36 \t LOSS: \t 0.19383\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    37 \t LOSS: \t 0.15988\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    38 \t LOSS: \t 0.13993\r",
                        "EPOCH: \t    39 \t LOSS: \t 0.12511\r",
                        "EPOCH: \t    40 \t LOSS: \t 0.11323\r",
                        "EPOCH: \t    41 \t LOSS: \t 0.10342\r",
                        "EPOCH: \t    42 \t LOSS: \t 0.09517\r",
                        "EPOCH: \t    43 \t LOSS: \t 0.08814\r",
                        "EPOCH: \t    44 \t LOSS: \t 0.08209\r",
                        "EPOCH: \t    45 \t LOSS: \t 0.07682\r",
                        "EPOCH: \t    46 \t LOSS: \t 0.07219\r",
                        "EPOCH: \t    47 \t LOSS: \t 0.06810\r",
                        "EPOCH: \t    48 \t LOSS: \t 0.06445\r",
                        "EPOCH: \t    49 \t LOSS: \t 0.06118\r",
                        "EPOCH: \t    50 \t LOSS: \t 0.05822\r",
                        "EPOCH: \t    51 \t LOSS: \t 0.05554\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    52 \t LOSS: \t 0.05310\r",
                        "EPOCH: \t    53 \t LOSS: \t 0.05087\r",
                        "EPOCH: \t    54 \t LOSS: \t 0.04882\r",
                        "EPOCH: \t    55 \t LOSS: \t 0.04693\r",
                        "EPOCH: \t    56 \t LOSS: \t 0.04518\r",
                        "EPOCH: \t    57 \t LOSS: \t 0.04356\r",
                        "EPOCH: \t    58 \t LOSS: \t 0.04206\r",
                        "EPOCH: \t    59 \t LOSS: \t 0.04065\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    60 \t LOSS: \t 0.03934\r",
                        "EPOCH: \t    61 \t LOSS: \t 0.03810\r",
                        "EPOCH: \t    62 \t LOSS: \t 0.03695\r",
                        "EPOCH: \t    63 \t LOSS: \t 0.03586\r",
                        "EPOCH: \t    64 \t LOSS: \t 0.03484\r",
                        "EPOCH: \t    65 \t LOSS: \t 0.03387\r",
                        "EPOCH: \t    66 \t LOSS: \t 0.03295\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    67 \t LOSS: \t 0.03209\r",
                        "EPOCH: \t    68 \t LOSS: \t 0.03127\r",
                        "EPOCH: \t    69 \t LOSS: \t 0.03049\r",
                        "EPOCH: \t    70 \t LOSS: \t 0.02974\r",
                        "EPOCH: \t    71 \t LOSS: \t 0.02904\r",
                        "EPOCH: \t    72 \t LOSS: \t 0.02836\r",
                        "EPOCH: \t    73 \t LOSS: \t 0.02772\r",
                        "EPOCH: \t    74 \t LOSS: \t 0.02710\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    75 \t LOSS: \t 0.02652\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    76 \t LOSS: \t 0.02595\r",
                        "EPOCH: \t    77 \t LOSS: \t 0.02542\r",
                        "EPOCH: \t    78 \t LOSS: \t 0.02490\r",
                        "EPOCH: \t    79 \t LOSS: \t 0.02440\r",
                        "EPOCH: \t    80 \t LOSS: \t 0.02393\r",
                        "EPOCH: \t    81 \t LOSS: \t 0.02347\r",
                        "EPOCH: \t    82 \t LOSS: \t 0.02303\r",
                        "EPOCH: \t    83 \t LOSS: \t 0.02260\r",
                        "EPOCH: \t    84 \t LOSS: \t 0.02219\r",
                        "EPOCH: \t    85 \t LOSS: \t 0.02180\r",
                        "EPOCH: \t    86 \t LOSS: \t 0.02141\r",
                        "EPOCH: \t    87 \t LOSS: \t 0.02105\r",
                        "EPOCH: \t    88 \t LOSS: \t 0.02069\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    89 \t LOSS: \t 0.02035\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t    90 \t LOSS: \t 0.02002\r",
                        "EPOCH: \t    91 \t LOSS: \t 0.01969\r",
                        "EPOCH: \t    92 \t LOSS: \t 0.01938\r",
                        "EPOCH: \t    93 \t LOSS: \t 0.01908\r",
                        "EPOCH: \t    94 \t LOSS: \t 0.01879\r",
                        "EPOCH: \t    95 \t LOSS: \t 0.01850\r",
                        "EPOCH: \t    96 \t LOSS: \t 0.01823\r",
                        "EPOCH: \t    97 \t LOSS: \t 0.01796\r",
                        "EPOCH: \t    98 \t LOSS: \t 0.01770\r",
                        "EPOCH: \t    99 \t LOSS: \t 0.01745\r",
                        "EPOCH: \t   100 \t LOSS: \t 0.01721\r",
                        "EPOCH: \t   101 \t LOSS: \t 0.01687\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   102 \t LOSS: \t 0.01682\r",
                        "EPOCH: \t   103 \t LOSS: \t 0.01678\r",
                        "EPOCH: \t   104 \t LOSS: \t 0.01673\r",
                        "EPOCH: \t   105 \t LOSS: \t 0.01668\r",
                        "EPOCH: \t   106 \t LOSS: \t 0.01664\r",
                        "EPOCH: \t   107 \t LOSS: \t 0.01660\r",
                        "EPOCH: \t   108 \t LOSS: \t 0.01655\r",
                        "EPOCH: \t   109 \t LOSS: \t 0.01651\r",
                        "EPOCH: \t   110 \t LOSS: \t 0.01646\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   111 \t LOSS: \t 0.01642\r",
                        "EPOCH: \t   112 \t LOSS: \t 0.01638\r",
                        "EPOCH: \t   113 \t LOSS: \t 0.01633\r",
                        "EPOCH: \t   114 \t LOSS: \t 0.01629\r",
                        "EPOCH: \t   115 \t LOSS: \t 0.01625\r",
                        "EPOCH: \t   116 \t LOSS: \t 0.01621\r",
                        "EPOCH: \t   117 \t LOSS: \t 0.01616\r",
                        "EPOCH: \t   118 \t LOSS: \t 0.01612\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   119 \t LOSS: \t 0.01608\r",
                        "EPOCH: \t   120 \t LOSS: \t 0.01604\r",
                        "EPOCH: \t   121 \t LOSS: \t 0.01600\r",
                        "EPOCH: \t   122 \t LOSS: \t 0.01595\r",
                        "EPOCH: \t   123 \t LOSS: \t 0.01591\r",
                        "EPOCH: \t   124 \t LOSS: \t 0.01587\r",
                        "EPOCH: \t   125 \t LOSS: \t 0.01583\r",
                        "EPOCH: \t   126 \t LOSS: \t 0.01579\r",
                        "EPOCH: \t   127 \t LOSS: \t 0.01575\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   128 \t LOSS: \t 0.01571\r",
                        "EPOCH: \t   129 \t LOSS: \t 0.01567\r",
                        "EPOCH: \t   130 \t LOSS: \t 0.01563\r",
                        "EPOCH: \t   131 \t LOSS: \t 0.01559\r",
                        "EPOCH: \t   132 \t LOSS: \t 0.01555\r",
                        "EPOCH: \t   133 \t LOSS: \t 0.01552\r",
                        "EPOCH: \t   134 \t LOSS: \t 0.01548\r",
                        "EPOCH: \t   135 \t LOSS: \t 0.01544\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   136 \t LOSS: \t 0.01540\r",
                        "EPOCH: \t   137 \t LOSS: \t 0.01536\r",
                        "EPOCH: \t   138 \t LOSS: \t 0.01532\r",
                        "EPOCH: \t   139 \t LOSS: \t 0.01529\r",
                        "EPOCH: \t   140 \t LOSS: \t 0.01525\r",
                        "EPOCH: \t   141 \t LOSS: \t 0.01521\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   142 \t LOSS: \t 0.01517\r",
                        "EPOCH: \t   143 \t LOSS: \t 0.01514\r",
                        "EPOCH: \t   144 \t LOSS: \t 0.01510\r",
                        "EPOCH: \t   145 \t LOSS: \t 0.01506\r",
                        "EPOCH: \t   146 \t LOSS: \t 0.01503\r",
                        "EPOCH: \t   147 \t LOSS: \t 0.01499\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   148 \t LOSS: \t 0.01495\r",
                        "EPOCH: \t   149 \t LOSS: \t 0.01492\r",
                        "EPOCH: \t   150 \t LOSS: \t 0.01488\r",
                        "EPOCH: \t   151 \t LOSS: \t 0.01485\r",
                        "EPOCH: \t   152 \t LOSS: \t 0.01481\r",
                        "EPOCH: \t   153 \t LOSS: \t 0.01478\r",
                        "EPOCH: \t   154 \t LOSS: \t 0.01474\r",
                        "EPOCH: \t   155 \t LOSS: \t 0.01471\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   156 \t LOSS: \t 0.01467\r",
                        "EPOCH: \t   157 \t LOSS: \t 0.01464\r",
                        "EPOCH: \t   158 \t LOSS: \t 0.01460\r",
                        "EPOCH: \t   159 \t LOSS: \t 0.01457\r",
                        "EPOCH: \t   160 \t LOSS: \t 0.01453\r",
                        "EPOCH: \t   161 \t LOSS: \t 0.01450\r",
                        "EPOCH: \t   162 \t LOSS: \t 0.01447\r",
                        "EPOCH: \t   163 \t LOSS: \t 0.01443\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   164 \t LOSS: \t 0.01440\r",
                        "EPOCH: \t   165 \t LOSS: \t 0.01436\r",
                        "EPOCH: \t   166 \t LOSS: \t 0.01433\r",
                        "EPOCH: \t   167 \t LOSS: \t 0.01430\r",
                        "EPOCH: \t   168 \t LOSS: \t 0.01427\r",
                        "EPOCH: \t   169 \t LOSS: \t 0.01423\r",
                        "EPOCH: \t   170 \t LOSS: \t 0.01420\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   171 \t LOSS: \t 0.01417\r",
                        "EPOCH: \t   172 \t LOSS: \t 0.01414\r",
                        "EPOCH: \t   173 \t LOSS: \t 0.01410\r",
                        "EPOCH: \t   174 \t LOSS: \t 0.01407\r",
                        "EPOCH: \t   175 \t LOSS: \t 0.01404\r",
                        "EPOCH: \t   176 \t LOSS: \t 0.01401\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   177 \t LOSS: \t 0.01398\r",
                        "EPOCH: \t   178 \t LOSS: \t 0.01394\r",
                        "EPOCH: \t   179 \t LOSS: \t 0.01391\r",
                        "EPOCH: \t   180 \t LOSS: \t 0.01388\r",
                        "EPOCH: \t   181 \t LOSS: \t 0.01385\r",
                        "EPOCH: \t   182 \t LOSS: \t 0.01382\r",
                        "EPOCH: \t   183 \t LOSS: \t 0.01379\r",
                        "EPOCH: \t   184 \t LOSS: \t 0.01376\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   185 \t LOSS: \t 0.01373\r",
                        "EPOCH: \t   186 \t LOSS: \t 0.01370\r",
                        "EPOCH: \t   187 \t LOSS: \t 0.01367\r",
                        "EPOCH: \t   188 \t LOSS: \t 0.01364\r",
                        "EPOCH: \t   189 \t LOSS: \t 0.01361\r",
                        "EPOCH: \t   190 \t LOSS: \t 0.01358\r",
                        "EPOCH: \t   191 \t LOSS: \t 0.01355\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   192 \t LOSS: \t 0.01352\r",
                        "EPOCH: \t   193 \t LOSS: \t 0.01349\r",
                        "EPOCH: \t   194 \t LOSS: \t 0.01346\r",
                        "EPOCH: \t   195 \t LOSS: \t 0.01343\r",
                        "EPOCH: \t   196 \t LOSS: \t 0.01340\r",
                        "EPOCH: \t   197 \t LOSS: \t 0.01337\r",
                        "EPOCH: \t   198 \t LOSS: \t 0.01334\r",
                        "EPOCH: \t   199 \t LOSS: \t 0.01331\r",
                        "EPOCH: \t   200 \t LOSS: \t 0.01329\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   201 \t LOSS: \t 0.01326\r",
                        "EPOCH: \t   202 \t LOSS: \t 0.01323\r",
                        "EPOCH: \t   203 \t LOSS: \t 0.01320\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   204 \t LOSS: \t 0.01317\r",
                        "EPOCH: \t   205 \t LOSS: \t 0.01315\r",
                        "EPOCH: \t   206 \t LOSS: \t 0.01312\r",
                        "EPOCH: \t   207 \t LOSS: \t 0.01309\r",
                        "EPOCH: \t   208 \t LOSS: \t 0.01306\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   209 \t LOSS: \t 0.01303\r",
                        "EPOCH: \t   210 \t LOSS: \t 0.01301\r",
                        "EPOCH: \t   211 \t LOSS: \t 0.01298\r",
                        "EPOCH: \t   212 \t LOSS: \t 0.01295\r",
                        "EPOCH: \t   213 \t LOSS: \t 0.01293\r",
                        "EPOCH: \t   214 \t LOSS: \t 0.01290\r",
                        "EPOCH: \t   215 \t LOSS: \t 0.01287\r",
                        "EPOCH: \t   216 \t LOSS: \t 0.01285\r",
                        "EPOCH: \t   217 \t LOSS: \t 0.01282\r",
                        "EPOCH: \t   218 \t LOSS: \t 0.01279\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   219 \t LOSS: \t 0.01277\r",
                        "EPOCH: \t   220 \t LOSS: \t 0.01274\r",
                        "EPOCH: \t   221 \t LOSS: \t 0.01271\r",
                        "EPOCH: \t   222 \t LOSS: \t 0.01269\r",
                        "EPOCH: \t   223 \t LOSS: \t 0.01266\r",
                        "EPOCH: \t   224 \t LOSS: \t 0.01264\r",
                        "EPOCH: \t   225 \t LOSS: \t 0.01261\r",
                        "EPOCH: \t   226 \t LOSS: \t 0.01258\r",
                        "EPOCH: \t   227 \t LOSS: \t 0.01256\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   228 \t LOSS: \t 0.01253\r",
                        "EPOCH: \t   229 \t LOSS: \t 0.01251\r",
                        "EPOCH: \t   230 \t LOSS: \t 0.01248\r",
                        "EPOCH: \t   231 \t LOSS: \t 0.01246\r",
                        "EPOCH: \t   232 \t LOSS: \t 0.01243\r",
                        "EPOCH: \t   233 \t LOSS: \t 0.01241\r",
                        "EPOCH: \t   234 \t LOSS: \t 0.01238\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   235 \t LOSS: \t 0.01236\r",
                        "EPOCH: \t   236 \t LOSS: \t 0.01233\r",
                        "EPOCH: \t   237 \t LOSS: \t 0.01231\r",
                        "EPOCH: \t   238 \t LOSS: \t 0.01228\r",
                        "EPOCH: \t   239 \t LOSS: \t 0.01226\r",
                        "EPOCH: \t   240 \t LOSS: \t 0.01224\r",
                        "EPOCH: \t   241 \t LOSS: \t 0.01221\r",
                        "EPOCH: \t   242 \t LOSS: \t 0.01219\r",
                        "EPOCH: \t   243 \t LOSS: \t 0.01216\r",
                        "EPOCH: \t   244 \t LOSS: \t 0.01214\r",
                        "EPOCH: \t   245 \t LOSS: \t 0.01212\r",
                        "EPOCH: \t   246 \t LOSS: \t 0.01209\r",
                        "EPOCH: \t   247 \t LOSS: \t 0.01207\r",
                        "EPOCH: \t   248 \t LOSS: \t 0.01205\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EPOCH: \t   249 \t LOSS: \t 0.01202\r",
                        "EPOCH: \t   250 \t LOSS: \t 0.01200\r"
                    ]
                },
                {
                    "data": {
                        "application/pdf": "JVBERi0xLjQKJazcIKu6CjEgMCBvYmoKPDwgL1R5cGUgL0NhdGFsb2cgL1BhZ2VzIDIgMCBSID4+CmVuZG9iago4IDAgb2JqCjw8IC9Gb250IDMgMCBSIC9YT2JqZWN0IDcgMCBSIC9FeHRHU3RhdGUgNCAwIFIgL1BhdHRlcm4gNSAwIFIKL1NoYWRpbmcgNiAwIFIgL1Byb2NTZXQgWyAvUERGIC9UZXh0IC9JbWFnZUIgL0ltYWdlQyAvSW1hZ2VJIF0gPj4KZW5kb2JqCjExIDAgb2JqCjw8IC9UeXBlIC9QYWdlIC9QYXJlbnQgMiAwIFIgL1Jlc291cmNlcyA4IDAgUgovTWVkaWFCb3ggWyAwIDAgMzg0Ljg3OTM3NSAyOTcuMTgzODc1IF0gL0NvbnRlbnRzIDkgMCBSIC9Bbm5vdHMgMTAgMCBSID4+CmVuZG9iago5IDAgb2JqCjw8IC9MZW5ndGggMTIgMCBSIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nKWZTY8ctxGG7/0r+hgfQrFY/DxG+RDgm+wFcgh8UmTFhhTAMRD//TxVPTvd7J0ZrCMDa83WzpDF+njqZc+bv3z8708fPn737u365++XN/tvH35dZP2Zn09rXH/m57dV1nf8fFoiv31ZtOfQ29BW+PXz8dc0WpCunZefefP067+W5cflzZ9Y5lc+9m5ZUgylbB9TFhB7G4u3Fmo7mT9P5j7CeF71sMjRfNksbZt9wnkOEjpHYWuzLFpDG0limnffrTHo8+bLW6Lx2/IL/4/rHyOrqYYqKUtuLbV1hNS6pLJ++LK8fVre/E1WievTjx6vp38u/1j/EL9Zf1ifvl3++rS8X9yTRaKYw631yYWj+aEPo4Saco4aa+2v8aHccqKSolZyybMTB/NDJ6SQF606RGscr/FC4g03kqagtYlOXhysD51IKYWeeuStpbzKh1uhsGotpfc4F+TR/NiL3kOU0XIaechr3Ei3QqE1hVZi7nKqzN38uDSLhiSSeUPW9Co3bkbj2lhaQu8EIFldtBDP1ltOtJBWkhf7iLFlqb+vM/a92wg51xLnvXfrvb0bvZFHTS0BhEeF8GhzIf64L2ne/WC+t70IYYciucUq+VHoH+5fayi1UVDz/rv57v5WLC12wNCzPNhfH+3PCYKkTuVN+x/M9/ZPsQZVyb0SgP5g//xwfyqZA4xT5R3Md/fPI3TjeC1pPCq+ctz/l/XWSFIIJyxJ1IV///Nx/fv67zWt364Sig0U/iqldoqtcIRcL/81/tKidYjixPrdu3WesYdZw9KxjR7Vh2uIAx/qCkpCGpoHx8y2FoQ1PvYgI6Vs1mSFmHlv0hiItFQ3a1Bcbav0SBmWwcewZkZWEVsiVyZHLQ6QXMKQzplZowQC17YN2TulhlWUM8Fgf3MPhUBEpgL5KGOIWwm2xmxLpIxzo5eKuUgQzakR9c5qRLV3M6eQtYutoZalOryPSSqgGMrUSyXQXYTGzCXEXJov0lEAVWIzM/VFfxuTebfU0aud0YZR0awDc2cbHLeAlB5GYVDbrMfBONqwM1aiU4UiwVxD6hcraYUdtgZWaW2ImxkxtbemPm3qqPhqZtZrqdoa1Dzx2BypJeRWc/Y3J2Kd7TDkufWYfI0WMqkRi0gl5bRptPihYejbYWGtIwCvwRpon97bBvwW2Ryvske1U3TRlm6IhlEqaxB/XkpOloPGLBgjZxsaw2JTPaqNWoiabBEr66J9eze1ANiiKY9iANvqie15PWwR2sJC7cXQKAYTQNYsybah2s1MUqVXX8QOlqJYfjvVQHkArZ6s+rpHhNc4mqzNGkVbW7JYd2pBI1uatTBNPR7dmNJaq27t+GPrDmLKYoSabI3I8LWQCv4TxdSGman75CehUwOpSAxRzNqoZFuDPFDXI5dkZuIq2ZJoDSiIG4anmZNkT7lSIYyfmruZY7UA3zcv3y/v1/+PLCAtZMMCrVAabIklDxSGxfYRTuhVugMiTThptljmOOYrpUW+KoLNlBPlnifICB9D8PiEu9KEDCqV7CW7w0RYC10+ZpYIFVGVCa1HlpArrTgmE0osV4WUyIQSYTH0kdQyo0SsQ2rpeUIJ5R5KROeMCSX8bhWYPcsHlER7GYH2hBJOGfBo9DGhhJ4OLoVlYgYKi4CmXsoEDVMH1fm3E6OZn1k36xUYBI42k1aPvLB8KSGYcUHB05bcfyZa2Huj5s36DAvveKZ+mlhhSWRmNp1QwQKaJPkg30lhnZgTDk+gIMCMk6IzJyi6HgmOTpiw6rDBOyZKWH8ySDYfrpAw8tKFjqWdEVZ/BM8LcGJEK83a9MQI/sg9YWYEnlElHvMRbbClzVrQxe7ZIHyNunQfKAF1FDDu6M7kRoqvDVsAJRPordb9FMR8qxEfMqXQ6iaHga7VxUyCYjxHrd83fwUgopXShQnggRjlQvnSD8e/PGQFadXhJX9kRXZWjO3mfWWFBc/09MwKSytTZMgkSHiVBB2gR4JkG+xja/QrQPg8Ge/b568AsfRBXFcMV35Y/qNkD/LODwM8h00TPvCaSxt3oYkethezx8fXDg8rICrbp9fODjs3B5A6ocOLLdO1EznU8hTjtsIVHOZYyS6ndgXiRsTNLEDsuMTIj7uzxKQe0dQJJvw5Mj49XjtMhK62G3gcR5oYunAhbdYrTrycrZHyxBP2hwddZ6B4rdTssmsHCq4h53IZM1D8qQFz/wVQIsdrE1BYIfsN9cQTU4+bGrnyRE35KYl9wZOiWydOPNFIKPOZJzn24qydeIJEyPUFT4SV/Hox84STqx1tkKHCHltHUPtOjhjtCRUqwn0AJLk5UIi1ql0DjShWhpY5IY2UjnhB0KUu0RN1xr1KyLjzBLnVb4CDSdTum7+KJ921BtJB4YmYJhIGVDGebFccSv8hT9Rld09l4gm91mm2rjNPbH71i8rYecLlvkjabiFXnlgJ0FOewCtPysAtV4EzTkraSnvCCeNPfYQfcUIntBcwUaPFTBMTnDTPONOkNBVf4cCNbuKPi8rEDcQC06O6Wzs3zEE6p5y4YW14EcgHcFifivfLgRudyYaglTM3BtcFl7w7ODiEaRCf6wcVQl0PsjBxw4zEcJMmuwqh4ZrUPlPDXRjJp8IVGqSI4ZvGDA2xm7tJltONhTsSfo2ZGqZiU6V+yoSNkYNdGTdtceUGLWN3odHn+wpH7kxBlQkc1Jo9LTyBg7MEOl185Z0cOfpT1vSCHMSXt5/JoUhWOd1WTInUTU+dlAjaS14qkeT30JMSoabyDSWC/K6uRFDhxeS9s4Bm90v1CRH1coe8Y/4qcpTLPaUOewgSnzliD0GcIfYo7LES4WAd4M1KZOux5iNpUiJsME7gsIteE8/UBA6D8piuMiY6W9X+Uomg6r3LZyXiKnlChz2C08vjlhkeSLA+w8M8APX5DA/lmul+HaQILnCj2GTLjhSqrdrDzFmKsFvVrVgOUsTkVPF71yQvErMyzVcVd+ElJuy8RWTWF3ZgzVvbTUTgPpj0RAQbzO0sIxrw0JFPMqLZh2I53Uu440UtY76XZOsd1dO9xFZoLXnP7Tzo9mSJnM33Erv+I5Hidom58qBao+k2SCYh0bjalLOOQFbnfIYBB6MnzzCwOeDsmWBgVxgf90cZkTm5ul7Yv/7xKkVbu7qwrziYixcZgYvJn3M01++tbw8dONyN/jZlcIMGz+bntqeZ4/VrsrlNb39Vd+e7N9a7+SXel7tf4vGJ3/Vl4Pz+faWHO7xf/gcjW8ilCmVuZHN0cmVhbQplbmRvYmoKMTIgMCBvYmoKMjQ0NgplbmRvYmoKMTAgMCBvYmoKWyBdCmVuZG9iagoxNyAwIG9iago8PCAvTGVuZ3RoIDI0OSAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJxNUUmKAzAMu+cV+kAhXpO8p0OZQ+f/18oOhTkECa+Sk5aYWAsPMYQfLD34kSFzN/0bfqLZu1l6ksnZ/5jnIlNR+FKoLmJCXYgbz6ER8D2haxJZsb3xOSyjmXO+Bx+FuAQzoQFjfUkyuajmlSETTgx1HA5apMK4a2LD4lrRPI3cbvtGZmUmhA2PZELcGICIIOsCshgslDY2EzJZzgPtDckNWmDXqRtRi4IrlNYJdKJWxKrM4LPm1nY3Qy3y4Kh98fpoVpdghdFL9Vh4X4U+mKmZdu6SQnrhTTsizB4KpDI7LSu1e8TqboH6P8tS8P3J9/gdrw/N/FycCmVuZHN0cmVhbQplbmRvYmoKMTggMCBvYmoKPDwgL0xlbmd0aCA5NCAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJxFjcERwCAIBP9UQQkKCtpPJpOH9v+NEDJ8YOcO7oQFC7Z5Rh8FlSZeFVgHSmPcUI9AveFyLcncBQ9wJ3/a0FScltN3aZFJVSncpBJ5/w5nJpCoedFjnfcLY/sjPAplbmRzdHJlYW0KZW5kb2JqCjE5IDAgb2JqCjw8IC9MZW5ndGggODMgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicRYy7DcAwCER7pmAEfib2PlGUwt6/DRAlbrgn3T1cHQmZKW4zw0MGngwshl1xgfSWMAtcR1COneyjYdW+6gSN9aZS8+8PlJ7srOKG6wECQhpmCmVuZHN0cmVhbQplbmRvYmoKMjAgMCBvYmoKPDwgL0xlbmd0aCAzNDAgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicNVI5bgQxDOv9Cn0ggG7b79kgSJH8vw2p2RQDcXRSlDtaVHbLh4VUtex0+bSV2hI35HdlhcQJyasS7VKGSKi8ViHV75kyr7c1ZwTIUqXC5KTkccmCP8OlpwvH+baxr+XIHY8eWBUjoUTAMsXE6BqWzu6wZlt+lmnAj3iEnCvWLcdYBVIb3TjtiveheS2yBoi9mZaKCh1WiRZ+QfGgR4199hhUWCDR7RxJcIyJUJGAdoHaSAw5eyx2UR/0MygxE+jaG0XcQYElkpg5xbp09N/40LGg/tiMN786KulbWllj0j4b7ZTGLDLpelj0dPPWx4MLNO+i/OfVDBI0ZY2Sxget2jmGoplRVni3Q5MNzTHHIfMOnsMZCUr6PBS/jyUTHZTI3w4NoX9fHqOMnDbeAuaiP20VBw7is8NeuYEVShdrkvcBqUzogen/r/G1vtfXHx3tgMYKZW5kc3RyZWFtCmVuZG9iagoyMSAwIG9iago8PCAvTGVuZ3RoIDI1MSAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJwtUUlyA0EIu88r9IRmp99jlyuH5P/XCMoHBg2LQHRa4qCMnyAsV7zlkatow98zMYLfBYd+K9dtWORAVCBJY1A1oXbxevQe2HGYCcyT1rAMZqwP/Iwp3OjF4TEZZ7fXZdQQ7F2vPZlByaxcxCUTF0zVYSNnDj+ZMi60cz03IOdGWJdhkG5WGjMSjjSFSCGFqpukzgRBEoyuRo02chT7pS+PdIZVjagx7HMtbV/PTThr0OxYrPLklB5dcS4nFy+sHPT1NgMXUWms8kBIwP1uD/VzspPfeEvnzhbT43vNyfLCVGDFm9duQDbV4t+8iOP7jK/n5/n8A19gW4gKZW5kc3RyZWFtCmVuZG9iagoyMiAwIG9iago8PCAvTGVuZ3RoIDIxNSAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJw1UTkOAyEM7PcV/kAkjC94T6Iozf6/zYzRVh7BXIa0lCGZ8lKTqCHlUz56mS6cutzXzGo055a0LXOAuLa8L62SwIlmiIPBaZi4AZo8AUPX0ahRQxce0NSlUyiw3AQ+irduD91jtYGXtiHniSBiKBksQc2pRRMWbc8npDW/Xosb3pft3chTpcaWGIEGAVY4HNfo1/CVPU8m0XQVMtSrNcsYCRNFIjz5jqbVE+taNNIyEtTGEaxqA7w7/TBOAAATccsCZJ9KlLPkxG+x9LMGV/r+AZ9HVJYKZW5kc3RyZWFtCmVuZG9iagoxNSAwIG9iago8PCAvVHlwZSAvRm9udCAvQmFzZUZvbnQgL0JNUVFEVitEZWphVnVTYW5zIC9GaXJzdENoYXIgMCAvTGFzdENoYXIgMjU1Ci9Gb250RGVzY3JpcHRvciAxNCAwIFIgL1N1YnR5cGUgL1R5cGUzIC9OYW1lIC9CTVFRRFYrRGVqYVZ1U2FucwovRm9udEJCb3ggWyAtMTAyMSAtNDYzIDE3OTQgMTIzMyBdIC9Gb250TWF0cml4IFsgMC4wMDEgMCAwIDAuMDAxIDAgMCBdCi9DaGFyUHJvY3MgMTYgMCBSCi9FbmNvZGluZyA8PCAvVHlwZSAvRW5jb2RpbmcgL0RpZmZlcmVuY2VzIFsgNDggL3plcm8gL29uZSAvdHdvIC90aHJlZSAvZm91ciAvZml2ZSBdCj4+Ci9XaWR0aHMgMTMgMCBSID4+CmVuZG9iagoxNCAwIG9iago8PCAvVHlwZSAvRm9udERlc2NyaXB0b3IgL0ZvbnROYW1lIC9CTVFRRFYrRGVqYVZ1U2FucyAvRmxhZ3MgMzIKL0ZvbnRCQm94IFsgLTEwMjEgLTQ2MyAxNzk0IDEyMzMgXSAvQXNjZW50IDkyOSAvRGVzY2VudCAtMjM2IC9DYXBIZWlnaHQgMAovWEhlaWdodCAwIC9JdGFsaWNBbmdsZSAwIC9TdGVtViAwIC9NYXhXaWR0aCAxMzQyID4+CmVuZG9iagoxMyAwIG9iagpbIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwCjYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgMzE4IDQwMSA0NjAgODM4IDYzNgo5NTAgNzgwIDI3NSAzOTAgMzkwIDUwMCA4MzggMzE4IDM2MSAzMTggMzM3IDYzNiA2MzYgNjM2IDYzNiA2MzYgNjM2IDYzNiA2MzYKNjM2IDYzNiAzMzcgMzM3IDgzOCA4MzggODM4IDUzMSAxMDAwIDY4NCA2ODYgNjk4IDc3MCA2MzIgNTc1IDc3NSA3NTIgMjk1CjI5NSA2NTYgNTU3IDg2MyA3NDggNzg3IDYwMyA3ODcgNjk1IDYzNSA2MTEgNzMyIDY4NCA5ODkgNjg1IDYxMSA2ODUgMzkwIDMzNwozOTAgODM4IDUwMCA1MDAgNjEzIDYzNSA1NTAgNjM1IDYxNSAzNTIgNjM1IDYzNCAyNzggMjc4IDU3OSAyNzggOTc0IDYzNCA2MTIKNjM1IDYzNSA0MTEgNTIxIDM5MiA2MzQgNTkyIDgxOCA1OTIgNTkyIDUyNSA2MzYgMzM3IDYzNiA4MzggNjAwIDYzNiA2MDAgMzE4CjM1MiA1MTggMTAwMCA1MDAgNTAwIDUwMCAxMzQyIDYzNSA0MDAgMTA3MCA2MDAgNjg1IDYwMCA2MDAgMzE4IDMxOCA1MTggNTE4CjU5MCA1MDAgMTAwMCA1MDAgMTAwMCA1MjEgNDAwIDEwMjMgNjAwIDUyNSA2MTEgMzE4IDQwMSA2MzYgNjM2IDYzNiA2MzYgMzM3CjUwMCA1MDAgMTAwMCA0NzEgNjEyIDgzOCAzNjEgMTAwMCA1MDAgNTAwIDgzOCA0MDEgNDAxIDUwMCA2MzYgNjM2IDMxOCA1MDAKNDAxIDQ3MSA2MTIgOTY5IDk2OSA5NjkgNTMxIDY4NCA2ODQgNjg0IDY4NCA2ODQgNjg0IDk3NCA2OTggNjMyIDYzMiA2MzIgNjMyCjI5NSAyOTUgMjk1IDI5NSA3NzUgNzQ4IDc4NyA3ODcgNzg3IDc4NyA3ODcgODM4IDc4NyA3MzIgNzMyIDczMiA3MzIgNjExIDYwNQo2MzAgNjEzIDYxMyA2MTMgNjEzIDYxMyA2MTMgOTgyIDU1MCA2MTUgNjE1IDYxNSA2MTUgMjc4IDI3OCAyNzggMjc4IDYxMiA2MzQKNjEyIDYxMiA2MTIgNjEyIDYxMiA4MzggNjEyIDYzNCA2MzQgNjM0IDYzNCA1OTIgNjM1IDU5MiBdCmVuZG9iagoxNiAwIG9iago8PCAvZml2ZSAxNyAwIFIgL2ZvdXIgMTggMCBSIC9vbmUgMTkgMCBSIC90aHJlZSAyMCAwIFIgL3R3byAyMSAwIFIKL3plcm8gMjIgMCBSID4+CmVuZG9iagozIDAgb2JqCjw8IC9GMSAxNSAwIFIgPj4KZW5kb2JqCjQgMCBvYmoKPDwgL0ExIDw8IC9UeXBlIC9FeHRHU3RhdGUgL0NBIDAgL2NhIDEgPj4KL0EyIDw8IC9UeXBlIC9FeHRHU3RhdGUgL0NBIDEgL2NhIDEgPj4gPj4KZW5kb2JqCjUgMCBvYmoKPDwgPj4KZW5kb2JqCjYgMCBvYmoKPDwgPj4KZW5kb2JqCjcgMCBvYmoKPDwgPj4KZW5kb2JqCjIgMCBvYmoKPDwgL1R5cGUgL1BhZ2VzIC9LaWRzIFsgMTEgMCBSIF0gL0NvdW50IDEgPj4KZW5kb2JqCjIzIDAgb2JqCjw8IC9DcmVhdG9yIChNYXRwbG90bGliIHYzLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZykKL1Byb2R1Y2VyIChNYXRwbG90bGliIHBkZiBiYWNrZW5kIHYzLjguMykKL0NyZWF0aW9uRGF0ZSAoRDoyMDI0MDQyMzE3MjQwNCswMicwMCcpID4+CmVuZG9iagp4cmVmCjAgMjQKMDAwMDAwMDAwMCA2NTUzNSBmIAowMDAwMDAwMDE2IDAwMDAwIG4gCjAwMDAwMDY0NzIgMDAwMDAgbiAKMDAwMDAwNjI3OCAwMDAwMCBuIAowMDAwMDA2MzEwIDAwMDAwIG4gCjAwMDAwMDY0MDkgMDAwMDAgbiAKMDAwMDAwNjQzMCAwMDAwMCBuIAowMDAwMDA2NDUxIDAwMDAwIG4gCjAwMDAwMDAwNjUgMDAwMDAgbiAKMDAwMDAwMDM0NCAwMDAwMCBuIAowMDAwMDAyODg2IDAwMDAwIG4gCjAwMDAwMDAyMDggMDAwMDAgbiAKMDAwMDAwMjg2NSAwMDAwMCBuIAowMDAwMDA1MTI2IDAwMDAwIG4gCjAwMDAwMDQ5MTkgMDAwMDAgbiAKMDAwMDAwNDU3NCAwMDAwMCBuIAowMDAwMDA2MTc5IDAwMDAwIG4gCjAwMDAwMDI5MDYgMDAwMDAgbiAKMDAwMDAwMzIyOCAwMDAwMCBuIAowMDAwMDAzMzk0IDAwMDAwIG4gCjAwMDAwMDM1NDkgMDAwMDAgbiAKMDAwMDAwMzk2MiAwMDAwMCBuIAowMDAwMDA0Mjg2IDAwMDAwIG4gCjAwMDAwMDY1MzIgMDAwMDAgbiAKdHJhaWxlcgo8PCAvU2l6ZSAyNCAvUm9vdCAxIDAgUiAvSW5mbyAyMyAwIFIgPj4Kc3RhcnR4cmVmCjY2ODkKJSVFT0YK",
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "\n",
                "# Create dataset.\n",
                "x = [\n",
                "    [0, 0],\n",
                "    [1, 0],\n",
                "    [0, -1],\n",
                "    [-1, 0],\n",
                "    [0, 1]\n",
                "]\n",
                "labels = [1, 0, 0, 0, 0]\n",
                "\n",
                "# Glorot initialization for the parameters.\n",
                "def init_params(param_nodes: List[Var], bias_names: List[str]) -> None:\n",
                "    b = math.sqrt(6 / 4)\n",
                "    for param_node in param_nodes:\n",
                "        if param_node.name in bias_names:\n",
                "            param_node.set_value(0.0)\n",
                "        else:\n",
                "            param_node.set_value(uniform(-b, b))\n",
                "\n",
                "init_params(parameter_nodes, ['b1', 'b2', 'c'])\n",
                "\n",
                "# Training Loop.\n",
                "# Losses dict follows (index of sample : list of losses during training).\n",
                "loss_dict = {i: [] for i in range(len(x))}\n",
                "for ep in range(250):\n",
                "    ep_loss = 0.\n",
                "    for i in range(len(x)):\n",
                "        # Set the correct values for the inputs and label.\n",
                "        x1.set_value(x[i][0])\n",
                "        x2.set_value(x[i][1])\n",
                "        y.set_value(labels[i])\n",
                "\n",
                "        loss_dict[i].append(\n",
                "            loss.eval()\n",
                "        )\n",
                "        ep_loss += loss_dict[i][-1]\n",
                "\n",
                "        gradient_graphs = get_gradient_graphs(loss, parameter_nodes)\n",
                "        gradients_eval = {key: graph.simplify().eval() for key, graph in gradient_graphs.items()}\n",
                "\n",
                "        for parameter_node in parameter_nodes:\n",
                "            lr = 2.5 if ep < 100 else 0.5\n",
                "            new_val = parameter_node.value - lr * gradients_eval[parameter_node.name]\n",
                "            parameter_node.set_value(new_val)\n",
                "\n",
                "    print('EPOCH: \\t {:5} \\t LOSS: \\t {:.5f}'.format(ep + 1, float(ep_loss)), end='\\r')\n",
                "\n",
                "\n",
                "# Plot loss development.\n",
                "for sample_loss in loss_dict.values():\n",
                "    plt.plot(sample_loss)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa993de7",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "You can clearly see how the loss of each individual training sample evolves over time.\n",
                "This also explains the \"saddle\" you might have noticed in the loss curve from the\n",
                "previous lab.\n",
                "\n",
                "And these are the predictions for the five points:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "01cc3c15",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-04-23T15:24:04.711340Z",
                    "iopub.status.busy": "2024-04-23T15:24:04.711162Z",
                    "iopub.status.idle": "2024-04-23T15:24:04.714449Z",
                    "shell.execute_reply": "2024-04-23T15:24:04.713937Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SAMPLE:\t0\tTRUE:\t1\tPRED:\t0.996\n",
                        "SAMPLE:\t1\tTRUE:\t0\tPRED:\t0.002\n",
                        "SAMPLE:\t2\tTRUE:\t0\tPRED:\t0.002\n",
                        "SAMPLE:\t3\tTRUE:\t0\tPRED:\t0.002\n",
                        "SAMPLE:\t4\tTRUE:\t0\tPRED:\t0.002\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "for i in range(len(x)):\n",
                "    x1.set_value(x[i][0])\n",
                "    x2.set_value(x[i][1])\n",
                "    y.set_value(labels[i])\n",
                "\n",
                "    prediction = fout.eval()\n",
                "    print('SAMPLE:\\t{}\\tTRUE:\\t{}\\tPRED:\\t{:.3f}'.format(i, labels[i], prediction))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c1ab3a1c",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Conclusion\n",
                "What we did in this exercise is (a simplification of) how deep learning frameworks\n",
                "evaluate the code you write. You only need to define how to compute the output of\n",
                "the network, and the framework figures out the necessary gradients on its own.\n",
                "They provide a much better user interface, allowing you to use `+`, `-`, `/`, `*` etc.\n",
                "as you normally would instead of the clumsy node constructors we defined here,\n",
                "but there is always a computational graph hidden behind the curtains.\n",
                "\n",
                "\n",
                "\n",
                "## Exercise 2\n",
                "\n",
                "This exercise should improve your understanding of weight decay (or L2 regularization).\n",
                "\n",
                "  1. Consider a quadratic error function $E(\\textbf{w})=E_0+\\textbf{b}^T\\textbf{w}+1/2\\cdot\\textbf{w}^T\\textbf{H}\\textbf{w}$ and its regularized counterpart $E'(\\textbf{w})=E(\\textbf{w})+\\tau/2 \\cdot\\textbf{w}^T\\textbf{w}$, and let $\\textbf{w}^*$ and $\\tilde{\\textbf{w}}$ be the minimizers of $E$ and $E'$ respectively. We want to find a node to express $\\tilde{\\textbf{w}}$ as a function of $\\textbf{w}^*$, i.e. find the displacement introduced by weight decay.\n",
                "\n",
                "      - Find the gradients of $E$ and $E'$. Note that, at the global minimum,\n",
                "     we have $\\nabla E(\\textbf{w}^*)=\\nabla E'(\\tilde{\\textbf{w}})=0$.\n",
                "      - In the equality above, express $\\textbf{w}^*$ and $\\tilde{\\textbf{w}}$ as a\n",
                "     linear combination of the eigenvectors of $\\textbf{H}$.\n",
                "      - Through algebraic manipulation, obtain $\\tilde{\\textbf{w}}_i$ as a function\n",
                "     of $\\textbf{w}^*_i$.\n",
                "      - Interpret this result geometrically.\n",
                "      - Note: $\\textbf{H}$ is square, symmetric, and positive definite,\n",
                "     which means that its eigenvectors are pairwise orthogonal and its eigenvalues\n",
                "     are positive (spectral theorem).\n",
                "\n",
                "  2. Consider a linear network of the form $y=\\textbf{w}^T\\textbf{x}$ and the mean\n",
                "squared error as a loss function. Assume that every observation is corrupted\n",
                "with Gaussian noise $\\epsilon\\sim\\mathcal{N}(\\textbf{0}, \\sigma^2\\textbf{I})$.\n",
                "Compute the expectation of the gradient under $\\epsilon$ and,\n",
                "show that adding gaussian noise to the inputs has the same effect of weight decay.\n",
                "\n",
                "\n",
                "### Solution\n",
                "\n",
                "#### Question 1\n",
                "\n",
                "The error is computed as:\n",
                "\n",
                "\\begin{equation}\n",
                "    E(\\textbf{w})=E_0+\\sum_i w_ib_i+\\frac 1 2 \\sum_i\\sum_j w_iw_jh_{ij}\n",
                "\\end{equation}\n",
                "\n",
                "The derivative with respect to $w_i$ is, then:\n",
                "\n",
                "\\begin{equation}\n",
                "    \\frac{\\partial E}{\\partial w_i}=b_i+\\sum_j w_jh_{ij}\n",
                "\\end{equation}\n",
                "\n",
                "Where the factor $1/2$ was removed since the pair $w_i$ and $w_j$ is multiplied\n",
                "together twice, and $h_{ij}=h_{ji}$. In vector form:\n",
                "\n",
                "\\begin{equation}\n",
                "    \\nabla_{\\textbf{w}} E(\\textbf{w})=\\textbf{b}+\\textbf{H}\\textbf{w}\n",
                "\\end{equation}\n",
                "\n",
                "The same reasoning applied to $E'$ yields:\n",
                "\n",
                "\\begin{equation}\n",
                "\\nabla_{\\textbf{w}} E'(\\textbf{w})=\\textbf{b}+\\textbf{H}\\textbf{w}+\\tau\\textbf{w}\n",
                "\\end{equation}\n",
                "\n",
                "Now let $\\textbf{u}_i$ and $\\lambda_i$ be the eigenvectors and eigenvalues of\n",
                "$\\textbf{H}$, so that $\\textbf{H}\\textbf{u}_i=\\lambda_i\\textbf{u}_i$.\n",
                " Any vector $\\textbf{v}$ can then be expressed as\n",
                " $\\textbf{v}=\\sum_i\\gamma_i\\textbf{u}_i$. Now, note that\n",
                "\n",
                "\\begin{equation}\n",
                "\\textbf{H}\\textbf{v}=\\sum_i\\gamma_i\\textbf{H}\\textbf{u}_i=\\sum_i\\gamma_i\\lambda_i\\textbf{u}_i\n",
                "\\end{equation}\n",
                "\n",
                "\n",
                "Moreover, at the global minimum, both gradients equal zero, hence:\n",
                "\n",
                "\\begin{equation}\n",
                "\\textbf{b}+\\underbrace{\n",
                "  \\sum_i\\alpha_i\\lambda_i\\textbf{u}_i\n",
                "}_{\n",
                "  \\textbf{H}\\textbf{w}^*\n",
                "}\n",
                "=\n",
                "\\textbf{b}+\\underbrace{\n",
                "  \\sum_i\\beta_i\\lambda_i\\textbf{u}_i\n",
                "}_{\n",
                "  \\textbf{H}\\tilde{\\textbf{w}}\n",
                "}+\\tau\\underbrace{\n",
                "  \\sum_i\\beta_i\\textbf{u}_i\n",
                "  }_{\n",
                "    \\tilde{\\textbf{w}}\n",
                "  }\n",
                "\\Longleftrightarrow\n",
                "\\sum_i\\left( \\alpha_i\\lambda_i-\\beta_i\\lambda_i-\\tau\\beta_i \\right)\\textbf{u}_i=\\textbf{0}\n",
                "\\end{equation}\n",
                "\n",
                "Since the eigenvectors are linearly independent, the above expression is zero only\n",
                "when each term inside the sum is zero, i.e.\n",
                "\n",
                "\\begin{equation}\n",
                "\\alpha_i\\lambda_i-\\beta_i\\lambda_i-\\tau\\beta_i=0\n",
                "\\Longleftrightarrow \\beta_i=\\frac{\\lambda_i}{\\lambda_i+\\tau}\\alpha_i\n",
                "\\end{equation}\n",
                "\n",
                "Now, by replacing this into the expression for $\\tilde{\\textbf{w}}$, we get:\n",
                "\n",
                "\\begin{equation}\n",
                "\\tilde{\\textbf{w}}=\\beta^T\\textbf{u}=\\sum_i\\beta_i\\textbf{u}_i=\\sum_i\\frac{\\lambda_i}{\\lambda_i+\\tau}\\alpha_i\\textbf{u}_i\n",
                "\\end{equation}\n",
                "\n",
                "The eigenvalues of $\\textbf{H}$ indicate how much the error changes by moving in\n",
                "the direction of the corresponding eigenvector, with larger changes associated\n",
                "to smaller eigenvalues. In light of this, the node above is saying that the largest\n",
                "changes are applied to the weights that have little influence on the error,\n",
                "while \"important\" weights are not perturbed much.\n",
                "\n",
                "#### Question 2\n",
                "\n",
                "The prediction for $\\tilde{\\textbf{x}}=\\textbf{x}+\\epsilon$ is:\n",
                "\n",
                "\\begin{equation}\n",
                "\\tilde{y}=\\textbf{w}^T\\left(\\textbf{x}+\\epsilon\\right)=\\textbf{w}^T\\textbf{x}+\\textbf{w}^T\\epsilon\n",
                "\\end{equation}\n",
                "\n",
                "The error of this sample is\n",
                "\n",
                "\\begin{equation}\n",
                "\\tilde{E}=\\frac 1 2 \\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)^2\n",
                "\\end{equation}\n",
                "\n",
                "And its gradient with respect to a single weight is\n",
                "\n",
                "\\begin{align*}\n",
                "\\frac{\\partial\\tilde{E}}{\\partial w_i}\n",
                "&=\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)(-x_i-\\epsilon_i) \\\\\n",
                "&=-x_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)-\\epsilon_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)\n",
                "\\end{align*}\n",
                "\n",
                "The expectation with respect to $\\epsilon$ is\n",
                "\n",
                "\\begin{align*}\n",
                "\\mathbb{E}\\left[\\frac{\\partial\\tilde{E}}{\\partial w_i}\\right]\n",
                "&=\\mathbb{E}\\left[\n",
                "-x_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)\n",
                "\\right]+\\mathbb{E}\\left[\n",
                "-\\epsilon_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)\n",
                "\\right] \\\\\n",
                "&= -x_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}\\right)+\\mathbb{E}\\left[\n",
                "-\\epsilon_i\\hat{y}+\\epsilon_i\\textbf{w}^T\\textbf{x}+\\epsilon_i\\textbf{w}^T\\epsilon\n",
                "\\right] \\\\\n",
                "&\\stackrel{*}{=} \\frac{\\partial E}{\\partial w_i}+\\sum_j w_j\\mathbb{E}\\left[\\epsilon_i\\epsilon_j\\right] \\\\\n",
                "&= \\frac{\\partial E}{\\partial w_i}+w_i\\sigma^2    \n",
                "\\end{align*}\n",
                "\n",
                "Where we used $\\partial E/\\partial w_i$ to denote the gradient of the error of the de-noised sample, and the step marked with $*$ follows because $\\mathbb{E}[\\epsilon_i\\epsilon_j]=\\text{Cov}\\left[\\epsilon_i, \\epsilon_j\\right]=\\delta_{ij}\\sigma^2$.\n",
                "\n",
                "Clearly, the gradient is the same that results from weight decay.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}