{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "9f506b01-f3ba-4744-9953-6a8d30f24717",
            "metadata": {
                "id": "9f506b01-f3ba-4744-9953-6a8d30f24717",
                "tags": []
            },
            "source": [
                "# Deep Learning for NLP - Exercise 05\n",
                "In this exercise, we will use Llama-2 to learn about and experiment with various prompting strategies to solve classification tasks with text generation models.\n",
                "\n",
                "___\n",
                "General hints:\n",
                "* Have a look at the imports below when solving the tasks\n",
                "* Use the given modules and all submodules of the imports, but don't import anything else!\n",
                "    * For instance, you can use other functions under the `torch` or `nn` namespace, but don't import e.g. PyTorch Lightning, etc.\n",
                "* It is recommended to install all packages from the provided environment file\n",
                "* Feel free to test your code between sub-tasks of the exercise sheet, so that you can spot mistakes early (wrong shapes, impossible numbers, NaNs, ...)\n",
                "* Just keep in mind that your final submission should be compliant to the provided initial format of this file\n",
                "\n",
                "Submission guidelines:\n",
                "* Make sure that the code runs on package versions from the the provided environment file\n",
                "* Do not add or change any imports (also don't change the naming of imports, e.g. `torch.nn.functional as f`)\n",
                "* Remove your personal, additional code testings and experiments throughout the notebook\n",
                "* Do not change the class, function or naming structure as we will run tests on the given names\n",
                "* Additionally export this notebook as a `.py` file, and submit **both** the executed `.ipynb` notebook with plots in it **and** the `.py` file\n",
                "* **Deviation from the above guidelines will result in partial or full loss of points**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "1268ff50-1807-4b72-8b2c-49dc8b66e2dd",
            "metadata": {
                "id": "1268ff50-1807-4b72-8b2c-49dc8b66e2dd"
            },
            "outputs": [],
            "source": [
                "# !pip install datasets==2.13.1\n",
                "# !pip3 install transformers==4.33.2\n",
                "# !pip3 install optimum==1.13.2\n",
                "# !pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bcabf15a-63ad-43c5-84d7-4602002285cc",
            "metadata": {
                "id": "bcabf15a-63ad-43c5-84d7-4602002285cc"
            },
            "source": [
                "* The [13-billion](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) parameter, instruction-finetuned version of [Llama-2](https://arxiv.org/abs/2307.09288) will serve as our base model\n",
                "* However, we will use a quantized version of the model since loading the original model (even in 16-bit floating point precision) would require approximately 26GB of GPU vRAM\n",
                "* Due to that limitation, people started to distill and quantize [[1]](https://www.youtube.com/watch?v=2ETNONas068), [[2]](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34) models into low-precision versions, which dramatically reduced memory requirements for inference while still keeping the performance (depending on quantization technique and strength) above the lower-parameter, full-precision version\n",
                "* In our case, we will use the [gptq-4bit-32g-actorder_True](https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ) version from Hugging Face, which takes up \"only\" around 8-10GB of vRAM, making it possible to use it on the T4 GPUs of Google Colab.\n",
                "* This technique allows us to load and use a full 13 billion parameter model for inference\n",
                "    * For comparison, the BERT model from exercises 3 and 4 had around 110 *million*\n",
                "* Since the model file already requires 8GB of disk space and you need the current versions of transformers, optimum, and auto-gtpq (as indicated above), it is generally recommended to complete this exercise on Colab\n",
                "    * Of course, you can still do it locally if you have access to a suitable GPU, just be aware that you need the above packages with the current versions (as also described [on this page](https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "90fad656-3022-4c76-879e-ca7e3cfb8516",
            "metadata": {
                "id": "90fad656-3022-4c76-879e-ca7e3cfb8516"
            },
            "source": [
                "# Task 1: Prompting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "af97fae3-f405-437c-aba4-3b98111c986e",
            "metadata": {
                "id": "af97fae3-f405-437c-aba4-3b98111c986e",
                "tags": []
            },
            "outputs": [],
            "source": [
                "import re\n",
                "import json\n",
                "import random\n",
                "from collections import defaultdict, Counter\n",
                "from tqdm import tqdm, trange\n",
                "\n",
                "import torch\n",
                "\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "832b5808-7baa-4705-b2b9-16308fcfd952",
            "metadata": {
                "id": "832b5808-7baa-4705-b2b9-16308fcfd952"
            },
            "source": [
                "* Start by loading the [gptq-4bit-32g-actorder_True](https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ) version from Hugging Face\n",
                "* Use the `from_pretrained` method with the options `TheBloke/Llama-2-13B-chat-GPTQ` as the general model path, `'gptq-4bit-32g-actorder_True'` as the `revision`, use `device_map='auto'` and `trust_remote_code=False`\n",
                "* Then, load the corresponding tokenizer from the same general model path (no `revision` path required)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "08a321d6-aa35-4267-933a-073c92cab912",
            "metadata": {
                "id": "08a321d6-aa35-4267-933a-073c92cab912",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
                        "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
                        "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
                        "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
                        "CUDA extension not installed.\n",
                        "CUDA extension not installed.\n",
                        "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
                        "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
                    ]
                }
            ],
            "source": [
                "torch.cuda.empty_cache()\n",
                "\n",
                "model_name_or_path = 'TheBloke/Llama-2-13B-chat-GPTQ'\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,device_map=\"cuda:0\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,device_map=\"cuda:0\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a3db3ca1-e447-4be2-a869-3d2855049046",
            "metadata": {
                "id": "a3db3ca1-e447-4be2-a869-3d2855049046"
            },
            "source": [
                "* As previously, define the device"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "bf673444-e945-4ea6-970d-5a4dc3d11bdf",
            "metadata": {
                "id": "bf673444-e945-4ea6-970d-5a4dc3d11bdf",
                "tags": []
            },
            "outputs": [],
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa5d6f8d-1c7e-4fec-8e8d-cef7fd20ab2f",
            "metadata": {
                "id": "aa5d6f8d-1c7e-4fec-8e8d-cef7fd20ab2f",
                "tags": []
            },
            "source": [
                "## Task 1.1: Changing the system prompt"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0f6c9d43-0053-4f20-b69d-394a6a0d1436",
            "metadata": {
                "id": "0f6c9d43-0053-4f20-b69d-394a6a0d1436"
            },
            "source": [
                "* The system prompt serves as a guiding instruction for the LLM\n",
                "* It helps define the context, style, and focus of all subsequently generated text\n",
                "* For instance, you can put the model in different 'roles', meaning that it will answer in the style of a pirate, rapper, poet, teacher etc.\n",
                "* Choosing the 'right' system prompt for your task is crucial, as the task below will highlight\n",
                "* The default system prompt in LLama-2 (and also in ChatGPT) is, according to the [Llama-2 paper](https://arxiv.org/abs/2307.09288):\n",
                "> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\n",
                "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
                "Please ensure that your responses are socially unbiased and positive in nature.\\\n",
                "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
                "If you don’t know the answer to a question, please don’t share false information.\n",
                "* To embed instructions and system prompts, the training procedure of Llama-2 introduced some new tokens, which follow the HTML style\n",
                "    * `[INST]` and  `[/INST]` are used to mark the beginning and end of an instruction\n",
                "    * `<<SYS>>\\n` and `\\n<</SYS>>\\n\\n` are used to mark the beginning and end of the system prompt itself\n",
                "* Write a helper function `create_prompt` that takes in an `instruction`, `system_prompt`, and the start and end tokens of both the instruction and system prompt\n",
                "    * It should return the concatenation of an opening instruction prompt token, followed by the opening system prompt token, followed by the system prompt itself, then closed off by the ending system prompt token, followed by the instruction itself, and closed off by the end of instruction token.\n",
                "    * Save the default system prompt from above in a variable and use it as the default system prompt in the function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "26cf2b95-3d83-4636-be3a-02d41ec84a72",
            "metadata": {
                "id": "26cf2b95-3d83-4636-be3a-02d41ec84a72",
                "tags": []
            },
            "outputs": [],
            "source": [
                "B_INST, E_INST = '[INST]', '[/INST]'\n",
                "B_SYS, E_SYS = '<<SYS>>\\n', '\\n<<SYS>>\\n\\n'\n",
                "\n",
                "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
                "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\\\n",
                "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\\\n",
                "Please ensure that your responses are socially unbiased and positive in nature.\n",
                "\n",
                "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\\n",
                "If you don’t know the answer to a question, please don’t share false information.\\\n",
                "\"\"\"\n",
                "\n",
                "def create_prompt(instruction, system_prompt=DEFAULT_SYSTEM_PROMPT, b_inst=B_INST, e_inst=E_INST, b_sys=B_SYS, e_sys=E_SYS):\n",
                "    return f\"{b_inst}{b_sys}{system_prompt}{e_sys}{instruction}{e_inst}\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "036135d0-89d6-4ac5-9547-e3b4b20ddeb3",
            "metadata": {
                "id": "036135d0-89d6-4ac5-9547-e3b4b20ddeb3"
            },
            "source": [
                "* In principle, using these instruction fine-tuned models is the same as the decoding strategies from last exercise\n",
                "* However, we generally want to remove the input prompt itself from the output, since these finetuned models don't just complete our initial prompt but actually follow our instruction\n",
                "* Therefore, write a small helper function `format_output` that takes in the raw model outputs, the prompt, and the tokenizer\n",
                "    * The function first decodes the generated indices using the tokenizer\n",
                "        * Enable the option `skip_special_tokens=True` in the decoding function\n",
                "    * Remove the input prompt string\n",
                "    * Finally, remove all additional spaces in the beginning and end\n",
                "    * Return the cleaned string\n",
                "    * Hint: If you want a concrete example, play around with the tokenizer and model to generate some outputs. The tokenization $\\rightarrow$ generation $\\rightarrow$ decoding procedure is the same as in last exercise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "c28766d6-e3b9-485f-b3f6-5ddfbe29d852",
            "metadata": {
                "id": "c28766d6-e3b9-485f-b3f6-5ddfbe29d852",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def format_output(raw_out, prompt, tokenizer):\n",
                "    decoded = tokenizer.decode(raw_out, skip_special_tokens=True)\n",
                "    return decoded.replace(prompt, '').strip()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2db61efb-473e-4500-8fe3-49f4a59eefa0",
            "metadata": {
                "id": "2db61efb-473e-4500-8fe3-49f4a59eefa0"
            },
            "source": [
                "* Then, write a `generate` function that lets us reuse the same decoding and printing process, but flexibly change the instruction and system prompt\n",
                "* The function takes in the `instruction`, `system_prompt`, `model`, `tokenizer`, and `device`\n",
                "    * `system_prompt`, `model`, `tokenizer`, and `device` can be initialized as default with the respective default system prompt, model, tokenizer, and device\n",
                "* Use the `create_prompt` function to form the prompt using the given `instruction` and `system_prompt`\n",
                "* Tokenize the prompt to torch tensors and move them to the respective `device`\n",
                "* Run them through the model to get the output indices\n",
                "    * In this case, we use [top-p nucleus sampling](https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling) with the hyperparameters `temperature=0.7`, `do_sample=True`, `top_p=0.95`, `top_k=40`, and limit `max_new_tokens=512`\n",
                "* Then, we format the output using the `format_output` function to get the cleaned string from the returned indices\n",
                "* Return the cleaned string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "011781fa-88a5-40ca-80cd-d35b13a4f281",
            "metadata": {
                "id": "011781fa-88a5-40ca-80cd-d35b13a4f281",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def generate(instruction, system_prompt=DEFAULT_SYSTEM_PROMPT, model=model, tokenizer=tokenizer, device=device):\n",
                "    prompt = create_prompt(instruction, system_prompt)\n",
                "    tokenized = tokenizer(prompt, return_tensors='pt', truncation=True).to(device)\n",
                "    raw_out = model.generate(\n",
                "        **tokenized,\n",
                "        temperature=0.7,\n",
                "        do_sample=True,\n",
                "        top_p = 0.95,\n",
                "        top_k = 40,\n",
                "        max_new_tokens=512\n",
                "    )\n",
                "\n",
                "    output = format_output(raw_out[0], prompt, tokenizer)\n",
                "    return output"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "acca98d8-bc4e-4667-af49-24244af4554e",
            "metadata": {
                "id": "acca98d8-bc4e-4667-af49-24244af4554e"
            },
            "source": [
                "* Now, invent an instruction for the model, use the `generate` function with the default system prompt and output your results\n",
                "    * Always print the returned strings (when the output needs to be visible, such as in this case), so that newline and similar symbols are actually rendered"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "023c7239-b93d-48e9-a540-de41a5719544",
            "metadata": {
                "id": "023c7239-b93d-48e9-a540-de41a5719544"
            },
            "outputs": [
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m inst \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the concept of LLMs to a high school student.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(output) \u001b[38;5;66;03m# TODO\u001b[39;00m\n",
                        "Cell \u001b[1;32mIn[17], line 4\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(instruction, system_prompt, model, tokenizer, device)\u001b[0m\n\u001b[0;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m create_prompt(instruction, system_prompt)\n\u001b[0;32m      3\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m raw_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenized,\n\u001b[0;32m      6\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m      7\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     top_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m,\n\u001b[0;32m      9\u001b[0m     top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m     10\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m format_output(raw_out[\u001b[38;5;241m0\u001b[39m], prompt, tokenizer)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\generation\\utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2252\u001b[0m     )\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2256\u001b[0m         input_ids,\n\u001b[0;32m   2257\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2258\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2259\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2260\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2261\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2262\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2263\u001b[0m     )\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2275\u001b[0m     )\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\generation\\utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3261\u001b[0m     outputs,\n\u001b[0;32m   3262\u001b[0m     model_kwargs,\n\u001b[0;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3264\u001b[0m )\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:831\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    828\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 831\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    832\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    833\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    834\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    835\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    836\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    837\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    838\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    839\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    840\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    841\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    843\u001b[0m )\n\u001b[0;32m    845\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:589\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    577\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    578\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    579\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    586\u001b[0m         position_embeddings,\n\u001b[0;32m    587\u001b[0m     )\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    590\u001b[0m         hidden_states,\n\u001b[0;32m    591\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    592\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    593\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    594\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    595\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    596\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    597\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    598\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    599\u001b[0m     )\n\u001b[0;32m    601\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:348\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    347\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 348\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    351\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:186\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 186\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\activation.py:396\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\functional.py:2059\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   2058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 2059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "inst = \"Explain the concept of LLMs to a high school student.\"\n",
                "output = generate(inst)\n",
                "print(output) # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73fa5fbd-014e-4248-be41-b7c2353086e0",
            "metadata": {
                "id": "73fa5fbd-014e-4248-be41-b7c2353086e0"
            },
            "source": [
                "* Secondly, repeat the generation process while sticking to your instruction from before, but change the system prompt to something very different\n",
                "    * with \"very different\", it is meant that you let the model act as e.g. a car salesman, youtube influencer, rapper, magician, angry uncle, etc. instead of the default nice-and-helpful-assistant prompt\n",
                "    * You can either invent your own system prompt, or take inspiration from one of the below links\n",
                "    * https://github.com/mustvlad/ChatGPT-System-Prompts#prompts-by-category\n",
                "    * https://github.com/f/awesome-chatgpt-prompts#prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b51cf6d-3e38-4ab2-8c82-574d7471f868",
            "metadata": {
                "id": "0b51cf6d-3e38-4ab2-8c82-574d7471f868"
            },
            "outputs": [],
            "source": [
                "# TODO\n",
                "\n",
                "print() # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ce95941-3aae-4cf2-a075-c833856eb768",
            "metadata": {
                "id": "3ce95941-3aae-4cf2-a075-c833856eb768"
            },
            "source": [
                "* Compare the results and briefly discuss how the system prompt has influenced the model's output"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ada9f2a0-8f30-4091-9466-f584d6bde613",
            "metadata": {
                "id": "ada9f2a0-8f30-4091-9466-f584d6bde613"
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9fd02f51-fcb7-4979-8875-3bc1119fec82",
            "metadata": {
                "id": "9fd02f51-fcb7-4979-8875-3bc1119fec82",
                "tags": []
            },
            "source": [
                "## Task 1.2: Zero-Shot Inference"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c1686658-e36c-40f0-834c-189578cb8003",
            "metadata": {
                "id": "c1686658-e36c-40f0-834c-189578cb8003"
            },
            "source": [
                "* The [first GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) model showed that language modeling can produce successful results at downstream task applications, e.g. for applications in natural language inference.\n",
                "* [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) used the same architecture from GPT, but scaled it to 1.5B parameters and trained on 40GB web text data instead of the previous 4GB book corpus\n",
                "* A key ability observerd in GPT-2 was the *zero-shot learning* ability for multiple tasks, i.e. the model could complete tasks *without* examples and *without* gradient updates\n",
                "* By simply prompting the model in the right way, it could perform tasks where you previously needed supervised models for, e.g. classification\n",
                "* For instance, in the paper they simply prompt the model with a news article, followed by the string \"TL;DR:\", and the model was able to summarize the news article\n",
                "* As the model wasn't specifically trained for summarization, this ability is named *zero-shot inference*, since we evaluate the model's performance at the \"zero-th shot\", i.e. before it was trained to do the task"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c575ed09-0537-4673-82c7-c253785552a9",
            "metadata": {
                "id": "c575ed09-0537-4673-82c7-c253785552a9"
            },
            "source": [
                "* In the following, we will make use of the system prompt to perform a zero-shot, multi-class classification task on the [DAIR-AI Emotion Dataset](https://huggingface.co/datasets/dair-ai/emotion) from the last two weeks\n",
                "* Load the test split\n",
                "    * select only the first 100 samples (since it still takes some time to pass each sample through 13B parameters)\n",
                "    * add a new column `index` with the index number of the selected samples to get a permanent index for each sample across experiments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a4e26f97-48bb-4e52-9d7a-780269a958c7",
            "metadata": {
                "id": "a4e26f97-48bb-4e52-9d7a-780269a958c7",
                "tags": []
            },
            "outputs": [],
            "source": [
                "emotion_dataset = # TODO\n",
                "test_samples = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4dc149d0-2004-4960-a7ce-ed344b29ffed",
            "metadata": {
                "id": "4dc149d0-2004-4960-a7ce-ed344b29ffed",
                "tags": []
            },
            "source": [
                "### Task 1.2.1: Zero-shot classification"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1473a106-8b73-4b03-9110-ae99745ec049",
            "metadata": {
                "id": "1473a106-8b73-4b03-9110-ae99745ec049"
            },
            "source": [
                "* We will instruct the model to generate a label for each tweet\n",
                "* To achieve that, we derive a new system prompt that instructs the model to:\n",
                "    * read each sample\n",
                "    * output only a single word: the class as a string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1f542051-8d48-410e-9dbe-6bbe4f835c0d",
            "metadata": {
                "id": "1f542051-8d48-410e-9dbe-6bbe4f835c0d",
                "tags": []
            },
            "outputs": [],
            "source": [
                "CLASSIFICATION_PROMPT = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "420d5bf4-deb7-4cf6-954a-af25c9c75dfa",
            "metadata": {
                "id": "420d5bf4-deb7-4cf6-954a-af25c9c75dfa"
            },
            "source": [
                "* Sometimes, the model does not perfectly follow instructions, but still fulfills the task\n",
                "    * e.g. the model might add phrases like `\"Sure! Here is the result:\"` to the output, despite us instructing otherwise\n",
                "    * However, after such phrases, it might still add the correct output\n",
                "    * Therefore, we will have some mercy with the model as it has \"only\" 13B parameter and lost about 4x its current numerical precision, so we accept some errors\n",
                "* To deal with that, write a function `get_label_from_string` that checks whether *exactly one* of the labels appears in the output string\n",
                "    * The function takes as input the `output_str`, as well as the six `possible_labels` as a list of strings\n",
                "    * It returns the label if exactly one label was found, and `False` otherwise\n",
                "    * Make sure that your check is case sensitive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5b4c65d9-2149-42af-babf-7cd4bf4818ba",
            "metadata": {
                "id": "5b4c65d9-2149-42af-babf-7cd4bf4818ba",
                "tags": []
            },
            "outputs": [],
            "source": [
                "DATASET_LABELS = # TODO\n",
                "\n",
                "def get_label_from_string(output_str, possible_labels=DATASET_LABELS):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75630609-6336-4a1a-bec0-25d87465d065",
            "metadata": {
                "id": "75630609-6336-4a1a-bec0-25d87465d065"
            },
            "source": [
                "* Create a `lbl2idx` dictionary to map the strings to classification indices of the labels\n",
                "* Write a function `get_classification`\n",
                "    * It takes as input a `pred_label`, the `true_lbl`, and the `lbl2idx` helper dictionary\n",
                "    * Compare the predicted label to the label of the dataset\n",
                "    * Return `1` if the correct label has been predicted, else `0`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "779f491c-91af-459c-a23b-0fc65c4b7df8",
            "metadata": {
                "id": "779f491c-91af-459c-a23b-0fc65c4b7df8",
                "tags": []
            },
            "outputs": [],
            "source": [
                "lbl2idx = # TODO\n",
                "\n",
                "def get_classification(pred_label, true_label, lbl2idx):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f133cb18-46fa-4ff0-b3d7-815f4f628573",
            "metadata": {
                "id": "f133cb18-46fa-4ff0-b3d7-815f4f628573"
            },
            "source": [
                "* Iterate through all samples\n",
                "    * Embed the iterator in `tqdm` to visualize the current progress\n",
                "    * When generating new output, the text of each samples should be used as the `instruction` of the `generate` function, along with the new classification system prompt\n",
                "    * Put together all previously created functions and fill in the necessary logic to perform the classification task\n",
                "    * When evaluating the `get_label_from_string`, work in the logic to repeat this step for up to additionally 2 times if the model did not produce a valid string\n",
                "* If, after three attempts, still no valid label could be found, save the output string in a separate list and print some of these samples after the task\n",
                "    * Discuss what went wrong in these cases\n",
                "* Report the accuracy between the correct classifications and:\n",
                "    * all possible test samples\n",
                "    * the number of strings containing a label\n",
                "* Briefly comment the results. At which step(s) in the process does the model make mistakes? Remember that our supervised baseline from tutorial 3 was between 93%-96%.\n",
                "* HINT: Due to the inference time, it might be helpful to complete all functions and testing on only a small subset of the 100 samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "57bfc5a4-51b9-4849-85cc-e5ab8af0b97d",
            "metadata": {
                "id": "57bfc5a4-51b9-4849-85cc-e5ab8af0b97d",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b97f9e00-e2b3-4184-a570-c910c0f869e9",
            "metadata": {
                "id": "b97f9e00-e2b3-4184-a570-c910c0f869e9",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d1f781a5-e0c5-4d35-b374-eb60dd5c8d40",
            "metadata": {
                "id": "d1f781a5-e0c5-4d35-b374-eb60dd5c8d40",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98059c66-7820-4f38-9867-799762740a49",
            "metadata": {
                "id": "98059c66-7820-4f38-9867-799762740a49"
            },
            "source": [
                "___\n",
                "Students answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4fe3a30c-10f7-4707-bdef-5f722b90e71b",
            "metadata": {
                "id": "4fe3a30c-10f7-4707-bdef-5f722b90e71b",
                "tags": []
            },
            "source": [
                "### Task 1.2.2: Batch classification with structured in- and output"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5cd75299-be96-4bc1-854d-ff2cacd6f728",
            "metadata": {
                "id": "5cd75299-be96-4bc1-854d-ff2cacd6f728"
            },
            "source": [
                "* Now, we will perform the same task again but instruct the model to\n",
                "    * read a structured JSON input containing batched samples with their respective index\n",
                "    * produce another JSON output containing the batch of indices with their classification labels\n",
                "* First, derive a new system prompt that instructs the model to:\n",
                "    * read the JSON structure containing the batch\n",
                "    * classify each sample\n",
                "    * create a JSON output that links the indices of each sample to their classification\n",
                "    * each index (the keys) should be integers\n",
                "    * each class should be the labels as strings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4f7b76e-601d-4a2f-aac3-dff7ccf98082",
            "metadata": {
                "id": "e4f7b76e-601d-4a2f-aac3-dff7ccf98082",
                "tags": []
            },
            "outputs": [],
            "source": [
                "STRUCTURED_CLASSIFICATION_PROMPT = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d85e07ad-68d7-4530-8ecb-abe164a43a35",
            "metadata": {
                "id": "d85e07ad-68d7-4530-8ecb-abe164a43a35"
            },
            "source": [
                "* As before, the model might fail in producing the desired output\n",
                "    * It might preface the correct answer with some additional text, or it may say something after the JSON structure is generated\n",
                "    * It might also not produce a parseable JSON\n",
                "    * Or it might produce a parseable JSON but fail to produce valid indices and/or labels\n",
                "    * In the next functions, we check for all those possibilities\n",
                "* Write a function `get_parseable_json_from_string` to extract a parseable JSON structure from the model's `output_str` string\n",
                "    * The function should scan the output string and check whether it contains a parseable JSON\n",
                "    * If a parseable JSON structure has been found, return it, otherwise `False`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c5b6c3e-5e38-4d80-aaa1-9d9784972dc5",
            "metadata": {
                "id": "4c5b6c3e-5e38-4d80-aaa1-9d9784972dc5",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def get_parseable_json_from_string(output_str):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "45a730ff-4b30-4b14-ae46-dfef1cbe1271",
            "metadata": {
                "id": "45a730ff-4b30-4b14-ae46-dfef1cbe1271"
            },
            "source": [
                "* Adapt the earlier created `get_label_from_string` function to a batched version: `parse_labels_from_batch`\n",
                "    * It takes as input the `parseable_json` we returned from the `get_parseable_json_from_string` function, and the `possible_labels`\n",
                "    * Parse the batch and check that all generated labels actually exist\n",
                "    * Make sure that the check is case sensitive\n",
                "    * Return a list of tuples which contain for each sample:\n",
                "        * the index as type `int`\n",
                "        * `True` or `False`, depending on whether or not the label exists\n",
                "        * and the label itself"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "050970af-cf70-4668-8e8c-e2c85926c125",
            "metadata": {
                "id": "050970af-cf70-4668-8e8c-e2c85926c125",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def parse_labels_from_batch(parseable_json, possible_labels=DATASET_LABELS):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "346a9288-0343-411d-b2d7-1e7bc8a9159b",
            "metadata": {
                "id": "346a9288-0343-411d-b2d7-1e7bc8a9159b"
            },
            "source": [
                "* Write a function `has_correct_indices` to assess whether the model was able to (re-)produce the correct indices of the input\n",
                "    * The function takes as input the `parseable_json`, and a list of `true_indices`\n",
                "    * Transform the type `str` indices of the JSON to `int` for easier comparison and subsequent use after returning\n",
                "    * Return a list of tuples which contain for each sample:\n",
                "        * the type `int` index\n",
                "        * `True` or `False`, depending on whether the original index was (re-)produced\n",
                "    * It is not important whether the order is kept in the reproduced indices, only that all indices are present and correct\n",
                "    * If the (re-)produced indices contain more indices, pair those with `False`\n",
                "    * If the (re-)produced indices contain fewer indices, get the missing indices from the `true_indices` and pair those with `False`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8c6f7101-d3aa-44e0-8586-c2b8ba35e8c1",
            "metadata": {
                "id": "8c6f7101-d3aa-44e0-8586-c2b8ba35e8c1",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def has_correct_indices(parseable_json, true_indices):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a6ba7dbb-f744-4977-94f8-a0d04f0a4600",
            "metadata": {
                "id": "a6ba7dbb-f744-4977-94f8-a0d04f0a4600"
            },
            "source": [
                "* Now we need to track the indices of our classification output\n",
                "* The `update_index_dicts` function takes in the `output_str`, `true_indices`, a `correct_index_dict`, and a `incorrect_index_dict`\n",
                "    * The dictionaries will be created in the next step (you can scroll below to see, they will be `defaultdict(list)`, and `defaultdict(int)`\n",
                "    * The `correct_index_dict` maps the valid indices to a list of label predictions (there will be multiple predictions for each label)\n",
                "    * The `incorrect_index_dict` maps each invalid index to the number of tries which have been done to produce a valid index or label\n",
                "* As the first step, we pass the `output_str` through the `get_parseable_json_from_string` function\n",
                "    * If no parseable JSON structure was found, we iterate through all `true_indices` of the batch and increase their count by one in the `incorrect_index_dict`\n",
                "    * Afterwards, we return both `correct_index_dict` and `incorrect_index_dict` and end the function here\n",
                "* If a successful JSON structure was found, check the validity of labels and indices with `parse_labels_from_batch` and `has_correct_indices`\n",
                "    * Sort both outputs in ascending order of their respective indices to guarantee non-random ordering\n",
                "* Now we iterate through the outputs of the index and label parsing functions\n",
                "    * If the indices match in both pairs and the index is correct (as checked by its parsing function), and if the label is correct (as checked by its parsing function), we append the label to the list of its index in the `correct_index_dict`\n",
                "    * If the above condition is not met, be increate the counter in the `incorrect_index_dict`\n",
                "* We return the `correct_index_dict` and `incorrect_index_dict`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dbe9223e-5830-480d-9d07-f981bf890274",
            "metadata": {
                "id": "dbe9223e-5830-480d-9d07-f981bf890274",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def update_index_dicts(output_str, true_indices, correct_index_dict, incorrect_index_dict):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "11b5118e-f535-42e0-a1a2-66955458a4d9",
            "metadata": {
                "id": "11b5118e-f535-42e0-a1a2-66955458a4d9"
            },
            "source": [
                "* Now we put everything together\n",
                "* Iterate through all samples in batches of 10\n",
                "    * Embed the iterator in `tqdm` to visualize the current progress\n",
                "    * When generating new output, the text of each samples should be used as the `instruction` of the `generate` function, along with the new classification system prompt\n",
                "    * Put together all previously created functions and fill in the necessary logic to perform the classification task\n",
                "    * HINT: The returned `correct_index_dict` and `incorrect_index_dict` of one loop will serve as the input for the `update_index_dicts` function in the next loop\n",
                "    * Here, we additionally store `all_outputs`, which should save tuples of a batch of indices with their generated JSON output\n",
                "        * After the classification task, print 2 batches and discuss their outputs. What did the model do well? What didn't work?\n",
                "* HINT: Due to the inference time, it might be helpful to complete all functions and testing on only a small subset of the 100 samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71c766e5-29f7-4c04-9a7c-1db8dfcab783",
            "metadata": {
                "id": "71c766e5-29f7-4c04-9a7c-1db8dfcab783",
                "tags": []
            },
            "outputs": [],
            "source": [
                "BATCH_SIZE = # TODO\n",
                "NUM_VOTES = # TODO\n",
                "\n",
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "039d0427-520a-4319-bf2c-ef1d913811a3",
            "metadata": {
                "id": "039d0427-520a-4319-bf2c-ef1d913811a3"
            },
            "source": [
                "* After the loop, we now have 2 outputs:\n",
                "    * A dictionary containing the valid indicies with a list of at maximum length 3 (for 3 label predictions)\n",
                "    * A dictionary containing the invalid indices with the number of tries performed to produce a valid index and label prediction\n",
                "* Create a function `get_incomplete_indices`, which takes as input the `correct_index_dict`, the `true_indices`, and the `num_votes=NUM_VOTES` parameter\n",
                "    * It checks whether we already have 3 valid predictions per index\n",
                "    * If an index is missing entirely or in there are fewer than 3 predicted labels, store that index in a list\n",
                "    * Return that list of incomplete indices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da8754b6-d102-4ddc-b15f-5cb8bec0ddd5",
            "metadata": {
                "id": "da8754b6-d102-4ddc-b15f-5cb8bec0ddd5",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def get_incomplete_indices(correct_index_dict, true_indices, num_votes=NUM_VOTES):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6c3c4b5e-599f-4269-b624-3b1d95a9a012",
            "metadata": {
                "id": "6c3c4b5e-599f-4269-b624-3b1d95a9a012"
            },
            "source": [
                "* Until now, we have always tried to get predictions for each sample while keeping the ordering of samples inside batches equal\n",
                "* Now, take all invalid indices (returned from the `get_incomplete_indices` function), pair them together in batches, and try to fill up the vote count dictionaries to 3 predictions per label\n",
                "    * Create new batches of size 10 from the invalid indices\n",
                "        * For the last batch, simply take the remaining samples, even if they don't add up to 10\n",
                "    * Repeat the steps performed above (you can keep using the `correct_index_dict`\n",
                "        * Classify the invalid incies for an additional 5 tries OR until they have 3 valid predictions  \n",
                "            * In the end, you should have at maximum a counter of 8 for the invalid indices\n",
                "        * Run it"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ab3bc127-35f5-4ab5-9260-4c9e033d5588",
            "metadata": {
                "id": "ab3bc127-35f5-4ab5-9260-4c9e033d5588",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "35af92d8-26da-4e59-8330-b0eb2528d49b",
            "metadata": {
                "id": "35af92d8-26da-4e59-8330-b0eb2528d49b"
            },
            "source": [
                "* Now, we just have to extract the majority class vote per sample and calculate the accuracy\n",
                "* Create a function `get_majority_vote`, which takes in the `correct_index_dict`\n",
                "* Iterate through the dictionary and extract the labels based on the following logic:\n",
                "    * If only one valid vote exists, take it\n",
                "    * If all votes are distinct, randomly sample one (using the imported random package, do not hardcode a choice of your own)\n",
                "    * In all other cases, take the predicted label with the most votes\n",
                "* Return a dictionary mapping the valid indices to their final prediction class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0cd1f0e-ca8a-4109-8b47-6a6c1c9f1fc9",
            "metadata": {
                "id": "c0cd1f0e-ca8a-4109-8b47-6a6c1c9f1fc9",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def get_majoriy_vote(correct_index_dict):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98985fe2-5d15-4511-b0bf-cf945aa5545e",
            "metadata": {
                "id": "98985fe2-5d15-4511-b0bf-cf945aa5545e"
            },
            "source": [
                "* Compare the majority votes to the ground truth class label of the dataset\n",
                "    * Since the model very likely still could not produce valid indices and/or classes for some samples, make sure to compare the correct indices\n",
                "* Report two accuracy metrics:\n",
                "    * The overall accuracy based on all possible test samples\n",
                "    * The accuracy given a valid index-label sample\n",
                "* Briefly discuss these results and compare them to the accuracy numbers we obtained in task 1.2.1\n",
                "* Also, remember to print the 2 batches that we saved in `all_outputs` and discuss what the model did well and what didn't work."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8a7436b4-4cfe-434f-ae15-ab461aa84807",
            "metadata": {
                "id": "8a7436b4-4cfe-434f-ae15-ab461aa84807",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "73323e25-a98f-4160-981a-9cbac144607d",
            "metadata": {
                "id": "73323e25-a98f-4160-981a-9cbac144607d",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee368061-2312-47bd-8343-2664f6a2ac9d",
            "metadata": {
                "id": "ee368061-2312-47bd-8343-2664f6a2ac9d",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "59c71a60-1453-4ff2-b855-dd64da1fb7a1",
            "metadata": {
                "id": "59c71a60-1453-4ff2-b855-dd64da1fb7a1"
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "237de223-3b4b-407c-ac62-9afcf8d63ed0",
            "metadata": {
                "id": "237de223-3b4b-407c-ac62-9afcf8d63ed0"
            },
            "source": [
                "## Task 1.3: Few-Shot Learning"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "817abf08-5179-4567-af11-f17b62bc3f7a",
            "metadata": {
                "id": "817abf08-5179-4567-af11-f17b62bc3f7a"
            },
            "source": [
                "* [GPT-3](https://arxiv.org/abs/2005.14165) was introduced by a paper titled \"Language Models are Few-Shot Learners\"\n",
                "* It showed that with increasing model size (from 1.5 billion parameters to 175 billion parameters) and increasing pre-training data size (from 40GB to >600 GB), \"few-shot\" learning abilities were observable in LLMs\n",
                "* When the LLM is given a new task with some examples of the task (i.e. \"few-shots\") before asking the question, it can learn this task\n",
                "* Again, no gradients are updated\n",
                "* As a result, this learning is also called \"in-context learning\", underscoring that the task is learned \"in context\" without updating the parameters\n",
                "* For instance, a prompt that makes use of 3-shot learning to teach a model to translate English to French would look like:\n",
                "```\n",
                "Translate English to French:\n",
                "\n",
                "sea otter => loutre de mer\n",
                "peppermint => menthe poivrée\n",
                "plush girafe => girafe peluche\n",
                "cheese => .........\n",
                "```\n",
                "\n",
                "* In the following, we will work with the [FewRel](https://huggingface.co/datasets/few_rel) dataset\n",
                "* It contains textual descriptions of relations\n",
                "* The goal is to classify whether a relation exists between two entities mentioned in the text\n",
                "* The dataset includes various sets of relations, making it suitable for few-shot learning"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "926dc119-deb0-4da4-96e2-c1fd7124689c",
            "metadata": {
                "id": "926dc119-deb0-4da4-96e2-c1fd7124689c"
            },
            "source": [
                "* Start by loading the validation split of the dataset, specifically the `split='val_wiki'`\n",
                "* Select the first 10 samples as train split and the next 50 samples as validation samples\n",
                "    * We choose these splits to minimize the distribution shift for this exercise because we only validate on 50 samples due to runtime constraints"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "81645542-5292-4217-9869-6f739878d7e4",
            "metadata": {
                "id": "81645542-5292-4217-9869-6f739878d7e4",
                "tags": []
            },
            "outputs": [],
            "source": [
                "fewrel_train = # TODO\n",
                "fewrel_val = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cf1d9794-ee50-4b86-8011-e2acad0c75d2",
            "metadata": {
                "id": "cf1d9794-ee50-4b86-8011-e2acad0c75d2"
            },
            "source": [
                "* Get familiar with the layout of the dataset\n",
                "* A sample contains\n",
                "    * `relation`: a string feature containing PID of the relation\n",
                "    * `tokens`: a list of string features containing tokens for the text\n",
                "    * `head`: a dictionary containting\n",
                "        * `text`: a string feature representing the head entity\n",
                "        * `type`: a string feature representing the type of the head entity\n",
                "        * `indices`: a list containing list of token indices\n",
                "    * tail: a dictionary containing:\n",
                "        * `text`: a string feature representing the tail entity\n",
                "        * `type`: a string feature representing the type of the tail entity\n",
                "        * `indices`: a list containing list of token indices\n",
                "    * `names`: a list of string features containing relation names"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6b72a392-7bed-4e01-81c7-4aa040b47184",
            "metadata": {
                "id": "6b72a392-7bed-4e01-81c7-4aa040b47184"
            },
            "source": [
                "* In this exercise, we will provide 3-shot examples of `tokens` along with their `names` and `head[text]` as well as `tail[text]` to extract the `head`- and `tail`-texts of new token sequences.\n",
                "* To achieve this, we will use a random selection mechanism of few-shot samples from the train split and evaluate on the 50 samples from the validation split\n",
                "* First, we need to design our system prompt template\n",
                "    * Here, the system prompt should contain\n",
                "        * the task description\n",
                "        * the context as a string (i.e. not the list of strings from above) given by the `tokens`\n",
                "        * the descriptions from the `names` list. The first list item is the name of the relation, the second list item the description of the relation\n",
                "        * the entities, as given by `head[text]` and `tail[text]`\n",
                "    * The prompt template should reserve the space and positions for all 3 sampled examples\n",
                "    * Furthermore, for subsequent parsing and text generation speed, it will be helpful to add instructions to only return the next, incomplete example\n",
                "    * Experiment with the template and some samples until you find a suitable version\n",
                "    * Embed this structure in a function `get_few_shot_sys_prompt`\n",
                "        * It takes the `train_set` as input and the `num_shots`\n",
                "        * Inside the function, sample `num_shots` indices or samples from the train set\n",
                "        * Fill in the `num_shots` train samples' context, descriptions, head, and tail text inside the earlier created template\n",
                "    * Return the complete prompt string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4dcda4f5-276f-46c1-8843-d87042c8d741",
            "metadata": {
                "id": "4dcda4f5-276f-46c1-8843-d87042c8d741",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def get_few_shot_sys_prompt(train_set, num_shots=3):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8d3597c6-6466-4648-b572-492e8baa9ca3",
            "metadata": {
                "id": "8d3597c6-6466-4648-b572-492e8baa9ca3"
            },
            "source": [
                "* Secondly, we need to prepare our `instruction` for the to-be-completed samples from the validation set\n",
                "* Create a function `get_few_shot_instruction`\n",
                "    * It takes in a sample of the validation set (as we will later iterate through the validation dataset)\n",
                "    * The function body is structured similar to one of the examples from the system prompt template\n",
                "    * It consists of:\n",
                "        * the instruction to fill up the blanks below (optional, depending on how you ended the system prompt)\n",
                "        * the \"context\" and the \"names\" descriptions\n",
                "    * Instead of completing the `head` and `tail`, leave those positions empty, e.g. in the style of\n",
                "    ```\n",
                "    head entity:\n",
                "    tail entity:\n",
                "    ```\n",
                "    * Return the instruction string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a890578-8fb0-4f0f-bd4d-f89f72744a2a",
            "metadata": {
                "id": "2a890578-8fb0-4f0f-bd4d-f89f72744a2a",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def get_few_shot_instruction(val_sample):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "76e5fd12-c737-4c94-a3ad-4cb19c424abf",
            "metadata": {
                "id": "76e5fd12-c737-4c94-a3ad-4cb19c424abf"
            },
            "source": [
                "* Thirdly, we need a fuction `get_valid_relations` to again check the validity of the output\n",
                "* Model this function in the style of `get_parseable_json_from_string`, which also takes as input the `output_str`\n",
                "    * But adjust it to the individual structure you chose to output the `head` and `tail`\n",
                "        * i.e. if you chose to output head and tail in separate lines, with or without preceding `:` or not, etc.\n",
                "        * parse and clean the output as needed\n",
                "    * return a tuple `(head, tail)` if parsing was successful, else `False`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a68c7e7-677a-4a8a-a97a-51619fa49ea8",
            "metadata": {
                "id": "2a68c7e7-677a-4a8a-a97a-51619fa49ea8",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def get_valid_relations(output_str):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "840adab2-f46b-4acc-bcf9-2cdf636ad816",
            "metadata": {
                "id": "840adab2-f46b-4acc-bcf9-2cdf636ad816"
            },
            "source": [
                "* Then, write a function `get_relation_classification`\n",
                "    * It takes in the tuple of extracted entities, as well as a tuple of ground-truth labels' head and tail entities\n",
                "    * Compare both lower-cased head and tail strings\n",
                "    * If both match, return 2\n",
                "    * If one matches, return 1\n",
                "    * If none match, return 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "197fbb12-3e1c-45f1-a0f7-286169b098bb",
            "metadata": {
                "id": "197fbb12-3e1c-45f1-a0f7-286169b098bb",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def get_relation_classification(extracted, labels):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c621e6c-670f-47ca-8b98-e70c187216ad",
            "metadata": {
                "id": "7c621e6c-670f-47ca-8b98-e70c187216ad"
            },
            "source": [
                "* Finally, iterate through the 50 validation set samples\n",
                "    * Embed the iterator in `tqdm` to visualize the current progress\n",
                "    * Put together all previously created functions and fill in the necessary logic to perform the classification task\n",
                "        * Separately save the samples in a list when `get_valid_relations` returned `False`\n",
                "    * Use the validation sample for the `instruction`, and the sampled 3-shot examples in the system prompt position of the `generate` function\n",
                "    * As previously, if we receive invalid output, try for an additional 2 times\n",
                "* Report the accuracy\n",
                "    * against all validation samples\n",
                "    * against validation samples with valid head and tail outputs\n",
                "    * make sure that each 'sample' accounts for 2 relations in your accuracy calculation\n",
                "* Briefly comment the results. At which step(s) in the process does the model make mistakes?\n",
                "* Show examples, if they exist, of the invalidly created outputs of the model and discuss what the model did wrong.\n",
                "* Discuss 2 examples of the validly created outputs of the model where they don't match the label relations.\n",
                "    \n",
                "* HINT: Due to the inference time, it might be helpful to complete all functions and testing on only a small subset of the 50 samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c933b73c-f0ba-4e2e-8217-56874667fe42",
            "metadata": {
                "id": "c933b73c-f0ba-4e2e-8217-56874667fe42",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3d27273b-d5d4-4d3b-9525-acdc87ba1ea0",
            "metadata": {
                "id": "3d27273b-d5d4-4d3b-9525-acdc87ba1ea0",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "076082cb-1382-4a69-b771-dbe871532e89",
            "metadata": {
                "id": "076082cb-1382-4a69-b771-dbe871532e89"
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bab0e2c2-4ef9-4b83-b989-4def5be9792f",
            "metadata": {
                "id": "bab0e2c2-4ef9-4b83-b989-4def5be9792f",
                "tags": []
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3cf4bad3-2197-4a74-bfea-16d1d313653f",
            "metadata": {
                "id": "3cf4bad3-2197-4a74-bfea-16d1d313653f"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "temp_dl4nlp",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "undefined.undefined.undefined"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
