{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "9f506b01-f3ba-4744-9953-6a8d30f24717",
            "metadata": {
                "id": "9f506b01-f3ba-4744-9953-6a8d30f24717"
            },
            "source": [
                "# Deep Learning for NLP - Exercise 03\n",
                "In this exercise, we will be using the Hugging Face ecosystem to finetune BERT for a classification task (part 1) and T5 for text summarization (part 2). Then, we will upload our training and validation results via TensorBoard, which makes interactive sharing and inspecting of results very easy.\n",
                "\n",
                "Part 1 and part 2 can be worked on independently.\n",
                "\n",
                "___\n",
                "General hints:\n",
                "* Have a look at the imports below when solving the tasks\n",
                "* Use the given modules and all submodules of the imports, but don't import anything else!\n",
                "    * For instance, you can use other functions under the `torch` or `nn` namespace, but don't import e.g. PyTorch Lightning, etc.\n",
                "* It is recommended to install all packages from the provided environment file\n",
                "* Feel free to test your code between sub-tasks of the exercise sheet, so that you can spot mistakes early (wrong shapes, impossible numbers, NaNs, ...)\n",
                "* Just keep in mind that your final submission should be compliant to the provided initial format of this file\n",
                "\n",
                "Submission guidelines:\n",
                "* Make sure that the code runs on package versions from the the provided environment file\n",
                "* Do not add or change any imports (also don't change the naming of imports, e.g. `torch.nn.functional as f`)\n",
                "* Remove your personal, additional code testings and experiments throughout the notebook\n",
                "* Do not change the class, function or naming structure as we will run tests on the given names\n",
                "* Additionally export this notebook as a `.py` file, and submit **both** the executed `.ipynb` notebook with plots in it **and** the `.py` file\n",
                "* **Deviation from the above guidelines will result in partial or full loss of points**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eaa3cf98-b17b-4c72-b876-c1f1d67b1486",
            "metadata": {
                "id": "eaa3cf98-b17b-4c72-b876-c1f1d67b1486"
            },
            "outputs": [],
            "source": [
                "# !pip install transformers==4.24.0\n",
                "# !pip install datasets==3.0.1\n",
                "# !pip install evaluate==0.4.0\n",
                "# !pip install rouge-score==0.1.2\n",
                "# !pip install py7zr"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a040d626-232c-4f9a-be5e-aaad77ecfbb0",
            "metadata": {
                "id": "a040d626-232c-4f9a-be5e-aaad77ecfbb0",
                "tags": []
            },
            "source": [
                "# Task 1: Sequence Classification with BERT"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "12fbdc10-854d-4c00-97b1-cedcd0bdde77",
            "metadata": {
                "id": "12fbdc10-854d-4c00-97b1-cedcd0bdde77"
            },
            "source": [
                "* In this task, we will finetune [BERT](https://huggingface.co/bert-base-uncased) on a custom sentiment classification dataset and perform multi-class sequence classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d5529e86-d1c1-4414-b0d3-56d6931f21e5",
            "metadata": {
                "id": "d5529e86-d1c1-4414-b0d3-56d6931f21e5"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import random\n",
                "import numpy as np\n",
                "from sklearn.metrics import f1_score\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "from datasets import load_dataset\n",
                "\n",
                "from transformers import (\n",
                "    Trainer,\n",
                "    TrainingArguments,\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSequenceClassification,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67aefa49-05b1-4b82-ba9d-ad0ebf4742f5",
            "metadata": {
                "id": "67aefa49-05b1-4b82-ba9d-ad0ebf4742f5"
            },
            "outputs": [],
            "source": [
                "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
                "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # there might be interferences with the parallelism of the Hugging Face Trainer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1cdb73ae-5cba-4c54-a65d-7af604c22fd9",
            "metadata": {
                "id": "1cdb73ae-5cba-4c54-a65d-7af604c22fd9"
            },
            "source": [
                "We start by downloading the [DAIR-AI Emotion Dataset](https://huggingface.co/datasets/dair-ai/emotion) from the Hugging Face Hub, which already comes with a train, validation, and test split of 16,000 - 2,000 - 2,000 samples. The dataset consists of tweets with their labeled emotions over 6 different classes.\n",
                "\n",
                "* As the dataset, by default, consists of integer labels, we first create two helper dictionaries\n",
                "    * `idx2lbl`, which maps the integer index to the label string\n",
                "    * `lbl2idx`, which maps the label strings to the integer index\n",
                "    * The downloaded Hugging Face dataset object implements a useful `.features` method for each dataset split, which contains the label strings in the correct order corresponding to the integers (the labels are consistent across each split, so it is enough to only inspect one split)\n",
                "* Try to get an overview of the data and the dataset format by printing 10 random (use the `random` library to sample random numbers!) samples of the train split with their corresponding integer and string labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "90261bf4-0b3f-44ae-af4c-2d2ed2d58d36",
            "metadata": {
                "id": "90261bf4-0b3f-44ae-af4c-2d2ed2d58d36"
            },
            "outputs": [],
            "source": [
                "emotion_dataset = # TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb96f49b-f101-47cb-8d69-febe03928969",
            "metadata": {
                "id": "cb96f49b-f101-47cb-8d69-febe03928969"
            },
            "outputs": [],
            "source": [
                "idx2lbl = # TODO\n",
                "lbl2idx = # TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4a160b43-c6ab-47fb-a727-907ec2a4c04a",
            "metadata": {
                "id": "4a160b43-c6ab-47fb-a727-907ec2a4c04a"
            },
            "outputs": [],
            "source": [
                "rand_idxs = # TODO\n",
                "\n",
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cd221019-21e4-4dcc-9cdb-fb37d10140ab",
            "metadata": {
                "id": "cd221019-21e4-4dcc-9cdb-fb37d10140ab"
            },
            "source": [
                "To start the preprocessing steps, we first need to load the pre-trained tokenizer for our [bert-base-uncased](https://huggingface.co/bert-base-uncased) model, which can be achieved very comfortable using the `AutoTokenizer` [class API](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) from Hugging Face."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "46d58ef2-7f79-4985-8dc1-2f8743c96e80",
            "metadata": {
                "id": "46d58ef2-7f79-4985-8dc1-2f8743c96e80"
            },
            "outputs": [],
            "source": [
                "tokenizer = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3074cb17-ec06-4ea5-8050-37686c6e0ee4",
            "metadata": {
                "id": "3074cb17-ec06-4ea5-8050-37686c6e0ee4"
            },
            "source": [
                "If we enter an example input from the train set into the tokenizer, we can see that we receive again dictionaries as output, which is the preferred data format from Hugging Face."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f946f992-2676-49dc-b6a2-c0ecc9e45502",
            "metadata": {
                "id": "f946f992-2676-49dc-b6a2-c0ecc9e45502"
            },
            "outputs": [],
            "source": [
                "example_seq = # TODO\n",
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3159455f-03c7-4fe0-ae53-36ab44aafa65",
            "metadata": {
                "id": "3159455f-03c7-4fe0-ae53-36ab44aafa65"
            },
            "source": [
                "* We can see that the dictionary contains `input_ids`, which represent the tokenized sequence *including* specials tokens\n",
                "    * For instance, notice that the sequence begins with `101`, which is used by BERT to mark the `[CLS]` token\n",
                "    * The sequence ends with the `102` token, which BERT uses as a separation mark between sequences, `[SEP]`\n",
                "    * You can check all special tokens using `tokenizer.all_special_tokens` and `tokenizer.all_special_ids`\n",
                "* Secondly, the tokenizer contains `token_type_ids`, which are used to distinguish different segments of input tokens in models that support token-level type embeddings, like BERT and its variants\n",
                "    * These embeddings are useful for tasks like sentence pair classification or question answering, where the model needs to understand which tokens belong to which part of the input. (not needed here)\n",
                "* Thirdly, the tokenizer automatically creates the attention mask, which consists of all `1` in this case\n",
                "    * The attention mask in input sequences is usually only `0` when padding symbols are added, as we don't want to attend to padding symbols"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "02af8005-8c7c-4dce-b34a-074576892e81",
            "metadata": {
                "id": "02af8005-8c7c-4dce-b34a-074576892e81"
            },
            "source": [
                "Following this, we can use the tokenizer to tokenize our entire dataset.\n",
                "\n",
                "* Hugging Face dataset objects provide a way to `map` a function onto every single object using powerful parallel processing operations\n",
                "* There, we need to create a function which acts like it:\n",
                "    * takes in text examples of a data split of our dataset object\n",
                "    * inputs it into the tokenizer\n",
                "        * use the tokenizer with options `truncation=True` and `max_length=512`, since the maximum context size of BERT is 512 tokens\n",
                "        * This shouldn't impact our dataset in any way since the Tweet dataset stems from the times when tweets were limited to 280 characters, but it's better to be safe than sorry and have your training interrupted\n",
                "    * return the output of the tokenizer\n",
                "* Apply the function onto the entire dataset object using its `map` method with the option `batched=True`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "827f4441-849c-4e8c-b4cf-07721fd1e823",
            "metadata": {
                "id": "827f4441-849c-4e8c-b4cf-07721fd1e823"
            },
            "outputs": [],
            "source": [
                "def tokenize_seqs(examples):\n",
                "    return # TODO\n",
                "\n",
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e4e1605-da0d-4dff-868b-720ae98f06a1",
            "metadata": {
                "id": "0e4e1605-da0d-4dff-868b-720ae98f06a1"
            },
            "source": [
                "* Lastly, the [Hugging Face Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) expects the class labels to be named `labels`, so we need to rename the label column from `label` to `labels`\n",
                "    * Make use of the provided functions in the [Hugging Face Datasets API](https://huggingface.co/docs/datasets/process)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "488618a4-3598-465f-b4d7-7454cacad50f",
            "metadata": {
                "id": "488618a4-3598-465f-b4d7-7454cacad50f"
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "972d3411-8b9d-44da-9d80-f5347ec247ea",
            "metadata": {
                "id": "972d3411-8b9d-44da-9d80-f5347ec247ea"
            },
            "source": [
                "* We can now load our pre-trained BERT model [bert-base-uncased](https://huggingface.co/bert-base-uncased) from the Hugging Face Hub\n",
                "    * Use the `AutoModelForSequenceClassification`, see [here](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification)\n",
                "    * Specifically, use the `from_pretrained` [method](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification.from_pretrained) and provide the number of labels, and the `id2lbl` and `lblidx` functions we created earlier\n",
                "    * This will configure a `PretrainedConfig` object, which will behave differently depending on whether we train or fine-tune. Have a look [here](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/configuration#transformers.PretrainedConfig)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a3cb65e0-a4fa-470a-a636-fdd5ea1bf405",
            "metadata": {
                "id": "a3cb65e0-a4fa-470a-a636-fdd5ea1bf405"
            },
            "outputs": [],
            "source": [
                "model = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8919626c-23fd-40e7-83c3-35d2f6ee894d",
            "metadata": {
                "id": "8919626c-23fd-40e7-83c3-35d2f6ee894d"
            },
            "source": [
                "Next, we are going to define a function that calculates our desired training objective, in our case, the F1 score. In typical Hugging Face fashion, it needs to return a dictionary\n",
                "* First, the function will be called `compute_metrics`\n",
                "    * In theory, it could be any name, but should include *all* metrics you want the return from training, each with its key-value pair in the dictionary\n",
                "    * The key should always represent the name of the metric, and the value should be the calculated metric  \n",
                "* Secondly, it will take a `eval_preds` parameter\n",
                "    * This is an intermediate result type output from the Hugging Face `Trainer` object\n",
                "    * It is a `NamedTuple` of type [EvalPred](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/configuration#transformers.PretrainedConfig), which holds our predictions and label ids\n",
                "    * You can access them using the `eval_preds.label_ids`, and `eval_preds.predictions` methods\n",
                "* Extract the predictions and labels\n",
                "* Find the predicted class index using `argmax()` with the correct dimension parameter\n",
                "* Calculate the F1 score using the previously imported sklearn function\n",
                "    * Since we have a multi-class classification problem, use the `average='weighted'` choice\n",
                "    * Other settings can also make sense, but in this case, we use the `'weighted'` option\n",
                "* Return a dictionary in the style `{'f1': f1}`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6590d17d-a3c9-4721-b4c4-8313c530d529",
            "metadata": {
                "id": "6590d17d-a3c9-4721-b4c4-8313c530d529"
            },
            "outputs": [],
            "source": [
                "def compute_metrics(eval_preds):\n",
                "    # TODO\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f340e5ff-a53c-4e0f-a4bc-958eae5f28eb",
            "metadata": {
                "id": "f340e5ff-a53c-4e0f-a4bc-958eae5f28eb"
            },
            "source": [
                "Finally, we are putting it all together in a [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) instance. It has *a lot* of options, so go and have a look at the above link to the documentation.\n",
                "\n",
                "In our case, we will set the following:\n",
                "* [output_dir](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.output_dir), the directory where you want to save model checkpoints and logging files\n",
                "    * Use something like `'./logs/run1'`, because we will later run more experiments, which should then be named `'./logs/run2/'`, so that everything is contained in one directory\n",
                "* [per_device_train_batch_size](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size), the batch size used for training\n",
                "    * since we need to store all ~110mio. parameters of BERT, the gradient for all parameters, and one tokenized batch on the GPU, this will be a rather small batch size of e.g. 4 or 8 (see below for some tricks we can use to artifically increase the effective training batch size)\n",
                "* [per_device_eval_batch_size](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.per_device_eval_batch_size), the batch size used for evaluation\n",
                "    * can be approximately 4x the training batch size\n",
                "* [gradient_accumulation_steps](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps), adds up the gradients of multiple batches in order to artificially increase the batch size while keeping GPU memory lower\n",
                "    * set it to 2, which approximates training with double our train batch size\n",
                "    * see [here](https://huggingface.co/docs/transformers/v4.18.0/en/performance) and [here](https://huggingface.co/docs/transformers/perf_train_gpu_one) for more memory and training speed tricks\n",
                "* [learning_rate](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.learning_rate), the learning rate for [AdamW](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/optimizer_schedules#transformers.AdamW) optimizier, a popular choice for pure transformer model training and fine-tuning\n",
                "    * set it to `2e-5`, transformers generally use much lower training rates than LSTMs or CNNs\n",
                "* [weight_decay](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.weight_decay), weight decay rate used for regularization\n",
                "    * set it to `1e-3`\n",
                "* [num_train_epochs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.num_train_epochs(float,), the number of epochs to train\n",
                "    * set it to 3\n",
                "* [evaluation_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.evaluation_strategy), in which intervals evaluation on the dev set should be performed\n",
                "    * set to `'epoch'`\n",
                "* [logging_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.logging_strategy), at which point during training logging should take place, e.g. at every $N$ steps, or per each epoch\n",
                "    * set it to `'steps'`, which requires us to set the `logging_steps` parameter\n",
                "* [logging_steps](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.logging_steps), after how many optimizer steps we should log the loss\n",
                "    * set it to `len(emotion_dataset['train']) / per_device_train_batch_size`\n",
                "* [save_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_strategy), when a model checkpoint should be save\n",
                "    * set it to `'epoch'`\n",
                "* [save_total_limit](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit), how many checkpoints should be saved\n",
                "    * set it to 1 to save save disk sapce\n",
                "* [seed](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.seed), the random seed for model initialization, which is important for experimental reproducibilty\n",
                "    * set it to 42 (it already is, by default, but just to be specific)\n",
                "* [data_seed](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.data_seed), the random seed for data loading, also important for experimental reproducibility\n",
                "    * set it also to 42\n",
                "* [fp16](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fp16), whether 16bit floating points should be used for training. This is another trick so save memory (roughly half of memory is used, because the default is 32bit floating points, see the two links above)\n",
                "    * set it to `True`\n",
                "* [dataloader_num_workers](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.dataloader_num_workers), how many workers to use for dataloading\n",
                "    * set it to 2\n",
                "* [load_best_model_at_end](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.load_best_model_at_end), allows us to automatically keep the best model (according to loss) in addition to the last saved, and load it after training for further prediction on the test set\n",
                "    * Set it to `True`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0f100e9-7558-44c5-94ad-f272c11b6ba0",
            "metadata": {
                "id": "f0f100e9-7558-44c5-94ad-f272c11b6ba0"
            },
            "outputs": [],
            "source": [
                "training_args = TrainingArguments(\n",
                "    # TODO\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0d7807c-a6f7-44bc-b72e-235d4f815445",
            "metadata": {
                "id": "a0d7807c-a6f7-44bc-b72e-235d4f815445"
            },
            "source": [
                "* Then, we just instantiate the `Trainer`, feed all arguments (`model`, `training_args`, `compute_metrics`, `train_dataset`, `eval_dataset`, `tokenizer`) into [it](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer), and start training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b14524e-4acc-4a0a-b82e-eea3dede7b41",
            "metadata": {
                "id": "0b14524e-4acc-4a0a-b82e-eea3dede7b41"
            },
            "outputs": [],
            "source": [
                "trainer = Trainer(\n",
                "    # TODO\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d9ee3363-5c94-4be3-ae45-883a557c0529",
            "metadata": {
                "id": "d9ee3363-5c94-4be3-ae45-883a557c0529"
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b3fe076d-0f0f-4a31-809c-eb370b6144fa",
            "metadata": {
                "id": "b3fe076d-0f0f-4a31-809c-eb370b6144fa"
            },
            "source": [
                "* As we specified in the `Trainer` class, we have already loaded the best model again (independent of whether it was last epoch's model or not).\n",
                "* We can now run one final epoch on the test set using `trainer.predict()`, which [returns](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.predict) us a `PredictionOutput` containing, among others, the test F1 score\n",
                "* Print the test F1 score and comment on the test set performance compared to the intermediate training evaluations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dabb9700-ba82-40e0-a957-699c8c9bbbf0",
            "metadata": {
                "id": "dabb9700-ba82-40e0-a957-699c8c9bbbf0"
            },
            "outputs": [],
            "source": [
                "test_predictions = # TODO\n",
                "\n",
                "print() # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6bb8be22-87d8-41ab-a7a5-130fcebb5ca1",
            "metadata": {
                "id": "6bb8be22-87d8-41ab-a7a5-130fcebb5ca1"
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bf617e14-5bf9-439b-b880-98fb430ca5f1",
            "metadata": {
                "id": "bf617e14-5bf9-439b-b880-98fb430ca5f1"
            },
            "source": [
                "However, we can go one step further in analyzing the model performances.\n",
                "\n",
                "* Start by creating a plot which shows the percentages of correct and incorrect class predictions per label\n",
                "* For example, you could extract the predicted classes and labels from the test predictions object, and plot a stacked bar chart with a sum that always equals 100%, and each part of the stack is made up of the percentages of correct and incorrect labels\n",
                "* You are also free to use other visualization techniques that show the same kind of information as long as it is clearly visible"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b8241746-06f9-4601-b045-10965cbaf53c",
            "metadata": {
                "id": "b8241746-06f9-4601-b045-10965cbaf53c"
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2511b6f7-4370-43f6-a8fd-5c60b3331881",
            "metadata": {
                "id": "2511b6f7-4370-43f6-a8fd-5c60b3331881"
            },
            "source": [
                "* What can we see from this? Briefly explain the problem."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8fa663bb-4208-477f-bc50-55f653709ce6",
            "metadata": {
                "id": "8fa663bb-4208-477f-bc50-55f653709ce6"
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "415c7f51-88d7-42be-8897-b34b7332fe75",
            "metadata": {
                "id": "415c7f51-88d7-42be-8897-b34b7332fe75"
            },
            "source": [
                "* To investigate possible reasons for this imbalance, let's look at our dataset\n",
                "* Plot the train set class distribution as normalized percentages\n",
                "    * You can make use of the Hugging Face datasets object's `to_pandas()` method\n",
                "    * Then you can use all available data plotting and data handling techniques"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ef3617d0-f3e4-4634-8325-2058c636f454",
            "metadata": {
                "id": "ef3617d0-f3e4-4634-8325-2058c636f454"
            },
            "outputs": [],
            "source": [
                "# TODO\n",
                "\n",
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "940e98ee-ee8d-4cdd-9b05-8055c257a771",
            "metadata": {
                "id": "940e98ee-ee8d-4cdd-9b05-8055c257a771"
            },
            "source": [
                "1) What problems can you imagine when seeing this class distribution?\n",
                "2) What problems could arise if we simply duplicated samples of the classes with fewer available samples?\n",
                "3) What problems could arise if we simply downsampled the majority classes to the number of minority samples?"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e70943ea-f474-469b-9d85-551b88b5da03",
            "metadata": {
                "id": "e70943ea-f474-469b-9d85-551b88b5da03"
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a1e05c3a-0540-4c22-aff7-3642f1c71f3d",
            "metadata": {
                "id": "a1e05c3a-0540-4c22-aff7-3642f1c71f3d"
            },
            "source": [
                "How to deal with this class imbalance?\n",
                "\n",
                "* We can modify the loss function to assign weight coefficients to the losses of certain classes\n",
                "* Therefore, a wrong classification of a certain class is weighted higher or lower, depending on its coefficients we provided\n",
                "* How do we calculate these weights?\n",
                "    * We can make use of the existing normalized frequency distribution we plotted above\n",
                "    * But calculate the element-wise complement per label, i.e. `1 - probability`, so that each resulting label represents a weight between 0 and 1 for its own class\n",
                "    * You can check your calculation by adding element-wise the initial normalized label distribution to your weights, you should receive an array of 6 `1`s (ignoring floating point imprecisions)\n",
                "    * Transform the type to a `torch.float32` tensor, and move the tensor separately to the `device` you want\n",
                "        * Huggingface will use a GPU as soon as it finds one available, but this separate tensor needs to be moved manually\n",
                "* However, since the Hugging Face Trainer we used before abstracted away the option to define our loss function, we need to overwrite the `compute_loss` (see [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.compute_loss)) function of the Trainer\n",
                "* To accomplish that:\n",
                "    *  we inherint from the `Trainer` class\n",
                "    *  define a `compute_loss` function that takes in a `model`, `inputs`, and a `return_outputs=False` keyword\n",
                "    *  First, we calculate the model `outputs` by giving our model the `inputs`\n",
                "        *  our `inputs` consist of a collated batch of inputs that the tokenizer creates, along with a `'labels'`, all inside a dictionary\n",
                "        *  therefore, we need to use dictionary expansion to assign all keyword arguments in the `model`\n",
                "    *  the `outputs` from Hugging Face transformers, again, are a dictionary, so we need to extract their `'logits'` and assign them to a `logits` variable\n",
                "    *  then, we extract the `labels` from the input\n",
                "    *  create a `criterion` using our well known `nn.CrossEntropyLoss()`\n",
                "        *  inside `nn.CrossEntropyLoss()`, we now use the `weight` argument and assign our calculated weight coefficients to it\n",
                "    *  compute the loss\n",
                "    *  return `(loss, outputs) if return_outputs else loss`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3b84fda9-a689-458a-a35a-dbca8dc1affd",
            "metadata": {
                "id": "3b84fda9-a689-458a-a35a-dbca8dc1affd"
            },
            "outputs": [],
            "source": [
                "complement_weights = # TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d509e6b5-e09e-4ca3-bf17-3115f2bbc3f7",
            "metadata": {
                "id": "d509e6b5-e09e-4ca3-bf17-3115f2bbc3f7"
            },
            "outputs": [],
            "source": [
                "class BalancedLossTrainer(Trainer):\n",
                "    def compute_loss(self, model, inputs, return_outputs=False):\n",
                "        # TODO\n",
                "        return (loss, outputs) if return_outputs else loss"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4acf666a-ffe8-4c09-a2cb-e505c0021788",
            "metadata": {
                "id": "4acf666a-ffe8-4c09-a2cb-e505c0021788"
            },
            "source": [
                "* Now, we can go back and re-create a second `TrainingArguments` instance\n",
                "    * Keep all settings equal, just change the `output_dir` to something like `'./logs/run2'` so that our earlier results aren't overwritten or appended to, and we keep everything in one directory\n",
                "* Re-load a new BERT model, so you start pre-training again from the base pre-trained model\n",
                "* Incorporate the new instance of `TrainingArguments` into our `BalancedLossTrainer`\n",
                "* Run all training, evaluation, and testing steps again\n",
                "* Repeat the plotting of correct and incorrect percentages per class again\n",
                "    * Comment on the new F1 score. Did it change compared to before? Why or why not?\n",
                "    * Comment also on the new results and try to explain what happened"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d4441570-c458-4868-a274-eac67e4ef650",
            "metadata": {
                "id": "d4441570-c458-4868-a274-eac67e4ef650"
            },
            "outputs": [],
            "source": [
                "model = # TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a5088fa3-fbad-4b16-abe1-e0ab9e9d1890",
            "metadata": {
                "id": "a5088fa3-fbad-4b16-abe1-e0ab9e9d1890"
            },
            "outputs": [],
            "source": [
                "training_args2 = TrainingArguments(\n",
                "    # TODO\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "55f6bbcb-2fb6-4c12-aff9-4cbac1ad0a9e",
            "metadata": {
                "id": "55f6bbcb-2fb6-4c12-aff9-4cbac1ad0a9e"
            },
            "outputs": [],
            "source": [
                "trainer = BalancedLossTrainer(\n",
                "    # TODO\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c722db60-ead1-4222-b1e7-0d3c662100ff",
            "metadata": {
                "id": "c722db60-ead1-4222-b1e7-0d3c662100ff"
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d79524c1-0b42-4bfd-ace4-0cb786d2c428",
            "metadata": {
                "id": "d79524c1-0b42-4bfd-ace4-0cb786d2c428"
            },
            "outputs": [],
            "source": [
                "test_predictions = # TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83faaec0-6cb9-4956-940e-d00d25469756",
            "metadata": {
                "id": "83faaec0-6cb9-4956-940e-d00d25469756"
            },
            "outputs": [],
            "source": [
                "# TODO\n",
                "\n",
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "61bbea8f-f026-45f9-9bce-ff1fc91e6937",
            "metadata": {
                "id": "61bbea8f-f026-45f9-9bce-ff1fc91e6937"
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37211a4f-1007-43d2-8d64-e6b5e29b513f",
            "metadata": {
                "id": "37211a4f-1007-43d2-8d64-e6b5e29b513f"
            },
            "source": [
                "# Task 2: Text Summarization with T5\n",
                "In this task, we will use the [SamSum](https://huggingface.co/datasets/samsum) dataset, which is a collection of back-and-forth chats between people, and a narrator-style summary of the chat.\n",
                "We will fine-tune our T5 model on this dataset and experiment with it on our own invented chats to see its performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e8b87c73-32c0-441a-910c-91129b5779e0",
            "metadata": {
                "id": "e8b87c73-32c0-441a-910c-91129b5779e0"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "\n",
                "import torch\n",
                "\n",
                "import evaluate\n",
                "from datasets import load_dataset\n",
                "\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    pipeline,\n",
                "    Seq2SeqTrainingArguments,\n",
                "    Seq2SeqTrainer,\n",
                "    DataCollatorForSeq2Seq,\n",
                "    AutoModelForSeq2SeqLM,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a254090d-56b9-474c-b3f1-011c8b707692",
            "metadata": {
                "id": "a254090d-56b9-474c-b3f1-011c8b707692"
            },
            "outputs": [],
            "source": [
                "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
                "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # there might be interferences with the parallelism of the Hugging Face Trainer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e57cc35-bc28-4881-aec1-1e3e02df9b19",
            "metadata": {
                "id": "0e57cc35-bc28-4881-aec1-1e3e02df9b19"
            },
            "source": [
                "* Download the dataset from the Hugging Face Hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "575512e5-9b1a-4250-9c61-c6dbc24b5848",
            "metadata": {
                "id": "575512e5-9b1a-4250-9c61-c6dbc24b5848"
            },
            "outputs": [],
            "source": [
                "samsum = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "26d9f4e7-80ff-42ac-9526-884e69f9875c",
            "metadata": {
                "id": "26d9f4e7-80ff-42ac-9526-884e69f9875c"
            },
            "source": [
                "* Use Hugging Face's `pipeline` interface to see the non-finetuned performance of our model.\n",
                "* Load the [T5-small](https://huggingface.co/t5-small) model inside the [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines) object, along with the `task=summarization` attribute.\n",
                "* Pick a sample from the [SamSum Test Set](https://huggingface.co/datasets/samsum/viewer/samsum/test). You can directly copy it from the website if you like.\n",
                "* Input it into the created pipeline object and check the result.\n",
                "    * In case of memory problems later on during fine-tuning, `del` the pipeline object to free up memory afterwards.\n",
                "* Comment: How well did it work?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1db252a-ad67-4397-a693-554493564d6f",
            "metadata": {
                "id": "b1db252a-ad67-4397-a693-554493564d6f"
            },
            "outputs": [],
            "source": [
                "pipe = # TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c4eba82f-66b4-4d28-900b-348a63a944d1",
            "metadata": {
                "id": "c4eba82f-66b4-4d28-900b-348a63a944d1"
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d6b258c2-9ecc-4c24-9bc6-8479fffad285",
            "metadata": {
                "id": "d6b258c2-9ecc-4c24-9bc6-8479fffad285"
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1608e2f0-dde3-4f7b-95f4-cdbaad81f1be",
            "metadata": {
                "id": "1608e2f0-dde3-4f7b-95f4-cdbaad81f1be"
            },
            "source": [
                "* Then, we will load [T5's](https://huggingface.co/t5-small) `t5-small` tokenizer, again using `AutoTokenizer` from before"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "701bfc05-bc7d-405e-afec-6b09b9e42c86",
            "metadata": {
                "id": "701bfc05-bc7d-405e-afec-6b09b9e42c86"
            },
            "outputs": [],
            "source": [
                "tokenizer = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4c5ab0c8-4388-4163-95f1-102a8fef3085",
            "metadata": {
                "id": "4c5ab0c8-4388-4163-95f1-102a8fef3085"
            },
            "source": [
                "T5 is a sequence-to-sequence encoder-decoder model, meaning that it takes sequences as input, transforms them, and outputs target sequences. The model can be used for translation, classification, summarization, and much more, given the correct input prompts.\n",
                "\n",
                "As we are dealing with a summarization task, we need to preface our sequences with the necessary `'summarize: '` prompt.\n",
                "* Write a preprocess function which takes in a dataset split and prefaces every input sequence with the string `'summarize: '`\n",
                "* Afterwards, use the tokenizer with the arguments `truncation=True` and `max_length=512` to limit the maximum lengths of all inputs, and tokenize all sequences\n",
                "* Then, separately transform the target sequences using `truncation=True` with a `max_length=128`\n",
                "* Extract the `input_ids` from the transformed target sequences and add them to the the dictionary output of the transformed input sequences with the key `labels`\n",
                "* Return the dictionary object afterwards"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1d7d0c26-fe49-4224-93a6-40d484c500fc",
            "metadata": {
                "id": "1d7d0c26-fe49-4224-93a6-40d484c500fc"
            },
            "outputs": [],
            "source": [
                "def preprocess_summarization(split):\n",
                "    # TODO\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0727e745-bfab-4612-aa30-3b78e6defa2e",
            "metadata": {
                "id": "0727e745-bfab-4612-aa30-3b78e6defa2e"
            },
            "source": [
                "* As before with the classification task, `map` the preprocessing function onto the entire dataset using the `batched=True` option"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "310ee61b-5b80-43a0-8a5d-d76c69dddff2",
            "metadata": {
                "id": "310ee61b-5b80-43a0-8a5d-d76c69dddff2"
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa2b20fa-e22d-42d1-b40f-4414b9f2a819",
            "metadata": {
                "id": "aa2b20fa-e22d-42d1-b40f-4414b9f2a819"
            },
            "source": [
                "* As the next step, we create a `collator` [object](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq), which takes in the `tokenizer` and our `model`\n",
                "    * At this stage, it is fine to just write `model='t5-small'` as input to the collator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "611afa55-9d71-4a5f-8017-3abce46ccd51",
            "metadata": {
                "id": "611afa55-9d71-4a5f-8017-3abce46ccd51"
            },
            "outputs": [],
            "source": [
                "collator = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2119c26e-7549-4995-a05d-1374c1e7b85c",
            "metadata": {
                "id": "2119c26e-7549-4995-a05d-1374c1e7b85c"
            },
            "source": [
                "* To evaluate our model's summarization performance, we employ the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) score.\n",
                "* Advantages include:\n",
                "    * The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric commonly used for evaluating the quality of machine-generated text, particularly summaries\n",
                "    * ROUGE measures the similarity between the generated summary and one or more reference (human-written) summaries\n",
                "    * ROUGE provides a standardized way to evaluate the quality of text summaries, which is important for at-scale comparison between different summarization models\n",
                "    * ROUGE includes multiple metrics, such as ROUGE-N (for n-grams), ROUGE-L (for longest common subsequence), and ROUGE-W (for weighted n-grams). Depending on the task, these metrics capture different aspects of summary quality, allowing a more comprehensive evaluation.\n",
                "* Disadvantages include:\n",
                "    * ROUGE relies on exact word or n-gram matches, which may not capture the semantic meaning well. Two sentences with similar meaning but slightly different phrasing might receive a low ROUGE score. (You can use the above link to play around with some references and generated sequences)\n",
                "    * ROUGE does not consider the order of words or phrases. If a model generates a summary with words or phrases in a different order than the reference, even if the content is correct, the ROUGE score might be lower.\n",
                "    * ROUGE measures lexical overlap and does not consider deeper linguistic structures or coherence in summaries.\n",
                "* Load the `rouge` score from the `evaluate` library"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cd578c4e-47c9-4d6d-bd4d-91a509cf8129",
            "metadata": {
                "id": "cd578c4e-47c9-4d6d-bd4d-91a509cf8129"
            },
            "outputs": [],
            "source": [
                "rouge = # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ba798853-94c9-4013-9fbb-514e1d166dd7",
            "metadata": {
                "id": "ba798853-94c9-4013-9fbb-514e1d166dd7"
            },
            "source": [
                "* Then, we need to define our `compute_metrics` function, but this time for the ROUGE score\n",
                "* At first, we receive again our tuple of `EvalPred` predictions and labels, which is also the input to our function\n",
                "    * Split up the input tuple by extracting the labels and predictions\n",
                "    * In the T5 case, the predictions are again a tuple of `(logits, hidden_representations)`\n",
                "    * For our task, extract only the logits\n",
                "    * Apply `argmax()` on the last dimension of the logits to get the predictions\n",
                "* Then, we use our `tokenizer` to `batch_decode` the argmaxed logits\n",
                "    * Here, we activate the tokenizer setting `skip_special_tokens=True`, and `clean_up_tokenization_spaces=True`\n",
                "* Before we can decode our labels, we must first replace the padding tokens of the labels batch, represented by the value `-100`, with the actual padding tokens of the tokenizer\n",
                "    * The `-100` are a convention of the `Seq2SeqTrainer`, and don't have any deeper meaning\n",
                "    * the padding token can be accessed by `tokenizer.pad_token_id`\n",
                "* Then, we also `batch_decode` the labels\n",
                "    * Also skip the special tokens, but no need clean up tokenization spaces since we handled them separately\n",
                "* Afterwards, we input both decoded predictions and labels into the `rouge.compute` function\n",
                "* That result dictionary is returned\n",
                "    * Optionally, round all numbers in the result dictionary to 4 or 6 digits, whichever format you prefer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df12172f-7fc2-437d-b167-b843f694d3b1",
            "metadata": {
                "id": "df12172f-7fc2-437d-b167-b843f694d3b1"
            },
            "outputs": [],
            "source": [
                "def compute_metrics(eval_preds):\n",
                "    # TODO\n",
                "\n",
                "    return # TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "41dff509-090d-4f01-a5dc-b02ecacc7069",
            "metadata": {
                "id": "41dff509-090d-4f01-a5dc-b02ecacc7069"
            },
            "source": [
                "* Then we create a `Seq2SeqTrainingArguments` [instance](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments), which is just a specialized version of our general `TrainingArguments` function from before.\n",
                "* In our case, we can base our settings on the training arguments from BERT classification, except:\n",
                "    * Since we now have longer sequences, it might be necessary to reduce batch sizes even more, so experiment a bit with your GPU memory availability\n",
                "        * Try to max-out your GPU memory by increasing the batch size as much as possible to speed up training\n",
                "    * change the `output_dir` to something under the same path from before, e.g. `'./logs/run3/'`\n",
                "* Once created, load the model using the `AutoModelForSeq2SeqLM`\n",
                "* Instantiate a `Seq2SeqTrainer` [object](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.Seq2SeqTrainer) in the same style as before\n",
                "    * This time, we just need to add the `collator` object to the `data_collator` argument\n",
                "* Call the `train()` method and start training\n",
                "* Comment briefly on the training progress"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "384c2efe-6aa7-4ad6-9705-bf4dffef05cf",
            "metadata": {
                "id": "384c2efe-6aa7-4ad6-9705-bf4dffef05cf"
            },
            "outputs": [],
            "source": [
                "training_args3 = Seq2SeqTrainingArguments(\n",
                "    # TODO\n",
                ")\n",
                "\n",
                "model = # TODO\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    # TODO\n",
                ")\n",
                "\n",
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "248d3881-7ffc-493f-ba2e-0594560e3e98",
            "metadata": {
                "id": "248d3881-7ffc-493f-ba2e-0594560e3e98"
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ad1afc9-c18a-4eef-9026-b569dff590ec",
            "metadata": {
                "id": "3ad1afc9-c18a-4eef-9026-b569dff590ec"
            },
            "source": [
                "* As the last step of this exercise, move the `model`, which represents the already loaded best model due to our `Seq2SeqTrainingArguments` settings, to the CPU and input it into a new `pipeline` object as the `model=` keyword.\n",
                "* Additionally, prove the `tokenizer=` argument\n",
                "* Then, fill in your chosen test set sample from before\n",
                "* Compare the results to your initial, pre fine-tuning output and discuss what got better or worse\n",
                "* Comment on whether the narrator-style summary performance increased"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a74f6176-f248-4582-83bb-16c440774ce1",
            "metadata": {
                "id": "a74f6176-f248-4582-83bb-16c440774ce1"
            },
            "outputs": [],
            "source": [
                "pipe = # TODO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f048bdb0-42f9-4586-8a30-e49bd48a8f22",
            "metadata": {
                "id": "f048bdb0-42f9-4586-8a30-e49bd48a8f22"
            },
            "outputs": [],
            "source": [
                "# TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a5c8ac38-1b16-4a42-aa24-e85c2a14a536",
            "metadata": {
                "id": "a5c8ac38-1b16-4a42-aa24-e85c2a14a536"
            },
            "source": [
                "___\n",
                "Student answers here:\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e576fcf-04ba-497d-9bc7-734bcff18979",
            "metadata": {
                "id": "0e576fcf-04ba-497d-9bc7-734bcff18979"
            },
            "source": [
                "# Inspect and share all your experiment results via TensorBoard (bonus)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "acb91278-d706-4daa-9521-8baa50996218",
            "metadata": {
                "id": "acb91278-d706-4daa-9521-8baa50996218"
            },
            "source": [
                "* While we now only had 3 runtime variations, in practice we often have tens or hundreds of hyperparameter and/or model combinatinations\n",
                "* To assemble manual train/val/test plots, scale all x- and y-axes for various units, and plot all desired combinations side by side is very cumbersome and time consuming\n",
                "* Services like [tensorboard-dev](https://tensorboard.dev/) and [tensorboard in general](https://www.tensorflow.org/tensorboard) offer the possibility to simply read in our created `'logs'` directory, which enables us to browse through an online interface of automatically generated plots for all our tracked metrics\n",
                "* Install (if you don't already have it) `tensorboard` via `pip install -U tensorboard)` and either:\n",
                "    * Upload your `'logs'` directly to Google (requires a Google account and consent to store the specific `'logs'` directory on Google's servers)\n",
                "        * execute on your command line `tensorboard dev upload --logdir logs --name \"whatever-name-you-want-to-call-it\"`\n",
                "        * then you will need to login to your Google account, consent to uploading the `'logs'` directory, and a shareable link will appear as command line output\n",
                "        * the link will lead you to the hosted experiment plots\n",
                "        * FYI: it is also possible to delete uploaded tensorboards via `tensorboard dev delete --experiment_id EXPERIMENT_ID`\n",
                "    * If you don't want to upload your experiments to Google or don't have a Google account, you can achieve the same browsable layout on your local machine\n",
                "    * execute on your command line `tensorboard --logdir logs`\n",
                "    * a localhost instance, usually on `https://localhost:6006/`, will open\n",
                "* Choose one of the above options to visualize your experiments in tensorboard\n",
                "* Include a screenshot of the results in the notebook submission of this exercise\n",
                "    * You can embed images in Jupyerlab in markdown cells [as described here](https://stackoverflow.com/questions/41604263/how-do-i-display-local-image-in-markdown)\n",
                "    * Also send us the screenshot file with it, otherwise we can't see your embedded image in the notebook\n",
                "* Last note: You aren't dependend on Hugging Face experiments to upload and visualize results to tensorboard\n",
                "    * You can upload every folder that is generated in the same format to tensorboard\n",
                "    * For instance, the same can be achived using [PyTorch](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) and the SummaryWriter\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0e8f75ed-1263-474b-9a99-1eacc6cfc733",
            "metadata": {
                "id": "0e8f75ed-1263-474b-9a99-1eacc6cfc733"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}