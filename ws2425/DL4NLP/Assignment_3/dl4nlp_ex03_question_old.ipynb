{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f506b01-f3ba-4744-9953-6a8d30f24717",
   "metadata": {
    "id": "9f506b01-f3ba-4744-9953-6a8d30f24717"
   },
   "source": [
    "# Deep Learning for NLP - Exercise 03\n",
    "In this exercise, we will be using the Hugging Face ecosystem to finetune BERT for a classification task (part 1) and T5 for text summarization (part 2). Then, we will upload our training and validation results via TensorBoard, which makes interactive sharing and inspecting of results very easy.\n",
    "\n",
    "Part 1 and part 2 can be worked on independently.\n",
    "\n",
    "___\n",
    "General hints:\n",
    "* Have a look at the imports below when solving the tasks\n",
    "* Use the given modules and all submodules of the imports, but don't import anything else!\n",
    "    * For instance, you can use other functions under the `torch` or `nn` namespace, but don't import e.g. PyTorch Lightning, etc.\n",
    "* It is recommended to install all packages from the provided environment file\n",
    "* Feel free to test your code between sub-tasks of the exercise sheet, so that you can spot mistakes early (wrong shapes, impossible numbers, NaNs, ...)\n",
    "* Just keep in mind that your final submission should be compliant to the provided initial format of this file\n",
    "\n",
    "Submission guidelines:\n",
    "* Make sure that the code runs on package versions from the the provided environment file\n",
    "* Do not add or change any imports (also don't change the naming of imports, e.g. `torch.nn.functional as f`)\n",
    "* Remove your personal, additional code testings and experiments throughout the notebook\n",
    "* Do not change the class, function or naming structure as we will run tests on the given names\n",
    "* Additionally export this notebook as a `.py` file, and submit **both** the executed `.ipynb` notebook with plots in it **and** the `.py` file\n",
    "* **Deviation from the above guidelines will result in partial or full loss of points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa3cf98-b17b-4c72-b876-c1f1d67b1486",
   "metadata": {
    "id": "eaa3cf98-b17b-4c72-b876-c1f1d67b1486"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.24.0\n",
    "# !pip install datasets==3.0.1\n",
    "# !pip install evaluate==0.4.0\n",
    "# !pip install rouge-score==0.1.2\n",
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a040d626-232c-4f9a-be5e-aaad77ecfbb0",
   "metadata": {
    "id": "a040d626-232c-4f9a-be5e-aaad77ecfbb0",
    "tags": []
   },
   "source": [
    "# Task 1: Sequence Classification with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbdc10-854d-4c00-97b1-cedcd0bdde77",
   "metadata": {
    "id": "12fbdc10-854d-4c00-97b1-cedcd0bdde77"
   },
   "source": [
    "* In this task, we will finetune [BERT](https://huggingface.co/bert-base-uncased) on a custom sentiment classification dataset and perform multi-class sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5529e86-d1c1-4414-b0d3-56d6931f21e5",
   "metadata": {
    "id": "d5529e86-d1c1-4414-b0d3-56d6931f21e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67aefa49-05b1-4b82-ba9d-ad0ebf4742f5",
   "metadata": {
    "id": "67aefa49-05b1-4b82-ba9d-ad0ebf4742f5"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # there might be interferences with the parallelism of the Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb73ae-5cba-4c54-a65d-7af604c22fd9",
   "metadata": {
    "id": "1cdb73ae-5cba-4c54-a65d-7af604c22fd9"
   },
   "source": [
    "We start by downloading the [DAIR-AI Emotion Dataset](https://huggingface.co/datasets/dair-ai/emotion) from the Hugging Face Hub, which already comes with a train, validation, and test split of 16,000 - 2,000 - 2,000 samples. The dataset consists of tweets with their labeled emotions over 6 different classes.\n",
    "\n",
    "* As the dataset, by default, consists of integer labels, we first create two helper dictionaries\n",
    "    * `idx2lbl`, which maps the integer index to the label string\n",
    "    * `lbl2idx`, which maps the label strings to the integer index\n",
    "    * The downloaded Hugging Face dataset object implements a useful `.features` method for each dataset split, which contains the label strings in the correct order corresponding to the integers (the labels are consistent across each split, so it is enough to only inspect one split)\n",
    "* Try to get an overview of the data and the dataset format by printing 10 random (use the `random` library to sample random numbers!) samples of the train split with their corresponding integer and string labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90261bf4-0b3f-44ae-af4c-2d2ed2d58d36",
   "metadata": {
    "id": "90261bf4-0b3f-44ae-af4c-2d2ed2d58d36"
   },
   "outputs": [],
   "source": [
    "emotion_dataset = load_dataset('dair-ai/emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb96f49b-f101-47cb-8d69-febe03928969",
   "metadata": {
    "id": "cb96f49b-f101-47cb-8d69-febe03928969"
   },
   "outputs": [],
   "source": [
    "idx2lbl = {i: key for i, key in enumerate(emotion_dataset['train'].features['label'].names)}\n",
    "lbl2idx = {key: i for i, key in enumerate(emotion_dataset['train'].features['label'].names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a160b43-c6ab-47fb-a727-907ec2a4c04a",
   "metadata": {
    "id": "4a160b43-c6ab-47fb-a727-907ec2a4c04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 10205 // text: i did not feel faster or stronger in that way but i did feel more energetic // label: 1 // string label: joy\n",
      "Sample 6637 // text: i feel after venting to a notebook is amazing // label: 1 // string label: joy\n",
      "Sample 15795 // text: i feel stressed tired worn out out of shape or neglected // label: 0 // string label: sadness\n",
      "Sample 10416 // text: i get to feel virtuous in comparison to him but i don t really have to put out // label: 1 // string label: joy\n",
      "Sample 309 // text: i feel to have these amazing people in my life // label: 5 // string label: surprise\n",
      "Sample 1926 // text: i remember feeling as if i didn t belong and that i wasn t smart enough cool enough or even young enough // label: 1 // string label: joy\n",
      "Sample 8046 // text: i walked away from that encounter feeling blessed too // label: 1 // string label: joy\n",
      "Sample 11566 // text: i feel terrible for him and want to cheer him up // label: 0 // string label: sadness\n",
      "Sample 10761 // text: id really hop to it quickly because i knew theyd cry and yell if they didnt get it quickly and i also knew scott was feeling rotten // label: 0 // string label: sadness\n",
      "Sample 369 // text: i should feel complimented or insulted // label: 3 // string label: anger\n"
     ]
    }
   ],
   "source": [
    "rand_idxs = np.random.choice(len(emotion_dataset['train']), 10)\n",
    "\n",
    "for idx in rand_idxs:\n",
    "    txt = emotion_dataset['train']['text'][idx]\n",
    "    lbl_int = emotion_dataset['train']['label'][idx]\n",
    "    lbl_str = idx2lbl[lbl_int]\n",
    "    print(f'Sample {idx} // text: {txt} // label: {lbl_int} // string label: {lbl_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd221019-21e4-4dcc-9cdb-fb37d10140ab",
   "metadata": {
    "id": "cd221019-21e4-4dcc-9cdb-fb37d10140ab"
   },
   "source": [
    "To start the preprocessing steps, we first need to load the pre-trained tokenizer for our [bert-base-uncased](https://huggingface.co/bert-base-uncased) model, which can be achieved very comfortable using the `AutoTokenizer` [class API](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d58ef2-7f79-4985-8dc1-2f8743c96e80",
   "metadata": {
    "id": "46d58ef2-7f79-4985-8dc1-2f8743c96e80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09145481ccab41f28fcb91ed11f020eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\esual\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad70352fc244dc0a869799bf873caeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870f3a5d88ff4d0eb75e307420ed955b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f393010d5a714648a96f60bbb8b69ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', force_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074cb17-ec06-4ea5-8050-37686c6e0ee4",
   "metadata": {
    "id": "3074cb17-ec06-4ea5-8050-37686c6e0ee4"
   },
   "source": [
    "If we enter an example input from the train set into the tokenizer, we can see that we receive again dictionaries as output, which is the preferred data format from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f946f992-2676-49dc-b6a2-c0ecc9e45502",
   "metadata": {
    "id": "f946f992-2676-49dc-b6a2-c0ecc9e45502"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_seq = emotion_dataset['train']['text'][0]\n",
    "tokenizer(example_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159455f-03c7-4fe0-ae53-36ab44aafa65",
   "metadata": {
    "id": "3159455f-03c7-4fe0-ae53-36ab44aafa65"
   },
   "source": [
    "* We can see that the dictionary contains `input_ids`, which represent the tokenized sequence *including* specials tokens\n",
    "    * For instance, notice that the sequence begins with `101`, which is used by BERT to mark the `[CLS]` token\n",
    "    * The sequence ends with the `102` token, which BERT uses as a separation mark between sequences, `[SEP]`\n",
    "    * You can check all special tokens using `tokenizer.all_special_tokens` and `tokenizer.all_special_ids`\n",
    "* Secondly, the tokenizer contains `token_type_ids`, which are used to distinguish different segments of input tokens in models that support token-level type embeddings, like BERT and its variants\n",
    "    * These embeddings are useful for tasks like sentence pair classification or question answering, where the model needs to understand which tokens belong to which part of the input. (not needed here)\n",
    "* Thirdly, the tokenizer automatically creates the attention mask, which consists of all `1` in this case\n",
    "    * The attention mask in input sequences is usually only `0` when padding symbols are added, as we don't want to attend to padding symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af8005-8c7c-4dce-b34a-074576892e81",
   "metadata": {
    "id": "02af8005-8c7c-4dce-b34a-074576892e81"
   },
   "source": [
    "Following this, we can use the tokenizer to tokenize our entire dataset.\n",
    "\n",
    "* Hugging Face dataset objects provide a way to `map` a function onto every single object using powerful parallel processing operations\n",
    "* There, we need to create a function which acts like it:\n",
    "    * takes in text examples of a data split of our dataset object\n",
    "    * inputs it into the tokenizer\n",
    "        * use the tokenizer with options `truncation=True` and `max_length=512`, since the maximum context size of BERT is 512 tokens\n",
    "        * This shouldn't impact our dataset in any way since the Tweet dataset stems from the times when tweets were limited to 280 characters, but it's better to be safe than sorry and have your training interrupted\n",
    "    * return the output of the tokenizer\n",
    "* Apply the function onto the entire dataset object using its `map` method with the option `batched=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "827f4441-849c-4e8c-b4cf-07721fd1e823",
   "metadata": {
    "id": "827f4441-849c-4e8c-b4cf-07721fd1e823"
   },
   "outputs": [],
   "source": [
    "def tokenize_seqs(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "\n",
    "emotion_dataset = emotion_dataset.map(tokenize_seqs, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e1605-da0d-4dff-868b-720ae98f06a1",
   "metadata": {
    "id": "0e4e1605-da0d-4dff-868b-720ae98f06a1"
   },
   "source": [
    "* Lastly, the [Hugging Face Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) expects the class labels to be named `labels`, so we need to rename the label column from `label` to `labels`\n",
    "    * Make use of the provided functions in the [Hugging Face Datasets API](https://huggingface.co/docs/datasets/process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488618a4-3598-465f-b4d7-7454cacad50f",
   "metadata": {
    "id": "488618a4-3598-465f-b4d7-7454cacad50f"
   },
   "outputs": [],
   "source": [
    "emotion_dataset = emotion_dataset.rename_column('label', 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d3411-8b9d-44da-9d80-f5347ec247ea",
   "metadata": {
    "id": "972d3411-8b9d-44da-9d80-f5347ec247ea"
   },
   "source": [
    "* We can now load our pre-trained BERT model [bert-base-uncased](https://huggingface.co/bert-base-uncased) from the Hugging Face Hub\n",
    "    * Use the `AutoModelForSequenceClassification`, see [here](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification)\n",
    "    * Specifically, use the `from_pretrained` [method](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification.from_pretrained) and provide the number of labels, and the `id2lbl` and `lblidx` functions we created earlier\n",
    "    * This will configure a `PretrainedConfig` object, which will behave differently depending on whether we train or fine-tune. Have a look [here](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/configuration#transformers.PretrainedConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05ef4326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3cb65e0-a4fa-470a-a636-fdd5ea1bf405",
   "metadata": {
    "id": "a3cb65e0-a4fa-470a-a636-fdd5ea1bf405"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61df4b884fb441af9d326ba6bf5df8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\esual\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442ac090f72244ad90ff2eeeab437160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'google-bert/bert-base-uncased',\n",
    "    num_labels = 6,\n",
    "    id2label = idx2lbl,\n",
    "    label2id = lbl2idx,\n",
    "    force_download = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919626c-23fd-40e7-83c3-35d2f6ee894d",
   "metadata": {
    "id": "8919626c-23fd-40e7-83c3-35d2f6ee894d"
   },
   "source": [
    "Next, we are going to define a function that calculates our desired training objective, in our case, the F1 score. In typical Hugging Face fashion, it needs to return a dictionary\n",
    "* First, the function will be called `compute_metrics`\n",
    "    * In theory, it could be any name, but should include *all* metrics you want the return from training, each with its key-value pair in the dictionary\n",
    "    * The key should always represent the name of the metric, and the value should be the calculated metric  \n",
    "* Secondly, it will take a `eval_preds` parameter\n",
    "    * This is an intermediate result type output from the Hugging Face `Trainer` object\n",
    "    * It is a `NamedTuple` of type [EvalPred](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/configuration#transformers.PretrainedConfig), which holds our predictions and label ids\n",
    "    * You can access them using the `eval_preds.label_ids`, and `eval_preds.predictions` methods\n",
    "* Extract the predictions and labels\n",
    "* Find the predicted class index using `argmax()` with the correct dimension parameter\n",
    "* Calculate the F1 score using the previously imported sklearn function\n",
    "    * Since we have a multi-class classification problem, use the `average='weighted'` choice\n",
    "    * Other settings can also make sense, but in this case, we use the `'weighted'` option\n",
    "* Return a dictionary in the style `{'f1': f1}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6590d17d-a3c9-4721-b4c4-8313c530d529",
   "metadata": {
    "id": "6590d17d-a3c9-4721-b4c4-8313c530d529"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    label_ids, predictions = eval_preds.label_ids, eval_preds.predictions\n",
    "    pred_class = np.argmax(predictions, axis=1)\n",
    "    f1 = f1_score(label_ids, pred_class, average='weighted')\n",
    "    return {'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340e5ff-a53c-4e0f-a4bc-958eae5f28eb",
   "metadata": {
    "id": "f340e5ff-a53c-4e0f-a4bc-958eae5f28eb"
   },
   "source": [
    "Finally, we are putting it all together in a [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) instance. It has *a lot* of options, so go and have a look at the above link to the documentation.\n",
    "\n",
    "In our case, we will set the following:\n",
    "* [output_dir](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.output_dir), the directory where you want to save model checkpoints and logging files\n",
    "    * Use something like `'./logs/run1'`, because we will later run more experiments, which should then be named `'./logs/run2/'`, so that everything is contained in one directory\n",
    "* [per_device_train_batch_size](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size), the batch size used for training\n",
    "    * since we need to store all ~110mio. parameters of BERT, the gradient for all parameters, and one tokenized batch on the GPU, this will be a rather small batch size of e.g. 4 or 8 (see below for some tricks we can use to artifically increase the effective training batch size)\n",
    "* [per_device_eval_batch_size](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.per_device_eval_batch_size), the batch size used for evaluation\n",
    "    * can be approximately 4x the training batch size\n",
    "* [gradient_accumulation_steps](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps), adds up the gradients of multiple batches in order to artificially increase the batch size while keeping GPU memory lower\n",
    "    * set it to 2, which approximates training with double our train batch size\n",
    "    * see [here](https://huggingface.co/docs/transformers/v4.18.0/en/performance) and [here](https://huggingface.co/docs/transformers/perf_train_gpu_one) for more memory and training speed tricks\n",
    "* [learning_rate](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.learning_rate), the learning rate for [AdamW](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/optimizer_schedules#transformers.AdamW) optimizier, a popular choice for pure transformer model training and fine-tuning\n",
    "    * set it to `2e-5`, transformers generally use much lower training rates than LSTMs or CNNs\n",
    "* [weight_decay](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.weight_decay), weight decay rate used for regularization\n",
    "    * set it to `1e-3`\n",
    "* [num_train_epochs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.num_train_epochs(float,), the number of epochs to train\n",
    "    * set it to 3\n",
    "* [evaluation_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.evaluation_strategy), in which intervals evaluation on the dev set should be performed\n",
    "    * set to `'epoch'`\n",
    "* [logging_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.logging_strategy), at which point during training logging should take place, e.g. at every $N$ steps, or per each epoch\n",
    "    * set it to `'steps'`, which requires us to set the `logging_steps` parameter\n",
    "* [logging_steps](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.logging_steps), after how many optimizer steps we should log the loss\n",
    "    * set it to `len(emotion_dataset['train']) / per_device_train_batch_size`\n",
    "* [save_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_strategy), when a model checkpoint should be save\n",
    "    * set it to `'epoch'`\n",
    "* [save_total_limit](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit), how many checkpoints should be saved\n",
    "    * set it to 1 to save save disk sapce\n",
    "* [seed](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.seed), the random seed for model initialization, which is important for experimental reproducibilty\n",
    "    * set it to 42 (it already is, by default, but just to be specific)\n",
    "* [data_seed](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.data_seed), the random seed for data loading, also important for experimental reproducibility\n",
    "    * set it also to 42\n",
    "* [fp16](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fp16), whether 16bit floating points should be used for training. This is another trick so save memory (roughly half of memory is used, because the default is 32bit floating points, see the two links above)\n",
    "    * set it to `True`\n",
    "* [dataloader_num_workers](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.dataloader_num_workers), how many workers to use for dataloading\n",
    "    * set it to 2\n",
    "* [load_best_model_at_end](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.load_best_model_at_end), allows us to automatically keep the best model (according to loss) in addition to the last saved, and load it after training for further prediction on the test set\n",
    "    * Set it to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed0c8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./logs/run1',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-3,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=len(emotion_dataset['train']) // 8,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7807c-a6f7-44bc-b72e-235d4f815445",
   "metadata": {
    "id": "a0d7807c-a6f7-44bc-b72e-235d4f815445"
   },
   "source": [
    "* Then, we just instantiate the `Trainer`, feed all arguments (`model`, `training_args`, `compute_metrics`, `train_dataset`, `eval_dataset`, `tokenizer`) into [it](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer), and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b14524e-4acc-4a0a-b82e-eea3dede7b41",
   "metadata": {
    "id": "0b14524e-4acc-4a0a-b82e-eea3dede7b41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    compute_metrics = compute_metrics,\n",
    "    train_dataset = emotion_dataset['train'],\n",
    "    eval_dataset = emotion_dataset['validation'],\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9ee3363-5c94-4be3-ae45-883a557c0529",
   "metadata": {
    "id": "d9ee3363-5c94-4be3-ae45-883a557c0529"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3000\n",
      "  Number of trainable parameters = 109486854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438ac6f11b1f4f0097e8b7c780bb9496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c30cf32691f428ea8c60b8ac01a8369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/run1\\checkpoint-1000\n",
      "Configuration saved in ./logs/run1\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2319028675556183, 'eval_f1': 0.9260381465934119, 'eval_runtime': 12.896, 'eval_samples_per_second': 155.087, 'eval_steps_per_second': 4.885, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./logs/run1\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run1\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run1\\checkpoint-1000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3233, 'learning_rate': 6.6866666666666665e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f3e7ed3e6649608b771af1beadd6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/run1\\checkpoint-2000\n",
      "Configuration saved in ./logs/run1\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1698744148015976, 'eval_f1': 0.9311178499358966, 'eval_runtime': 12.1575, 'eval_samples_per_second': 164.508, 'eval_steps_per_second': 5.182, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./logs/run1\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run1\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run1\\checkpoint-2000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dc72a4c3cd4af0ad1b62c6c0b4bc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/run1\\checkpoint-3000\n",
      "Configuration saved in ./logs/run1\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18489809334278107, 'eval_f1': 0.9364241769239968, 'eval_runtime': 11.961, 'eval_samples_per_second': 167.21, 'eval_steps_per_second': 5.267, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./logs/run1\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run1\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run1\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [logs\\run1\\checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./logs/run1\\checkpoint-2000 (score: 0.1698744148015976).\n",
      "Deleting older checkpoint [logs\\run1\\checkpoint-2000] due to args.save_total_limit\n",
      "Deleting older checkpoint [logs\\run1\\checkpoint-3000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 549.7397, 'train_samples_per_second': 87.314, 'train_steps_per_second': 5.457, 'train_loss': 0.24728325907389323, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.24728325907389323, metrics={'train_runtime': 549.7397, 'train_samples_per_second': 87.314, 'train_steps_per_second': 5.457, 'train_loss': 0.24728325907389323, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe076d-0f0f-4a31-809c-eb370b6144fa",
   "metadata": {
    "id": "b3fe076d-0f0f-4a31-809c-eb370b6144fa"
   },
   "source": [
    "* As we specified in the `Trainer` class, we have already loaded the best model again (independent of whether it was last epoch's model or not).\n",
    "* We can now run one final epoch on the test set using `trainer.predict()`, which [returns](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.predict) us a `PredictionOutput` containing, among others, the test F1 score\n",
    "* Print the test F1 score and comment on the test set performance compared to the intermediate training evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dabb9700-ba82-40e0-a957-699c8c9bbbf0",
   "metadata": {
    "id": "dabb9700-ba82-40e0-a957-699c8c9bbbf0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb036d5bc7c64f28bd791285efe37a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.17748534679412842, 'test_f1': 0.9301425293987967, 'test_runtime': 13.1703, 'test_samples_per_second': 151.857, 'test_steps_per_second': 4.783}\n"
     ]
    }
   ],
   "source": [
    "test_predictions = trainer.predict(\n",
    "    test_dataset = emotion_dataset['test']\n",
    ")\n",
    "\n",
    "print(test_predictions.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8be22-87d8-41ab-a7a5-130fcebb5ca1",
   "metadata": {
    "id": "6bb8be22-87d8-41ab-a7a5-130fcebb5ca1"
   },
   "source": [
    "___\n",
    "Student answers here: For the test data, both loss and F1 Score are close to what we've achieved with evaluation set during training (intermediate steps & final output). Having a similar F1 score for unseen data is a sign of good generalization (no overfitting or underfitting observed).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf617e14-5bf9-439b-b880-98fb430ca5f1",
   "metadata": {
    "id": "bf617e14-5bf9-439b-b880-98fb430ca5f1"
   },
   "source": [
    "However, we can go one step further in analyzing the model performances.\n",
    "\n",
    "* Start by creating a plot which shows the percentages of correct and incorrect class predictions per label\n",
    "* For example, you could extract the predicted classes and labels from the test predictions object, and plot a stacked bar chart with a sum that always equals 100%, and each part of the stack is made up of the percentages of correct and incorrect labels\n",
    "* You are also free to use other visualization techniques that show the same kind of information as long as it is clearly visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfa64910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.582 , -1.104 , -2.105 , -0.796 , -1.897 , -1.0625],\n",
       "       [ 6.902 , -2.033 , -1.882 , -0.839 , -1.467 , -1.09  ],\n",
       "       [ 6.87  , -1.985 , -1.828 , -0.845 , -1.589 , -1.029 ],\n",
       "       ...,\n",
       "       [-1.368 ,  6.895 , -0.4004, -2.006 , -2.389 , -1.261 ],\n",
       "       [-1.609 ,  6.84  , -0.7676, -2.09  , -1.728 , -1.071 ],\n",
       "       [-1.243 , -0.9844, -2.113 , -1.733 ,  3.678 ,  3.295 ]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "test_predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c26dd009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8241746-06f9-4601-b045-10965cbaf53c",
   "metadata": {
    "id": "b8241746-06f9-4601-b045-10965cbaf53c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "true = test_predictions.label_ids\n",
    "\n",
    "def confusion_matrix(true, preds, n_classes):\n",
    "    m = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for t, p in zip(true, preds):\n",
    "        m[t][p] += 1\n",
    "\n",
    "    return m\n",
    "\n",
    "cm = confusion_matrix(true=true, preds=preds, n_classes=6)\n",
    "perc_correct = (np.diag(cm) / np.sum(cm, axis=1)) * 100\n",
    "perc_incorrect = 100 - perc_correct\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "plt.bar(lbl2idx.keys(), perc_correct, label = 'Correct')\n",
    "plt.bar(lbl2idx.keys(), perc_incorrect, bottom=perc_correct, label = 'Incorrect')\n",
    "plt.title('True prediction rate per each class')\n",
    "y_ticks = np.arange(0, 101, 20)  \n",
    "plt.yticks(y_ticks, [f'{y}%' for y in y_ticks])\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511b6f7-4370-43f6-a8fd-5c60b3331881",
   "metadata": {
    "id": "2511b6f7-4370-43f6-a8fd-5c60b3331881"
   },
   "source": [
    "* What can we see from this? Briefly explain the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa663bb-4208-477f-bc50-55f653709ce6",
   "metadata": {
    "id": "8fa663bb-4208-477f-bc50-55f653709ce6"
   },
   "source": [
    "___\n",
    "Student answers here: Class: Surprise is far less correctly predicted compared to the others. This might be related to the class distribution within training dataset (a.k.a. imbalanced dataset). Also since surprise and love are considered positive terms, the model might not be able to distinguish their context based on the provided data.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c7f51-88d7-42be-8897-b34b7332fe75",
   "metadata": {
    "id": "415c7f51-88d7-42be-8897-b34b7332fe75"
   },
   "source": [
    "* To investigate possible reasons for this imbalance, let's look at our dataset\n",
    "* Plot the train set class distribution as normalized percentages\n",
    "    * You can make use of the Hugging Face datasets object's `to_pandas()` method\n",
    "    * Then you can use all available data plotting and data handling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef3617d0-f3e4-4634-8325-2058c636f454",
   "metadata": {
    "id": "ef3617d0-f3e4-4634-8325-2058c636f454"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np_to_pie = emotion_dataset['train'].to_pandas().labels.value_counts(normalize=True).rename(index=idx2lbl)\n",
    "plt.pie(\n",
    "    np_to_pie,\n",
    "    labels=np_to_pie.index,\n",
    "    autopct='%1.1f%%',\n",
    "    )\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e98ee-ee8d-4cdd-9b05-8055c257a771",
   "metadata": {
    "id": "940e98ee-ee8d-4cdd-9b05-8055c257a771"
   },
   "source": [
    "1) What problems can you imagine when seeing this class distribution?\n",
    "2) What problems could arise if we simply duplicated samples of the classes with fewer available samples?\n",
    "3) What problems could arise if we simply downsampled the majority classes to the number of minority samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70943ea-f474-469b-9d85-551b88b5da03",
   "metadata": {
    "id": "e70943ea-f474-469b-9d85-551b88b5da03"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "1. Surprise is far less represented in the training dataset. Therefore, our model is biased towards predicting the more frequent labels, as even a constant predictor would incur less loss by estimating joy and sadness. This is a classic case of imbalanced data.\n",
    "\n",
    "2. Duplicating samples will not provide any new information to our model. The classes which have majority in distribution will be still predicted better since they see more variety. Whereas, less represented classes (like surprise) will cause overfitting after we duplicate them.\n",
    "\n",
    "3. Downsampling the majority classes is also a problematic option, given that reduced training data can't be more informative regarding to its lower size. After downsampling, a part of information will be consequently removed and therefore the a decrease in model performance is expected due to more unseen area. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e05c3a-0540-4c22-aff7-3642f1c71f3d",
   "metadata": {
    "id": "a1e05c3a-0540-4c22-aff7-3642f1c71f3d"
   },
   "source": [
    "How to deal with this class imbalance?\n",
    "\n",
    "* We can modify the loss function to assign weight coefficients to the losses of certain classes\n",
    "* Therefore, a wrong classification of a certain class is weighted higher or lower, depending on its coefficients we provided\n",
    "* How do we calculate these weights?\n",
    "    * We can make use of the existing normalized frequency distribution we plotted above\n",
    "    * But calculate the element-wise complement per label, i.e. `1 - probability`, so that each resulting label represents a weight between 0 and 1 for its own class\n",
    "    * You can check your calculation by adding element-wise the initial normalized label distribution to your weights, you should receive an array of 6 `1`s (ignoring floating point imprecisions)\n",
    "    * Transform the type to a `torch.float32` tensor, and move the tensor separately to the `device` you want\n",
    "        * Huggingface will use a GPU as soon as it finds one available, but this separate tensor needs to be moved manually\n",
    "* However, since the Hugging Face Trainer we used before abstracted away the option to define our loss function, we need to overwrite the `compute_loss` (see [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.compute_loss)) function of the Trainer\n",
    "* To accomplish that:\n",
    "    *  we inherint from the `Trainer` class\n",
    "    *  define a `compute_loss` function that takes in a `model`, `inputs`, and a `return_outputs=False` keyword\n",
    "    *  First, we calculate the model `outputs` by giving our model the `inputs`\n",
    "        *  our `inputs` consist of a collated batch of inputs that the tokenizer creates, along with a `'labels'`, all inside a dictionary\n",
    "        *  therefore, we need to use dictionary expansion to assign all keyword arguments in the `model`\n",
    "    *  the `outputs` from Hugging Face transformers, again, are a dictionary, so we need to extract their `'logits'` and assign them to a `logits` variable\n",
    "    *  then, we extract the `labels` from the input\n",
    "    *  create a `criterion` using our well known `nn.CrossEntropyLoss()`\n",
    "        *  inside `nn.CrossEntropyLoss()`, we now use the `weight` argument and assign our calculated weight coefficients to it\n",
    "    *  compute the loss\n",
    "    *  return `(loss, outputs) if return_outputs else loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b84fda9-a689-458a-a35a-dbca8dc1affd",
   "metadata": {
    "id": "3b84fda9-a689-458a-a35a-dbca8dc1affd"
   },
   "outputs": [],
   "source": [
    "complement_weights = torch.tensor(\n",
    "    ((1 - emotion_dataset['train'].to_pandas().labels.value_counts(normalize=True)).sort_index()), \n",
    "    dtype=torch.float32,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eee5f4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b37a4767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7084, 0.6649, 0.9185, 0.8651, 0.8789, 0.9643], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complement_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "929bd669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 3,  ..., 1, 3, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(emotion_dataset['train']['labels']).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d509e6b5-e09e-4ca3-bf17-3115f2bbc3f7",
   "metadata": {
    "id": "d509e6b5-e09e-4ca3-bf17-3115f2bbc3f7"
   },
   "outputs": [],
   "source": [
    "class BalancedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        criterion = nn.CrossEntropyLoss(weight=complement_weights)\n",
    "        loss = criterion(logits.view(-1, 6), inputs['labels'].view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf666a-ffe8-4c09-a2cb-e505c0021788",
   "metadata": {
    "id": "4acf666a-ffe8-4c09-a2cb-e505c0021788"
   },
   "source": [
    "* Now, we can go back and re-create a second `TrainingArguments` instance\n",
    "    * Keep all settings equal, just change the `output_dir` to something like `'./logs/run2'` so that our earlier results aren't overwritten or appended to, and we keep everything in one directory\n",
    "* Re-load a new BERT model, so you start pre-training again from the base pre-trained model\n",
    "* Incorporate the new instance of `TrainingArguments` into our `BalancedLossTrainer`\n",
    "* Run all training, evaluation, and testing steps again\n",
    "* Repeat the plotting of correct and incorrect percentages per class again\n",
    "    * Comment on the new F1 score. Did it change compared to before? Why or why not?\n",
    "    * Comment also on the new results and try to explain what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4441570-c458-4868-a274-eac67e4ef650",
   "metadata": {
    "id": "d4441570-c458-4868-a274-eac67e4ef650"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5c7334a63c403485a704b52d295a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"sadness\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"love\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"fear\",\n",
      "    \"5\": \"surprise\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"fear\": 4,\n",
      "    \"joy\": 1,\n",
      "    \"love\": 2,\n",
      "    \"sadness\": 0,\n",
      "    \"surprise\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'google-bert/bert-base-uncased',\n",
    "    num_labels = 6,\n",
    "    id2label = idx2lbl,\n",
    "    label2id = lbl2idx,\n",
    "    force_download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5088fa3-fbad-4b16-abe1-e0ab9e9d1890",
   "metadata": {
    "id": "a5088fa3-fbad-4b16-abe1-e0ab9e9d1890"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args2 = TrainingArguments(\n",
    "    output_dir='./logs/run2',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-3,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=len(emotion_dataset['train']) // 8,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55f6bbcb-2fb6-4c12-aff9-4cbac1ad0a9e",
   "metadata": {
    "id": "55f6bbcb-2fb6-4c12-aff9-4cbac1ad0a9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = BalancedLossTrainer(\n",
    "    model = model,\n",
    "    args = training_args2,\n",
    "    compute_metrics = compute_metrics,\n",
    "    train_dataset = emotion_dataset['train'],\n",
    "    eval_dataset = emotion_dataset['validation'],\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c722db60-ead1-4222-b1e7-0d3c662100ff",
   "metadata": {
    "id": "c722db60-ead1-4222-b1e7-0d3c662100ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3000\n",
      "  Number of trainable parameters = 109486854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafb60d2eae44eb0816319880169a140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30c1a1853074a158df12263c664a8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/run2\\checkpoint-1000\n",
      "Configuration saved in ./logs/run2\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21918249130249023, 'eval_f1': 0.9266841738225775, 'eval_runtime': 19.1052, 'eval_samples_per_second': 104.683, 'eval_steps_per_second': 3.298, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./logs/run2\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run2\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run2\\checkpoint-1000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.323, 'learning_rate': 6.693333333333334e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3219f68f2b44ce81b14487322db800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/run2\\checkpoint-2000\n",
      "Configuration saved in ./logs/run2\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17878811061382294, 'eval_f1': 0.9324238991103515, 'eval_runtime': 13.5026, 'eval_samples_per_second': 148.12, 'eval_steps_per_second': 4.666, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./logs/run2\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run2\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run2\\checkpoint-2000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2d2632c33f4cf9a1f27514780b67ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/run2\\checkpoint-3000\n",
      "Configuration saved in ./logs/run2\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18820041418075562, 'eval_f1': 0.9365360918750442, 'eval_runtime': 12.8712, 'eval_samples_per_second': 155.386, 'eval_steps_per_second': 4.895, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./logs/run2\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run2\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run2\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [logs\\run2\\checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./logs/run2\\checkpoint-2000 (score: 0.17878811061382294).\n",
      "Deleting older checkpoint [logs\\run2\\checkpoint-2000] due to args.save_total_limit\n",
      "Deleting older checkpoint [logs\\run2\\checkpoint-3000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 677.2177, 'train_samples_per_second': 70.878, 'train_steps_per_second': 4.43, 'train_loss': 0.24867243194580077, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.24867243194580077, metrics={'train_runtime': 677.2177, 'train_samples_per_second': 70.878, 'train_steps_per_second': 4.43, 'train_loss': 0.24867243194580077, 'epoch': 3.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d79524c1-0b42-4bfd-ace4-0cb786d2c428",
   "metadata": {
    "id": "d79524c1-0b42-4bfd-ace4-0cb786d2c428"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6879610545384321a0daeddbbf0e3ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.1770351231098175, 'test_f1': 0.9334860580646495, 'test_runtime': 17.7177, 'test_samples_per_second': 112.882, 'test_steps_per_second': 3.556}\n"
     ]
    }
   ],
   "source": [
    "test_predictions = trainer.predict(\n",
    "    test_dataset = emotion_dataset['test']\n",
    ")\n",
    "\n",
    "print(test_predictions.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83faaec0-6cb9-4956-940e-d00d25469756",
   "metadata": {
    "id": "83faaec0-6cb9-4956-940e-d00d25469756"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "true = test_predictions.label_ids\n",
    "\n",
    "def confusion_matrix(true, preds, n_classes):\n",
    "    m = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for t, p in zip(true, preds):\n",
    "        m[t][p] += 1\n",
    "\n",
    "    return m\n",
    "\n",
    "cm = confusion_matrix(true=true, preds=preds, n_classes=6)\n",
    "perc_correct = (np.diag(cm) / np.sum(cm, axis=1)) * 100\n",
    "perc_incorrect = 100 - perc_correct\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "plt.bar(lbl2idx.keys(), perc_correct, label = 'Correct')\n",
    "plt.bar(lbl2idx.keys(), perc_incorrect, bottom=perc_correct, label = 'Incorrect')\n",
    "plt.title('True prediction rate per each class (after weigthed loss)')\n",
    "y_ticks = np.arange(0, 101, 20)  \n",
    "plt.yticks(y_ticks, [f'{y}%' for y in y_ticks])\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bbea8f-f026-45f9-9bce-ff1fc91e6937",
   "metadata": {
    "id": "61bbea8f-f026-45f9-9bce-ff1fc91e6937"
   },
   "source": [
    "___\n",
    "Student answers here: The model is slightly improved in terms of F1 score. New model can predict better some of the classes like 'love', 'anger' and 'fear'. Yet, it is hard to talk about an improvement for 'surprise'. Regarding these results, we can say that having weighted loss provides an improvement. To achieve better prediction for 'surprise', we might need to tune our model again (we can change the parameters of the trainer like batch size etc.)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37211a4f-1007-43d2-8d64-e6b5e29b513f",
   "metadata": {
    "id": "37211a4f-1007-43d2-8d64-e6b5e29b513f"
   },
   "source": [
    "# Task 2: Text Summarization with T5\n",
    "In this task, we will use the [SamSum](https://huggingface.co/datasets/samsum) dataset, which is a collection of back-and-forth chats between people, and a narrator-style summary of the chat.\n",
    "We will fine-tune our T5 model on this dataset and experiment with it on our own invented chats to see its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8b87c73-32c0-441a-910c-91129b5779e0",
   "metadata": {
    "id": "e8b87c73-32c0-441a-910c-91129b5779e0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a254090d-56b9-474c-b3f1-011c8b707692",
   "metadata": {
    "id": "a254090d-56b9-474c-b3f1-011c8b707692"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' # there might be interferences with the parallelism of the Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57cc35-bc28-4881-aec1-1e3e02df9b19",
   "metadata": {
    "id": "0e57cc35-bc28-4881-aec1-1e3e02df9b19"
   },
   "source": [
    "* Download the dataset from the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "575512e5-9b1a-4250-9c61-c6dbc24b5848",
   "metadata": {
    "id": "575512e5-9b1a-4250-9c61-c6dbc24b5848"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0085661cc774ceeb66be2d879bf2623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.7z:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\esual\\.cache\\huggingface\\hub\\datasets--samsum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a701f771e9042ceb282356969e0475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78e8f5c597b4178a6791cc922426ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea74a8da20d40dfbda97a2300c8c11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samsum = load_dataset('Samsung/samsum', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9f4e7-80ff-42ac-9526-884e69f9875c",
   "metadata": {
    "id": "26d9f4e7-80ff-42ac-9526-884e69f9875c"
   },
   "source": [
    "* Use Hugging Face's `pipeline` interface to see the non-finetuned performance of our model.\n",
    "* Load the [T5-small](https://huggingface.co/t5-small) model inside the [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines) object, along with the `task=summarization` attribute.\n",
    "* Pick a sample from the [SamSum Test Set](https://huggingface.co/datasets/samsum/viewer/samsum/test). You can directly copy it from the website if you like.\n",
    "* Input it into the created pipeline object and check the result.\n",
    "    * In case of memory problems later on during fine-tuning, `del` the pipeline object to free up memory afterwards.\n",
    "* Comment: How well did it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1db252a-ad67-4397-a693-554493564d6f",
   "metadata": {
    "id": "b1db252a-ad67-4397-a693-554493564d6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2eec5723ea747d69f501fff00160d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\esual\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading configuration file config.json from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--google-t5--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--google-t5--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c289137259b14a3aa96dacc060addfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--google-t5--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7342b93a9d954cadb8ce7f07a5802d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aed31ec42b3478e9375756ec20663c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea3ad8835c74a57a2a176aa14fd0d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--google-t5--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--google-t5--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--google-t5--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(model='google-t5/t5-small', task='summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4eba82f-66b4-4d28-900b-348a63a944d1",
   "metadata": {
    "id": "c4eba82f-66b4-4d28-900b-348a63a944d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but you input_length is only 39. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample id 13612174\n",
      "\n",
      "Dialogue: Daniel: Yo, at what time do you get out of work?\n",
      "Missy: At 6.\n",
      "Daniel: Drinks after dinner?\n",
      "Missy: Totally!\n",
      "Daniel: Cool.\n",
      "\n",
      "Model output: missy: At 6. Daniel: Drinks after dinner? Missy: Totally! Daniel: Cool . Daniel: cool, cool and cool .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "random_sample = samsum['test'][np.random.randint(0, len(samsum['test']))]\n",
    "output = pipe(random_sample['dialogue'])\n",
    "\n",
    "\n",
    "print(f'''\n",
    "Sample id {random_sample['id']}\n",
    "\n",
    "Dialogue: {random_sample['dialogue']}\n",
    "\n",
    "Model output: {output[0]['summary_text']}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b258c2-9ecc-4c24-9bc6-8479fffad285",
   "metadata": {
    "id": "d6b258c2-9ecc-4c24-9bc6-8479fffad285"
   },
   "source": [
    "___\n",
    "Student answers here: It did not work well given that the output is not meaningful and not able to summarize the dialogue at all. It mostly copies words from the dialogue and somehow duplicates \"cool\" three times.  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608e2f0-dde3-4f7b-95f4-cdbaad81f1be",
   "metadata": {
    "id": "1608e2f0-dde3-4f7b-95f4-cdbaad81f1be"
   },
   "source": [
    "* Then, we will load [T5's](https://huggingface.co/t5-small) `t5-small` tokenizer, again using `AutoTokenizer` from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "701bfc05-bc7d-405e-afec-6b09b9e42c86",
   "metadata": {
    "id": "701bfc05-bc7d-405e-afec-6b09b9e42c86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ccca47c3754140b5ed232905cd5a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\esual\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5cd4f72cb7461aa74159f52d8b02bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44547b7f66bc444cbf167cf2c3fb102d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ab0c8-4388-4163-95f1-102a8fef3085",
   "metadata": {
    "id": "4c5ab0c8-4388-4163-95f1-102a8fef3085"
   },
   "source": [
    "T5 is a sequence-to-sequence encoder-decoder model, meaning that it takes sequences as input, transforms them, and outputs target sequences. The model can be used for translation, classification, summarization, and much more, given the correct input prompts.\n",
    "\n",
    "As we are dealing with a summarization task, we need to preface our sequences with the necessary `'summarize: '` prompt.\n",
    "* Write a preprocess function which takes in a dataset split and prefaces every input sequence with the string `'summarize: '`\n",
    "* Afterwards, use the tokenizer with the arguments `truncation=True` and `max_length=512` to limit the maximum lengths of all inputs, and tokenize all sequences\n",
    "* Then, separately transform the target sequences using `truncation=True` with a `max_length=128`\n",
    "* Extract the `input_ids` from the transformed target sequences and add them to the the dictionary output of the transformed input sequences with the key `labels`\n",
    "* Return the dictionary object afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d7d0c26-fe49-4224-93a6-40d484c500fc",
   "metadata": {
    "id": "1d7d0c26-fe49-4224-93a6-40d484c500fc"
   },
   "outputs": [],
   "source": [
    "def preprocess_summarization(split):\n",
    "    dialogues = ['summarize: ' + i for i in split['dialogue']]\n",
    "    tokenized_dialogues = tokenizer(dialogues, truncation=True, max_length=512)\n",
    "    targets = split['summary']\n",
    "    tokenized_targets = tokenizer(targets, truncation=True, max_length=128)\n",
    "    tokenized_dialogues['labels'] = tokenized_targets['input_ids']\n",
    "    return tokenized_dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727e745-bfab-4612-aa30-3b78e6defa2e",
   "metadata": {
    "id": "0727e745-bfab-4612-aa30-3b78e6defa2e"
   },
   "source": [
    "* As before with the classification task, `map` the preprocessing function onto the entire dataset using the `batched=True` option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "310ee61b-5b80-43a0-8a5d-d76c69dddff2",
   "metadata": {
    "id": "310ee61b-5b80-43a0-8a5d-d76c69dddff2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a325c398914a118f93670740d1f69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4395490ed814b35ad35a264e94ea412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55eaef772367444eb02e511f464f5049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_samsum = samsum.map(preprocess_summarization, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b20fa-e22d-42d1-b40f-4414b9f2a819",
   "metadata": {
    "id": "aa2b20fa-e22d-42d1-b40f-4414b9f2a819"
   },
   "source": [
    "* As the next step, we create a `collator` [object](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq), which takes in the `tokenizer` and our `model`\n",
    "    * At this stage, it is fine to just write `model='t5-small'` as input to the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "611afa55-9d71-4a5f-8017-3abce46ccd51",
   "metadata": {
    "id": "611afa55-9d71-4a5f-8017-3abce46ccd51"
   },
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model='t5-small'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119c26e-7549-4995-a05d-1374c1e7b85c",
   "metadata": {
    "id": "2119c26e-7549-4995-a05d-1374c1e7b85c"
   },
   "source": [
    "* To evaluate our model's summarization performance, we employ the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) score.\n",
    "* Advantages include:\n",
    "    * The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric commonly used for evaluating the quality of machine-generated text, particularly summaries\n",
    "    * ROUGE measures the similarity between the generated summary and one or more reference (human-written) summaries\n",
    "    * ROUGE provides a standardized way to evaluate the quality of text summaries, which is important for at-scale comparison between different summarization models\n",
    "    * ROUGE includes multiple metrics, such as ROUGE-N (for n-grams), ROUGE-L (for longest common subsequence), and ROUGE-W (for weighted n-grams). Depending on the task, these metrics capture different aspects of summary quality, allowing a more comprehensive evaluation.\n",
    "* Disadvantages include:\n",
    "    * ROUGE relies on exact word or n-gram matches, which may not capture the semantic meaning well. Two sentences with similar meaning but slightly different phrasing might receive a low ROUGE score. (You can use the above link to play around with some references and generated sequences)\n",
    "    * ROUGE does not consider the order of words or phrases. If a model generates a summary with words or phrases in a different order than the reference, even if the content is correct, the ROUGE score might be lower.\n",
    "    * ROUGE measures lexical overlap and does not consider deeper linguistic structures or coherence in summaries.\n",
    "* Load the `rouge` score from the `evaluate` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd578c4e-47c9-4d6d-bd4d-91a509cf8129",
   "metadata": {
    "id": "cd578c4e-47c9-4d6d-bd4d-91a509cf8129"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a845cd082b8442d49b3b5b2610bf18c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba798853-94c9-4013-9fbb-514e1d166dd7",
   "metadata": {
    "id": "ba798853-94c9-4013-9fbb-514e1d166dd7"
   },
   "source": [
    "* Then, we need to define our `compute_metrics` function, but this time for the ROUGE score\n",
    "* At first, we receive again our tuple of `EvalPred` predictions and labels, which is also the input to our function\n",
    "    * Split up the input tuple by extracting the labels and predictions\n",
    "    * In the T5 case, the predictions are again a tuple of `(logits, hidden_representations)`\n",
    "    * For our task, extract only the logits\n",
    "    * Apply `argmax()` on the last dimension of the logits to get the predictions\n",
    "* Then, we use our `tokenizer` to `batch_decode` the argmaxed logits\n",
    "    * Here, we activate the tokenizer setting `skip_special_tokens=True`, and `clean_up_tokenization_spaces=True`\n",
    "* Before we can decode our labels, we must first replace the padding tokens of the labels batch, represented by the value `-100`, with the actual padding tokens of the tokenizer\n",
    "    * The `-100` are a convention of the `Seq2SeqTrainer`, and don't have any deeper meaning\n",
    "    * the padding token can be accessed by `tokenizer.pad_token_id`\n",
    "* Then, we also `batch_decode` the labels\n",
    "    * Also skip the special tokens, but no need clean up tokenization spaces since we handled them separately\n",
    "* Afterwards, we input both decoded predictions and labels into the `rouge.compute` function\n",
    "* That result dictionary is returned\n",
    "    * Optionally, round all numbers in the result dictionary to 4 or 6 digits, whichever format you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea674e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df12172f-7fc2-437d-b167-b843f694d3b1",
   "metadata": {
    "id": "df12172f-7fc2-437d-b167-b843f694d3b1"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    labels, predictions = eval_preds.label_ids, eval_preds.predictions\n",
    "    logits, _ = predictions\n",
    "    pred_class = np.argmax(logits, axis=-1)\n",
    "    dec_preds = tokenizer.batch_decode(pred_class, skip_special_tokens = True, clean_up_tokenization_spaces = True)\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "    dec_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    rouge_score = rouge.compute(predictions=dec_preds, references=dec_labels)\n",
    "    return {'rouge':rouge_score.round(4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dff509-090d-4f01-a5dc-b02ecacc7069",
   "metadata": {
    "id": "41dff509-090d-4f01-a5dc-b02ecacc7069"
   },
   "source": [
    "* Then we create a `Seq2SeqTrainingArguments` [instance](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments), which is just a specialized version of our general `TrainingArguments` function from before.\n",
    "* In our case, we can base our settings on the training arguments from BERT classification, except:\n",
    "    * Since we now have longer sequences, it might be necessary to reduce batch sizes even more, so experiment a bit with your GPU memory availability\n",
    "        * Try to max-out your GPU memory by increasing the batch size as much as possible to speed up training\n",
    "    * change the `output_dir` to something under the same path from before, e.g. `'./logs/run3/'`\n",
    "* Once created, load the model using the `AutoModelForSeq2SeqLM`\n",
    "* Instantiate a `Seq2SeqTrainer` [object](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.Seq2SeqTrainer) in the same style as before\n",
    "    * This time, we just need to add the `collator` object to the `data_collator` argument\n",
    "* Call the `train()` method and start training\n",
    "* Comment briefly on the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c2efe-6aa7-4ad6-9705-bf4dffef05cf",
   "metadata": {
    "id": "384c2efe-6aa7-4ad6-9705-bf4dffef05cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\esual/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, id, summary. If dialogue, id, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 11049\n",
      "  Number of trainable parameters = 60506624\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c08302b08f48b68aa602393d2453c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 35\u001b[0m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     27\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1755\u001b[0m ):\n\u001b[0;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2508\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2511\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2539\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1648\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1648\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1663\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1040\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[0;32m   1028\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1029\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1040\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:725\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    722\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m--> 725\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(hidden_states)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:328\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    327\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 328\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:293\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    291\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m    292\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m--> 293\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\esual\\miniconda3\\envs\\temp_dl4nlp\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args3 = Seq2SeqTrainingArguments(\n",
    "    output_dir='./logs/run3',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-3,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=len(tokenized_samsum['train']) // 2,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    't5-small'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    data_collator=collator,\n",
    "    args=training_args3,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_samsum['train'],\n",
    "    eval_dataset=tokenized_samsum['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d3881-7ffc-493f-ba2e-0594560e3e98",
   "metadata": {
    "id": "248d3881-7ffc-493f-ba2e-0594560e3e98"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1afc9-c18a-4eef-9026-b569dff590ec",
   "metadata": {
    "id": "3ad1afc9-c18a-4eef-9026-b569dff590ec"
   },
   "source": [
    "* As the last step of this exercise, move the `model`, which represents the already loaded best model due to our `Seq2SeqTrainingArguments` settings, to the CPU and input it into a new `pipeline` object as the `model=` keyword.\n",
    "* Additionally, prove the `tokenizer=` argument\n",
    "* Then, fill in your chosen test set sample from before\n",
    "* Compare the results to your initial, pre fine-tuning output and discuss what got better or worse\n",
    "* Comment on whether the narrator-style summary performance increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f6176-f248-4582-83bb-16c440774ce1",
   "metadata": {
    "id": "a74f6176-f248-4582-83bb-16c440774ce1"
   },
   "outputs": [],
   "source": [
    "pipe = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048bdb0-42f9-4586-8a30-e49bd48a8f22",
   "metadata": {
    "id": "f048bdb0-42f9-4586-8a30-e49bd48a8f22"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8ac38-1b16-4a42-aa24-e85c2a14a536",
   "metadata": {
    "id": "a5c8ac38-1b16-4a42-aa24-e85c2a14a536"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e576fcf-04ba-497d-9bc7-734bcff18979",
   "metadata": {
    "id": "0e576fcf-04ba-497d-9bc7-734bcff18979"
   },
   "source": [
    "# Inspect and share all your experiment results via TensorBoard (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb91278-d706-4daa-9521-8baa50996218",
   "metadata": {
    "id": "acb91278-d706-4daa-9521-8baa50996218"
   },
   "source": [
    "* While we now only had 3 runtime variations, in practice we often have tens or hundreds of hyperparameter and/or model combinatinations\n",
    "* To assemble manual train/val/test plots, scale all x- and y-axes for various units, and plot all desired combinations side by side is very cumbersome and time consuming\n",
    "* Services like [tensorboard-dev](https://tensorboard.dev/) and [tensorboard in general](https://www.tensorflow.org/tensorboard) offer the possibility to simply read in our created `'logs'` directory, which enables us to browse through an online interface of automatically generated plots for all our tracked metrics\n",
    "* Install (if you don't already have it) `tensorboard` via `pip install -U tensorboard)` and either:\n",
    "    * Upload your `'logs'` directly to Google (requires a Google account and consent to store the specific `'logs'` directory on Google's servers)\n",
    "        * execute on your command line `tensorboard dev upload --logdir logs --name \"whatever-name-you-want-to-call-it\"`\n",
    "        * then you will need to login to your Google account, consent to uploading the `'logs'` directory, and a shareable link will appear as command line output\n",
    "        * the link will lead you to the hosted experiment plots\n",
    "        * FYI: it is also possible to delete uploaded tensorboards via `tensorboard dev delete --experiment_id EXPERIMENT_ID`\n",
    "    * If you don't want to upload your experiments to Google or don't have a Google account, you can achieve the same browsable layout on your local machine\n",
    "    * execute on your command line `tensorboard --logdir logs`\n",
    "    * a localhost instance, usually on `https://localhost:6006/`, will open\n",
    "* Choose one of the above options to visualize your experiments in tensorboard\n",
    "* Include a screenshot of the results in the notebook submission of this exercise\n",
    "    * You can embed images in Jupyerlab in markdown cells [as described here](https://stackoverflow.com/questions/41604263/how-do-i-display-local-image-in-markdown)\n",
    "    * Also send us the screenshot file with it, otherwise we can't see your embedded image in the notebook\n",
    "* Last note: You aren't dependend on Hugging Face experiments to upload and visualize results to tensorboard\n",
    "    * You can upload every folder that is generated in the same format to tensorboard\n",
    "    * For instance, the same can be achived using [PyTorch](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) and the SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f75ed-1263-474b-9a99-1eacc6cfc733",
   "metadata": {
    "id": "0e8f75ed-1263-474b-9a99-1eacc6cfc733"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "temp_dl4nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
